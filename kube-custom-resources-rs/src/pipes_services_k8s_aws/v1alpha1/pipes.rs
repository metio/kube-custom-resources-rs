// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --filename=./crd-catalog/aws-controllers-k8s/pipes-controller/pipes.services.k8s.aws/v1alpha1/pipes.yaml --derive=Default --derive=PartialEq
// kopium version: 0.17.1

use kube::CustomResource;
use serde::{Serialize, Deserialize};
use std::collections::BTreeMap;
use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;

/// PipeSpec defines the desired state of Pipe.
/// 
/// 
/// An object that represents a pipe. Amazon EventBridgePipes connect event sources
/// to targets and reduces the need for specialized knowledge and integration
/// code.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
#[kube(group = "pipes.services.k8s.aws", version = "v1alpha1", kind = "Pipe", plural = "pipes")]
#[kube(namespaced)]
#[kube(status = "PipeStatus")]
#[kube(schema = "disabled")]
pub struct PipeSpec {
    /// A description of the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// The state the pipe should be in.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "desiredState")]
    pub desired_state: Option<String>,
    /// The ARN of the enrichment resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enrichment: Option<String>,
    /// The parameters required to set up enrichment on your pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentParameters")]
    pub enrichment_parameters: Option<PipeEnrichmentParameters>,
    /// The name of the pipe.
    pub name: String,
    /// The ARN of the role that allows the pipe to send data to the target.
    #[serde(rename = "roleARN")]
    pub role_arn: String,
    /// The ARN of the source resource.
    pub source: String,
    /// The parameters required to set up a source for your pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceParameters")]
    pub source_parameters: Option<PipeSourceParameters>,
    /// The list of key-value pairs to associate with the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<BTreeMap<String, String>>,
    /// The ARN of the target resource.
    pub target: String,
    /// The parameters required to set up a target for your pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetParameters")]
    pub target_parameters: Option<PipeTargetParameters>,
}

/// The parameters required to set up enrichment on your pipe.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeEnrichmentParameters {
    /// These are custom parameter to be used when the target is an API Gateway REST
    /// APIs or EventBridge ApiDestinations. In the latter case, these are merged
    /// with any InvocationParameters specified on the Connection, with any values
    /// from the Connection taking precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeEnrichmentParametersHttpParameters>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
}

/// These are custom parameter to be used when the target is an API Gateway REST
/// APIs or EventBridge ApiDestinations. In the latter case, these are merged
/// with any InvocationParameters specified on the Connection, with any values
/// from the Connection taking precedence.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeEnrichmentParametersHttpParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<BTreeMap<String, String>>,
}

/// The parameters required to set up a source for your pipe.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParameters {
    /// The parameters for using an Active MQ broker as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "activeMQBrokerParameters")]
    pub active_mq_broker_parameters: Option<PipeSourceParametersActiveMqBrokerParameters>,
    /// The parameters for using a DynamoDB stream as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dynamoDBStreamParameters")]
    pub dynamo_db_stream_parameters: Option<PipeSourceParametersDynamoDbStreamParameters>,
    /// The collection of event patterns used to filter events. For more information,
    /// see Events and Event Patterns (https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-and-event-patterns.html)
    /// in the Amazon EventBridge User Guide.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "filterCriteria")]
    pub filter_criteria: Option<PipeSourceParametersFilterCriteria>,
    /// The parameters for using a Kinesis stream as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeSourceParametersKinesisStreamParameters>,
    /// The parameters for using an MSK stream as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedStreamingKafkaParameters")]
    pub managed_streaming_kafka_parameters: Option<PipeSourceParametersManagedStreamingKafkaParameters>,
    /// The parameters for using a Rabbit MQ broker as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rabbitMQBrokerParameters")]
    pub rabbit_mq_broker_parameters: Option<PipeSourceParametersRabbitMqBrokerParameters>,
    /// The parameters for using a self-managed Apache Kafka stream as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "selfManagedKafkaParameters")]
    pub self_managed_kafka_parameters: Option<PipeSourceParametersSelfManagedKafkaParameters>,
    /// The parameters for using a Amazon SQS stream as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeSourceParametersSqsQueueParameters>,
}

/// The parameters for using an Active MQ broker as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersActiveMqBrokerParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    /// The Secrets Manager secret that stores your broker credentials.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeSourceParametersActiveMqBrokerParametersCredentials>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
}

/// The Secrets Manager secret that stores your broker credentials.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersActiveMqBrokerParametersCredentials {
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a DynamoDB stream as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersDynamoDbStreamParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    /// A DeadLetterConfig object that contains information about a dead-letter queue
    /// configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeSourceParametersDynamoDbStreamParametersDeadLetterConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
}

/// A DeadLetterConfig object that contains information about a dead-letter queue
/// configuration.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersDynamoDbStreamParametersDeadLetterConfig {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The collection of event patterns used to filter events. For more information,
/// see Events and Event Patterns (https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-and-event-patterns.html)
/// in the Amazon EventBridge User Guide.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersFilterCriteria {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub filters: Option<Vec<PipeSourceParametersFilterCriteriaFilters>>,
}

/// Filter events using an event pattern. For more information, see Events and
/// Event Patterns (https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-and-event-patterns.html)
/// in the Amazon EventBridge User Guide.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersFilterCriteriaFilters {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pattern: Option<String>,
}

/// The parameters for using a Kinesis stream as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersKinesisStreamParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    /// A DeadLetterConfig object that contains information about a dead-letter queue
    /// configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeSourceParametersKinesisStreamParametersDeadLetterConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPositionTimestamp")]
    pub starting_position_timestamp: Option<String>,
}

/// A DeadLetterConfig object that contains information about a dead-letter queue
/// configuration.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersKinesisStreamParametersDeadLetterConfig {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The parameters for using an MSK stream as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersManagedStreamingKafkaParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupID")]
    pub consumer_group_id: Option<String>,
    /// The Secrets Manager secret that stores your stream credentials.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeSourceParametersManagedStreamingKafkaParametersCredentials>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
}

/// The Secrets Manager secret that stores your stream credentials.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersManagedStreamingKafkaParametersCredentials {
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTLSAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslSCRAM512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// The parameters for using a Rabbit MQ broker as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersRabbitMqBrokerParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    /// The Secrets Manager secret that stores your broker credentials.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeSourceParametersRabbitMqBrokerParametersCredentials>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "virtualHost")]
    pub virtual_host: Option<String>,
}

/// The Secrets Manager secret that stores your broker credentials.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersRabbitMqBrokerParametersCredentials {
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a self-managed Apache Kafka stream as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersSelfManagedKafkaParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "additionalBootstrapServers")]
    pub additional_bootstrap_servers: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupID")]
    pub consumer_group_id: Option<String>,
    /// The Secrets Manager secret that stores your stream credentials.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeSourceParametersSelfManagedKafkaParametersCredentials>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<i64>,
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serverRootCaCertificate")]
    pub server_root_ca_certificate: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
    /// This structure specifies the VPC subnets and security groups for the stream,
    /// and whether a public IP address is to be used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub vpc: Option<PipeSourceParametersSelfManagedKafkaParametersVpc>,
}

/// The Secrets Manager secret that stores your stream credentials.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersSelfManagedKafkaParametersCredentials {
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTLSAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslSCRAM256Auth")]
    pub sasl_scram256_auth: Option<String>,
    /// // Optional SecretManager ARN which stores the database credentials
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslSCRAM512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// This structure specifies the VPC subnets and security groups for the stream,
/// and whether a public IP address is to be used.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersSelfManagedKafkaParametersVpc {
    /// List of SecurityGroupId.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroup")]
    pub security_group: Option<Vec<String>>,
    /// List of SubnetId.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The parameters for using a Amazon SQS stream as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeSourceParametersSqsQueueParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<i64>,
}

/// The parameters required to set up a target for your pipe.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParameters {
    /// The parameters for using an Batch job as a target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchJobParameters")]
    pub batch_job_parameters: Option<PipeTargetParametersBatchJobParameters>,
    /// The parameters for using an CloudWatch Logs log stream as a target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cloudWatchLogsParameters")]
    pub cloud_watch_logs_parameters: Option<PipeTargetParametersCloudWatchLogsParameters>,
    /// The parameters for using an Amazon ECS task as a target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ecsTaskParameters")]
    pub ecs_task_parameters: Option<PipeTargetParametersEcsTaskParameters>,
    /// The parameters for using an EventBridge event bus as a target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "eventBridgeEventBusParameters")]
    pub event_bridge_event_bus_parameters: Option<PipeTargetParametersEventBridgeEventBusParameters>,
    /// These are custom parameter to be used when the target is an API Gateway REST
    /// APIs or EventBridge ApiDestinations.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeTargetParametersHttpParameters>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
    /// The parameters for using a Kinesis stream as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeTargetParametersKinesisStreamParameters>,
    /// The parameters for using a Lambda function as a target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lambdaFunctionParameters")]
    pub lambda_function_parameters: Option<PipeTargetParametersLambdaFunctionParameters>,
    /// These are custom parameters to be used when the target is a Amazon Redshift
    /// cluster to invoke the Amazon Redshift Data API ExecuteStatement.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "redshiftDataParameters")]
    pub redshift_data_parameters: Option<PipeTargetParametersRedshiftDataParameters>,
    /// The parameters for using a SageMaker pipeline as a target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sageMakerPipelineParameters")]
    pub sage_maker_pipeline_parameters: Option<PipeTargetParametersSageMakerPipelineParameters>,
    /// The parameters for using a Amazon SQS stream as a source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeTargetParametersSqsQueueParameters>,
    /// The parameters for using a Step Functions state machine as a target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stepFunctionStateMachineParameters")]
    pub step_function_state_machine_parameters: Option<PipeTargetParametersStepFunctionStateMachineParameters>,
}

/// The parameters for using an Batch job as a target.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersBatchJobParameters {
    /// The array properties for the submitted job, such as the size of the array.
    /// The array size can be between 2 and 10,000. If you specify array properties
    /// for a job, it becomes an array job. This parameter is used only if the target
    /// is an Batch job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "arrayProperties")]
    pub array_properties: Option<PipeTargetParametersBatchJobParametersArrayProperties>,
    /// The overrides that are sent to a container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverrides")]
    pub container_overrides: Option<PipeTargetParametersBatchJobParametersContainerOverrides>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dependsOn")]
    pub depends_on: Option<Vec<PipeTargetParametersBatchJobParametersDependsOn>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobDefinition")]
    pub job_definition: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobName")]
    pub job_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<BTreeMap<String, String>>,
    /// The retry strategy that's associated with a job. For more information, see
    /// Automated job retries (https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html)
    /// in the Batch User Guide.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryStrategy")]
    pub retry_strategy: Option<PipeTargetParametersBatchJobParametersRetryStrategy>,
}

/// The array properties for the submitted job, such as the size of the array.
/// The array size can be between 2 and 10,000. If you specify array properties
/// for a job, it becomes an array job. This parameter is used only if the target
/// is an Batch job.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersBatchJobParametersArrayProperties {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub size: Option<i64>,
}

/// The overrides that are sent to a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersBatchJobParametersContainerOverrides {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeTargetParametersBatchJobParametersContainerOverridesEnvironment>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirements")]
    pub resource_requirements: Option<Vec<PipeTargetParametersBatchJobParametersContainerOverridesResourceRequirements>>,
}

/// The environment variables to send to the container. You can add new environment
/// variables, which are added to the container at launch, or you can override
/// the existing environment variables from the Docker image or the task definition.
/// 
/// 
/// Environment variables cannot start with "Batch". This naming convention is
/// reserved for variables that Batch sets.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersBatchJobParametersContainerOverridesEnvironment {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The type and amount of a resource to assign to a container. The supported
/// resources include GPU, MEMORY, and VCPU.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersBatchJobParametersContainerOverridesResourceRequirements {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// An object that represents an Batch job dependency.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersBatchJobParametersDependsOn {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobID")]
    pub job_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
}

/// The retry strategy that's associated with a job. For more information, see
/// Automated job retries (https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html)
/// in the Batch User Guide.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersBatchJobParametersRetryStrategy {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub attempts: Option<i64>,
}

/// The parameters for using an CloudWatch Logs log stream as a target.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersCloudWatchLogsParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logStreamName")]
    pub log_stream_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timestamp: Option<String>,
}

/// The parameters for using an Amazon ECS task as a target.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProviderStrategy")]
    pub capacity_provider_strategy: Option<Vec<PipeTargetParametersEcsTaskParametersCapacityProviderStrategy>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableECSManagedTags")]
    pub enable_ecs_managed_tags: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableExecuteCommand")]
    pub enable_execute_command: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub group: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "launchType")]
    pub launch_type: Option<String>,
    /// This structure specifies the network configuration for an Amazon ECS task.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "networkConfiguration")]
    pub network_configuration: Option<PipeTargetParametersEcsTaskParametersNetworkConfiguration>,
    /// The overrides that are associated with a task.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub overrides: Option<PipeTargetParametersEcsTaskParametersOverrides>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementConstraints")]
    pub placement_constraints: Option<Vec<PipeTargetParametersEcsTaskParametersPlacementConstraints>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementStrategy")]
    pub placement_strategy: Option<Vec<PipeTargetParametersEcsTaskParametersPlacementStrategy>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "platformVersion")]
    pub platform_version: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "propagateTags")]
    pub propagate_tags: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "referenceID")]
    pub reference_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<PipeTargetParametersEcsTaskParametersTags>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskCount")]
    pub task_count: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskDefinitionARN")]
    pub task_definition_arn: Option<String>,
}

/// The details of a capacity provider strategy. To learn more, see CapacityProviderStrategyItem
/// (https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CapacityProviderStrategyItem.html)
/// in the Amazon ECS API Reference.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersCapacityProviderStrategy {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub base: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProvider")]
    pub capacity_provider: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub weight: Option<i64>,
}

/// This structure specifies the network configuration for an Amazon ECS task.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersNetworkConfiguration {
    /// This structure specifies the VPC subnets and security groups for the task,
    /// and whether a public IP address is to be used. This structure is relevant
    /// only for ECS tasks that use the awsvpc network mode.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "awsVPCConfiguration")]
    pub aws_vpc_configuration: Option<PipeTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration>,
}

/// This structure specifies the VPC subnets and security groups for the task,
/// and whether a public IP address is to be used. This structure is relevant
/// only for ECS tasks that use the awsvpc network mode.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "assignPublicIP")]
    pub assign_public_ip: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroups")]
    pub security_groups: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The overrides that are associated with a task.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersOverrides {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverrides")]
    pub container_overrides: Option<Vec<PipeTargetParametersEcsTaskParametersOverridesContainerOverrides>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<String>,
    /// The amount of ephemeral storage to allocate for the task. This parameter
    /// is used to expand the total amount of ephemeral storage available, beyond
    /// the default amount, for tasks hosted on Fargate. For more information, see
    /// Fargate task storage (https://docs.aws.amazon.com/AmazonECS/latest/userguide/using_data_volumes.html)
    /// in the Amazon ECS User Guide for Fargate.
    /// 
    /// 
    /// This parameter is only supported for tasks hosted on Fargate using Linux
    /// platform version 1.4.0 or later. This parameter is not supported for Windows
    /// containers on Fargate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ephemeralStorage")]
    pub ephemeral_storage: Option<PipeTargetParametersEcsTaskParametersOverridesEphemeralStorage>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionRoleARN")]
    pub execution_role_arn: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inferenceAcceleratorOverrides")]
    pub inference_accelerator_overrides: Option<Vec<PipeTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverrides>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskRoleARN")]
    pub task_role_arn: Option<String>,
}

/// The overrides that are sent to a container. An empty container override can
/// be passed in. An example of an empty container override is {"containerOverrides":
/// [ ] }. If a non-empty container override is specified, the name parameter
/// must be included.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersOverridesContainerOverrides {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeTargetParametersEcsTaskParametersOverridesContainerOverridesEnvironment>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "environmentFiles")]
    pub environment_files: Option<Vec<PipeTargetParametersEcsTaskParametersOverridesContainerOverridesEnvironmentFiles>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryReservation")]
    pub memory_reservation: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirements")]
    pub resource_requirements: Option<Vec<PipeTargetParametersEcsTaskParametersOverridesContainerOverridesResourceRequirements>>,
}

/// The environment variables to send to the container. You can add new environment
/// variables, which are added to the container at launch, or you can override
/// the existing environment variables from the Docker image or the task definition.
/// You must also specify a container name.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersOverridesContainerOverridesEnvironment {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// A list of files containing the environment variables to pass to a container.
/// You can specify up to ten environment files. The file must have a .env file
/// extension. Each line in an environment file should contain an environment
/// variable in VARIABLE=VALUE format. Lines beginning with # are treated as
/// comments and are ignored. For more information about the environment variable
/// file syntax, see Declare default environment variables in file (https://docs.docker.com/compose/env-file/).
/// 
/// 
/// If there are environment variables specified using the environment parameter
/// in a container definition, they take precedence over the variables contained
/// within an environment file. If multiple environment files are specified that
/// contain the same variable, they're processed from the top down. We recommend
/// that you use unique variable names. For more information, see Specifying
/// environment variables (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html)
/// in the Amazon Elastic Container Service Developer Guide.
/// 
/// 
/// This parameter is only supported for tasks hosted on Fargate using the following
/// platform versions:
/// 
/// 
///    * Linux platform version 1.4.0 or later.
/// 
/// 
///    * Windows platform version 1.0.0 or later.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersOverridesContainerOverridesEnvironmentFiles {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The type and amount of a resource to assign to a container. The supported
/// resource types are GPUs and Elastic Inference accelerators. For more information,
/// see Working with GPUs on Amazon ECS (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-gpu.html)
/// or Working with Amazon Elastic Inference on Amazon ECS (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-inference.html)
/// in the Amazon Elastic Container Service Developer Guide
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersOverridesContainerOverridesResourceRequirements {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The amount of ephemeral storage to allocate for the task. This parameter
/// is used to expand the total amount of ephemeral storage available, beyond
/// the default amount, for tasks hosted on Fargate. For more information, see
/// Fargate task storage (https://docs.aws.amazon.com/AmazonECS/latest/userguide/using_data_volumes.html)
/// in the Amazon ECS User Guide for Fargate.
/// 
/// 
/// This parameter is only supported for tasks hosted on Fargate using Linux
/// platform version 1.4.0 or later. This parameter is not supported for Windows
/// containers on Fargate.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersOverridesEphemeralStorage {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sizeInGiB")]
    pub size_in_gi_b: Option<i64>,
}

/// Details on an Elastic Inference accelerator task override. This parameter
/// is used to override the Elastic Inference accelerator specified in the task
/// definition. For more information, see Working with Amazon Elastic Inference
/// on Amazon ECS (https://docs.aws.amazon.com/AmazonECS/latest/userguide/ecs-inference.html)
/// in the Amazon Elastic Container Service Developer Guide.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverrides {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceName")]
    pub device_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceType")]
    pub device_type: Option<String>,
}

/// An object representing a constraint on task placement. To learn more, see
/// Task Placement Constraints (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html)
/// in the Amazon Elastic Container Service Developer Guide.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersPlacementConstraints {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub expression: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
}

/// The task placement strategy for a task or service. To learn more, see Task
/// Placement Strategies (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html)
/// in the Amazon Elastic Container Service Service Developer Guide.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersPlacementStrategy {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub field: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
}

/// A key-value pair associated with an Amazon Web Services resource. In EventBridge,
/// rules and event buses support tagging.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEcsTaskParametersTags {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The parameters for using an EventBridge event bus as a target.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersEventBridgeEventBusParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "detailType")]
    pub detail_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointID")]
    pub endpoint_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub time: Option<String>,
}

/// These are custom parameter to be used when the target is an API Gateway REST
/// APIs or EventBridge ApiDestinations.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersHttpParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<BTreeMap<String, String>>,
}

/// The parameters for using a Kinesis stream as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersKinesisStreamParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "partitionKey")]
    pub partition_key: Option<String>,
}

/// The parameters for using a Lambda function as a target.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersLambdaFunctionParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

/// These are custom parameters to be used when the target is a Amazon Redshift
/// cluster to invoke the Amazon Redshift Data API ExecuteStatement.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersRedshiftDataParameters {
    /// // Redshift Database
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub database: Option<String>,
    /// // Database user name
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dbUser")]
    pub db_user: Option<String>,
    /// // For targets, can either specify an ARN or a jsonpath pointing to the ARN.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretManagerARN")]
    pub secret_manager_arn: Option<String>,
    /// // A list of SQLs.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sqls: Option<Vec<String>>,
    /// // A name for Redshift DataAPI statement which can be used as filter of //
    /// ListStatement.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statementName")]
    pub statement_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "withEvent")]
    pub with_event: Option<bool>,
}

/// The parameters for using a SageMaker pipeline as a target.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersSageMakerPipelineParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pipelineParameterList")]
    pub pipeline_parameter_list: Option<Vec<PipeTargetParametersSageMakerPipelineParametersPipelineParameterList>>,
}

/// Name/Value pair of a parameter to start execution of a SageMaker Model Building
/// Pipeline.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersSageMakerPipelineParametersPipelineParameterList {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The parameters for using a Amazon SQS stream as a source.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersSqsQueueParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageDeduplicationID")]
    pub message_deduplication_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageGroupID")]
    pub message_group_id: Option<String>,
}

/// The parameters for using a Step Functions state machine as a target.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeTargetParametersStepFunctionStateMachineParameters {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

/// PipeStatus defines the observed state of Pipe
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeStatus {
    /// All CRs managed by ACK have a common `Status.ACKResourceMetadata` member
    /// that is used to contain resource sync state, account ownership,
    /// constructed ARN for the resource
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ackResourceMetadata")]
    pub ack_resource_metadata: Option<PipeStatusAckResourceMetadata>,
    /// All CRS managed by ACK have a common `Status.Conditions` member that
    /// contains a collection of `ackv1alpha1.Condition` objects that describe
    /// the various terminal states of the CR and its backend AWS service API
    /// resource
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Condition>>,
    /// The time the pipe was created.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "creationTime")]
    pub creation_time: Option<String>,
    /// The state the pipe is in.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "currentState")]
    pub current_state: Option<String>,
    /// When the pipe was last updated, in ISO-8601 format (https://www.w3.org/TR/NOTE-datetime)
    /// (YYYY-MM-DDThh:mm:ss.sTZD).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lastModifiedTime")]
    pub last_modified_time: Option<String>,
    /// The reason the pipe is in its current state.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stateReason")]
    pub state_reason: Option<String>,
}

/// All CRs managed by ACK have a common `Status.ACKResourceMetadata` member
/// that is used to contain resource sync state, account ownership,
/// constructed ARN for the resource
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PipeStatusAckResourceMetadata {
    /// ARN is the Amazon Resource Name for the resource. This is a
    /// globally-unique identifier and is set only by the ACK service controller
    /// once the controller has orchestrated the creation of the resource OR
    /// when it has verified that an "adopted" resource (a resource where the
    /// ARN annotation was set by the Kubernetes user on the CR) exists and
    /// matches the supplied CR's Spec field values.
    /// TODO(vijat@): Find a better strategy for resources that do not have ARN in CreateOutputResponse
    /// https://github.com/aws/aws-controllers-k8s/issues/270
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
    /// OwnerAccountID is the AWS Account ID of the account that owns the
    /// backend AWS service API resource.
    #[serde(rename = "ownerAccountID")]
    pub owner_account_id: String,
    /// Region is the AWS region in which the resource exists or will exist.
    pub region: String,
}

