// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --filename=./crd-catalog/netobserv/network-observability-operator/flows.netobserv.io/v1beta1/flowcollectors.yaml --derive=PartialEq
// kopium version: 0.16.2

use kube::CustomResource;
use serde::{Serialize, Deserialize};
use std::collections::BTreeMap;
use k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;

/// Defines the desired state of the FlowCollector resource. <br><br> *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature is not officially supported by Red Hat. It might have been, for instance, contributed by the community and accepted without a formal agreement for maintenance. The product maintainers might provide some support for these features as a best effort only.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, PartialEq)]
#[kube(group = "flows.netobserv.io", version = "v1beta1", kind = "FlowCollector", plural = "flowcollectors")]
#[kube(status = "FlowCollectorStatus")]
#[kube(schema = "disabled")]
pub struct FlowCollectorSpec {
    /// Agent configuration for flows extraction.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub agent: Option<FlowCollectorAgent>,
    /// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consolePlugin")]
    pub console_plugin: Option<FlowCollectorConsolePlugin>,
    /// `deploymentModel` defines the desired type of deployment for flow processing. Possible values are:<br> - `DIRECT` (default) to make the flow processor listening directly from the agents.<br> - `KAFKA` to make flows sent to a Kafka pipeline before consumption by the processor.<br> Kafka can provide better scalability, resiliency, and high availability (for more details, see https://www.redhat.com/en/topics/integration/what-is-apache-kafka).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deploymentModel")]
    pub deployment_model: Option<FlowCollectorDeploymentModel>,
    /// `exporters` define additional optional exporters for custom consumption or storage.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exporters: Option<Vec<FlowCollectorExporters>>,
    /// Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the `spec.deploymentModel` is `KAFKA`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kafka: Option<FlowCollectorKafka>,
    /// `loki`, the flow store, client settings.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub loki: Option<FlowCollectorLoki>,
    /// Namespace where NetObserv pods are deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub processor: Option<FlowCollectorProcessor>,
}

/// Agent configuration for flows extraction.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgent {
    /// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `EBPF`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ebpf: Option<FlowCollectorAgentEbpf>,
    /// `ipfix` [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when `spec.agent.type` is set to `IPFIX`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ipfix: Option<FlowCollectorAgentIpfix>,
    /// `type` selects the flows tracing agent. Possible values are:<br> - `EBPF` (default) to use NetObserv eBPF agent.<br> - `IPFIX` [deprecated (*)] - to use the legacy IPFIX collector.<br> `EBPF` is recommended as it offers better performances and should work regardless of the CNI installed on the cluster. `IPFIX` works with OVN-Kubernetes CNI (other CNIs could work if they support exporting IPFIX, but they would require manual configuration).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentType>,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `EBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpf {
    /// `cacheActiveTimeout` is the max period during which the reporter aggregates flows before sending. Increasing `cacheMaxFlows` and `cacheActiveTimeout` can decrease the network traffic overhead and the CPU load, however you can expect higher memory consumption and an increased latency in the flow collection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheActiveTimeout")]
    pub cache_active_timeout: Option<String>,
    /// `cacheMaxFlows` is the max number of flows in an aggregate; when reached, the reporter sends the flows. Increasing `cacheMaxFlows` and `cacheActiveTimeout` can decrease the network traffic overhead and the CPU load, however you can expect higher memory consumption and an increased latency in the flow collection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheMaxFlows")]
    pub cache_max_flows: Option<i32>,
    /// `debug` allows setting some aspects of the internal configuration of the eBPF agent. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Users setting its values do it at their own risk.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub debug: Option<FlowCollectorAgentEbpfDebug>,
    /// `excludeInterfaces` contains the interface names that are excluded from flow tracing. An entry enclosed by slashes, such as `/br-/`, is matched as a regular expression. Otherwise it is matched as a case-sensitive string.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "excludeInterfaces")]
    pub exclude_interfaces: Option<Vec<String>>,
    /// List of additional features to enable. They are all disabled by default. Enabling additional features might have performance impacts. Possible values are:<br> - `PacketDrop`: enable the packets drop flows logging feature. This feature requires mounting the kernel debug filesystem, so the eBPF pod has to run as privileged. If the `spec.agent.eBPF.privileged` parameter is not set, an error is reported.<br> - `DNSTracking`: enable the DNS tracking feature.<br> - `FlowRTT` [unsupported (*)]: enable flow latency (RTT) calculations in the eBPF agent during TCP handshakes. This feature better works with `sampling` set to 1.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub features: Option<Vec<String>>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorAgentEbpfImagePullPolicy>,
    /// `interfaces` contains the interface names from where flows are collected. If empty, the agent fetches all the interfaces in the system, excepting the ones listed in ExcludeInterfaces. An entry enclosed by slashes, such as `/br-/`, is matched as a regular expression. Otherwise it is matched as a case-sensitive string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub interfaces: Option<Vec<String>>,
    /// `kafkaBatchSize` limits the maximum size of a request in bytes before being sent to a partition. Ignored when not using Kafka. Default: 10MB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaBatchSize")]
    pub kafka_batch_size: Option<i64>,
    /// `logLevel` defines the log level for the NetObserv eBPF Agent
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorAgentEbpfLogLevel>,
    /// Privileged mode for the eBPF Agent container. In general this setting can be ignored or set to `false`: in that case, the operator sets granular capabilities (BPF, PERFMON, NET_ADMIN, SYS_RESOURCE) to the container, to enable its correct operation. If for some reason these capabilities cannot be set, such as if an old kernel version not knowing CAP_BPF is in use, then you can turn on this mode for more global privileges.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorAgentEbpfResources>,
    /// Sampling rate of the flow reporter. 100 means one flow on 100 is sent. 0 or 1 means all flows are sampled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sampling: Option<i32>,
}

/// `debug` allows setting some aspects of the internal configuration of the eBPF agent. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Users setting its values do it at their own risk.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfDebug {
    /// `env` allows passing custom environment variables to underlying components. Useful for passing some very concrete performance-tuning options, such as `GOGC` and `GOMAXPROCS`, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<BTreeMap<String, String>>,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `EBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `EBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container. 
    ///  This is an alpha field and requires enabling the DynamicResourceAllocation feature gate. 
    ///  This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorAgentEbpfResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container.
    pub name: String,
}

/// `ipfix` [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when `spec.agent.type` is set to `IPFIX`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentIpfix {
    /// `cacheActiveTimeout` is the max period during which the reporter aggregates flows before sending.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheActiveTimeout")]
    pub cache_active_timeout: Option<String>,
    /// `cacheMaxFlows` is the max number of flows in an aggregate; when reached, the reporter sends the flows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheMaxFlows")]
    pub cache_max_flows: Option<i32>,
    /// `clusterNetworkOperator` defines the settings related to the OpenShift Cluster Network Operator, when available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterNetworkOperator")]
    pub cluster_network_operator: Option<FlowCollectorAgentIpfixClusterNetworkOperator>,
    /// `forceSampleAll` allows disabling sampling in the IPFIX-based flow reporter. It is not recommended to sample all the traffic with IPFIX, as it might generate cluster instability. If you REALLY want to do that, set this flag to `true`. Use at your own risk. When it is set to `true`, the value of `sampling` is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "forceSampleAll")]
    pub force_sample_all: Option<bool>,
    /// `ovnKubernetes` defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN's IPFIX exports, without OpenShift. When using OpenShift, refer to the `clusterNetworkOperator` property instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ovnKubernetes")]
    pub ovn_kubernetes: Option<FlowCollectorAgentIpfixOvnKubernetes>,
    /// `sampling` is the sampling rate on the reporter. 100 means one flow on 100 is sent. To ensure cluster stability, it is not possible to set a value below 2. If you really want to sample every packet, which might impact the cluster stability, refer to `forceSampleAll`. Alternatively, you can use the eBPF Agent instead of IPFIX.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sampling: Option<i32>,
}

/// `clusterNetworkOperator` defines the settings related to the OpenShift Cluster Network Operator, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentIpfixClusterNetworkOperator {
    /// Namespace  where the config map is going to be deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// `ovnKubernetes` defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN's IPFIX exports, without OpenShift. When using OpenShift, refer to the `clusterNetworkOperator` property instead.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentIpfixOvnKubernetes {
    /// `containerName` defines the name of the container to configure for IPFIX.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// `daemonSetName` defines the name of the DaemonSet controlling the OVN-Kubernetes pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "daemonSetName")]
    pub daemon_set_name: Option<String>,
    /// Namespace where OVN-Kubernetes pods are deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// Agent configuration for flows extraction.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentType {
    #[serde(rename = "EBPF")]
    Ebpf,
    #[serde(rename = "IPFIX")]
    Ipfix,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePlugin {
    /// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub autoscaler: Option<FlowCollectorConsolePluginAutoscaler>,
    /// Enables the console plugin deployment. `spec.Loki.enable` must also be `true`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorConsolePluginImagePullPolicy>,
    /// `logLevel` for the console plugin backend
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorConsolePluginLogLevel>,
    /// `port` is the plugin service port. Do not use 9002, which is reserved for metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// `portNaming` defines the configuration of the port-to-service name translation
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portNaming")]
    pub port_naming: Option<FlowCollectorConsolePluginPortNaming>,
    /// `quickFilters` configures quick filter presets for the Console plugin
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "quickFilters")]
    pub quick_filters: Option<Vec<FlowCollectorConsolePluginQuickFilters>>,
    /// `register` allows, when set to `true`, to automatically register the provided console plugin with the OpenShift Console operator. When set to `false`, you can still register it manually by editing console.operator.openshift.io/cluster with the following command: `oc patch console.operator.openshift.io cluster --type='json' -p '[{"op": "add", "path": "/spec/plugins/-", "value": "netobserv-plugin"}]'`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub register: Option<bool>,
    /// `replicas` defines the number of replicas (pods) to start.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub replicas: Option<i32>,
    /// `resources`, in terms of compute resources, required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorConsolePluginResources>,
}

/// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscaler {
    /// `maxReplicas` is the upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than MinReplicas.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxReplicas")]
    pub max_replicas: Option<i32>,
    /// Metrics used by the pod autoscaler
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<FlowCollectorConsolePluginAutoscalerMetrics>>,
    /// `minReplicas` is the lower limit for the number of replicas to which the autoscaler can scale down. It defaults to 1 pod. minReplicas is allowed to be 0 if the alpha feature gate HPAScaleToZero is enabled and at least one Object or External metric is configured. Scaling is active as long as at least one metric value is available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minReplicas")]
    pub min_replicas: Option<i32>,
    /// `status` describes the desired status regarding deploying an horizontal pod autoscaler.<br> - `DISABLED` does not deploy an horizontal pod autoscaler.<br> - `ENABLED` deploys an horizontal pod autoscaler.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub status: Option<FlowCollectorConsolePluginAutoscalerStatus>,
}

/// MetricSpec specifies how to scale based on a single metric (only `type` and one other matching field should be set at once).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetrics {
    /// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerResource")]
    pub container_resource: Option<FlowCollectorConsolePluginAutoscalerMetricsContainerResource>,
    /// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub external: Option<FlowCollectorConsolePluginAutoscalerMetricsExternal>,
    /// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub object: Option<FlowCollectorConsolePluginAutoscalerMetricsObject>,
    /// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pods: Option<FlowCollectorConsolePluginAutoscalerMetricsPods>,
    /// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resource: Option<FlowCollectorConsolePluginAutoscalerMetricsResource>,
    /// type is the type of metric source.  It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object. Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
    #[serde(rename = "type")]
    pub r#type: String,
}

/// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsContainerResource {
    /// container is the name of the container in the pods of the scaling target
    pub container: String,
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsContainerResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsContainerResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternal {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsExternalMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsExternalTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObject {
    /// describedObject specifies the descriptions of a object,such as kind,name apiVersion
    #[serde(rename = "describedObject")]
    pub described_object: FlowCollectorConsolePluginAutoscalerMetricsObjectDescribedObject,
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsObjectMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsObjectTarget,
}

/// describedObject specifies the descriptions of a object,such as kind,name apiVersion
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectDescribedObject {
    /// apiVersion is the API version of the referent
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// kind is the kind of the referent; More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
    pub kind: String,
    /// name is the name of the referent; More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPods {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsPodsMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsPodsTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsResource {
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginAutoscalerStatus {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "ENABLED")]
    Enabled,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `portNaming` defines the configuration of the port-to-service name translation
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginPortNaming {
    /// Enable the console plugin port-to-service name translation
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `portNames` defines additional port names to use in the console, for example, `portNames: {"3100": "loki"}`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portNames")]
    pub port_names: Option<BTreeMap<String, String>>,
}

/// `QuickFilter` defines preset configuration for Console's quick filters
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginQuickFilters {
    /// `default` defines whether this filter should be active by default or not
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub default: Option<bool>,
    /// `filter` is a set of keys and values to be set when this filter is selected. Each key can relate to a list of values using a coma-separated string, for example, `filter: {"src_namespace": "namespace1,namespace2"}`.
    pub filter: BTreeMap<String, String>,
    /// Name of the filter, that is displayed in the Console
    pub name: String,
}

/// `resources`, in terms of compute resources, required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container. 
    ///  This is an alpha field and requires enabling the DynamicResourceAllocation feature gate. 
    ///  This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorConsolePluginResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container.
    pub name: String,
}

/// Defines the desired state of the FlowCollector resource. <br><br> *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature is not officially supported by Red Hat. It might have been, for instance, contributed by the community and accepted without a formal agreement for maintenance. The product maintainers might provide some support for these features as a best effort only.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorDeploymentModel {
    #[serde(rename = "DIRECT")]
    Direct,
    #[serde(rename = "KAFKA")]
    Kafka,
}

/// `FlowCollectorExporter` defines an additional exporter to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExporters {
    /// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ipfix: Option<FlowCollectorExportersIpfix>,
    /// Kafka configuration, such as the address and topic, to send enriched flows to.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kafka: Option<FlowCollectorExportersKafka>,
    /// `type` selects the type of exporters. The available options are `KAFKA` and `IPFIX`.
    #[serde(rename = "type")]
    pub r#type: FlowCollectorExportersType,
}

/// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersIpfix {
    /// Address of the IPFIX external receiver
    #[serde(rename = "targetHost")]
    pub target_host: String,
    /// Port for the IPFIX external receiver
    #[serde(rename = "targetPort")]
    pub target_port: i64,
    /// Transport protocol (`TCP` or `UDP`) to be used for the IPFIX connection, defaults to `TCP`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub transport: Option<FlowCollectorExportersIpfixTransport>,
}

/// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersIpfixTransport {
    #[serde(rename = "TCP")]
    Tcp,
    #[serde(rename = "UDP")]
    Udp,
}

/// Kafka configuration, such as the address and topic, to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafka {
    /// Address of the Kafka server
    pub address: String,
    /// SASL authentication configuration. [Unsupported (*)].
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl: Option<FlowCollectorExportersKafkaSasl>,
    /// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorExportersKafkaTls>,
    /// Kafka topic to use. It must exist. NetObserv does not create it.
    pub topic: String,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaSasl {
    /// Reference to the secret or config map containing the client ID
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientIDReference")]
    pub client_id_reference: Option<FlowCollectorExportersKafkaSaslClientIdReference>,
    /// Reference to the secret or config map containing the client secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientSecretReference")]
    pub client_secret_reference: Option<FlowCollectorExportersKafkaSaslClientSecretReference>,
    /// Type of SASL authentication to use, or `DISABLED` if SASL is not used
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaSaslClientIdReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslClientIdReferenceType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslClientIdReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaSaslClientSecretReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslClientSecretReferenceType>,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslClientSecretReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslType {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "PLAIN")]
    Plain,
    #[serde(rename = "SCRAM-SHA512")]
    ScramSha512,
}

/// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorExportersKafkaTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorExportersKafkaTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `FlowCollectorExporter` defines an additional exporter to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersType {
    #[serde(rename = "KAFKA")]
    Kafka,
    #[serde(rename = "IPFIX")]
    Ipfix,
}

/// Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the `spec.deploymentModel` is `KAFKA`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafka {
    /// Address of the Kafka server
    pub address: String,
    /// SASL authentication configuration. [Unsupported (*)].
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl: Option<FlowCollectorKafkaSasl>,
    /// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorKafkaTls>,
    /// Kafka topic to use. It must exist. NetObserv does not create it.
    pub topic: String,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaSasl {
    /// Reference to the secret or config map containing the client ID
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientIDReference")]
    pub client_id_reference: Option<FlowCollectorKafkaSaslClientIdReference>,
    /// Reference to the secret or config map containing the client secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientSecretReference")]
    pub client_secret_reference: Option<FlowCollectorKafkaSaslClientSecretReference>,
    /// Type of SASL authentication to use, or `DISABLED` if SASL is not used
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaSaslClientIdReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslClientIdReferenceType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslClientIdReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaSaslClientSecretReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslClientSecretReferenceType>,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslClientSecretReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslType {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "PLAIN")]
    Plain,
    #[serde(rename = "SCRAM-SHA512")]
    ScramSha512,
}

/// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorKafkaTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorKafkaTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `loki`, the flow store, client settings.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLoki {
    /// `authToken` describes the way to get a token to authenticate to Loki.<br> - `DISABLED` does not send any token with the request.<br> - `FORWARD` forwards the user token for authorization.<br> - `HOST` [deprecated (*)] - uses the local pod service account to authenticate to Loki.<br> When using the Loki Operator, this must be set to `FORWARD`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "authToken")]
    pub auth_token: Option<FlowCollectorLokiAuthToken>,
    /// `batchSize` is the maximum batch size (in bytes) of logs to accumulate before sending.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    /// `batchWait` is the maximum time to wait before sending a batch.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchWait")]
    pub batch_wait: Option<String>,
    /// Set `enable` to `true` to store flows in Loki. It is required for the OpenShift Console plugin installation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `maxBackoff` is the maximum backoff time for client connection between retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxBackoff")]
    pub max_backoff: Option<String>,
    /// `maxRetries` is the maximum number of retries for client connections.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxRetries")]
    pub max_retries: Option<i32>,
    /// `minBackoff` is the initial backoff time for client connection between retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minBackoff")]
    pub min_backoff: Option<String>,
    /// `querierURL` specifies the address of the Loki querier service, in case it is different from the Loki ingester URL. If empty, the URL value is used (assuming that the Loki ingester and querier are in the same server). When using the Loki Operator, do not set it, since ingestion and queries use the Loki gateway.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "querierUrl")]
    pub querier_url: Option<String>,
    /// `staticLabels` is a map of common labels to set on each flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "staticLabels")]
    pub static_labels: Option<BTreeMap<String, String>>,
    /// TLS client configuration for Loki status URL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusTls")]
    pub status_tls: Option<FlowCollectorLokiStatusTls>,
    /// `statusURL` specifies the address of the Loki `/ready`, `/metrics` and `/config` endpoints, in case it is different from the Loki querier URL. If empty, the `querierURL` value is used. This is useful to show error messages and some context in the frontend. When using the Loki Operator, set it to the Loki HTTP query frontend service, for example https://loki-query-frontend-http.netobserv.svc:3100/. `statusTLS` configuration is used when `statusUrl` is set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusUrl")]
    pub status_url: Option<String>,
    /// `tenantID` is the Loki `X-Scope-OrgID` that identifies the tenant for each request. When using the Loki Operator, set it to `network`, which corresponds to a special tenant mode.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tenantID")]
    pub tenant_id: Option<String>,
    /// `timeout` is the maximum time connection / request limit. A timeout of zero means no timeout.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timeout: Option<String>,
    /// TLS client configuration for Loki URL.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorLokiTls>,
    /// `url` is the address of an existing Loki service to push the flows to. When using the Loki Operator, set it to the Loki gateway service with the `network` tenant set in path, for example https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub url: Option<String>,
}

/// `loki`, the flow store, client settings.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiAuthToken {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "HOST")]
    Host,
    #[serde(rename = "FORWARD")]
    Forward,
}

/// TLS client configuration for Loki status URL.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiStatusTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiStatusTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiStatusTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiStatusTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiStatusTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiStatusTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiStatusTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiStatusTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiStatusTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS client configuration for Loki URL.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessor {
    /// `clusterName` is the name of the cluster to appear in the flows data. This is useful in a multi-cluster context. When using OpenShift, leave empty to make it automatically determined.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    pub cluster_name: Option<String>,
    /// `conversationEndTimeout` is the time to wait after a network flow is received, to consider the conversation ended. This delay is ignored when a FIN packet is collected for TCP flows (see `conversationTerminatingTimeout` instead).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationEndTimeout")]
    pub conversation_end_timeout: Option<String>,
    /// `conversationHeartbeatInterval` is the time to wait between "tick" events of a conversation
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationHeartbeatInterval")]
    pub conversation_heartbeat_interval: Option<String>,
    /// `conversationTerminatingTimeout` is the time to wait from detected FIN flag to end a conversation. Only relevant for TCP flows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationTerminatingTimeout")]
    pub conversation_terminating_timeout: Option<String>,
    /// `debug` allows setting some aspects of the internal configuration of the flow processor. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Users setting its values do it at their own risk.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub debug: Option<FlowCollectorProcessorDebug>,
    /// `dropUnusedFields` allows, when set to `true`, to drop fields that are known to be unused by OVS, to save storage space.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dropUnusedFields")]
    pub drop_unused_fields: Option<bool>,
    /// `enableKubeProbes` is a flag to enable or disable Kubernetes liveness and readiness probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKubeProbes")]
    pub enable_kube_probes: Option<bool>,
    /// `healthPort` is a collector HTTP port in the Pod that exposes the health check API
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "healthPort")]
    pub health_port: Option<i32>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorProcessorImagePullPolicy>,
    /// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerAutoscaler")]
    pub kafka_consumer_autoscaler: Option<FlowCollectorProcessorKafkaConsumerAutoscaler>,
    /// `kafkaConsumerBatchSize` indicates to the broker the maximum batch size, in bytes, that the consumer accepts. Ignored when not using Kafka. Default: 10MB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerBatchSize")]
    pub kafka_consumer_batch_size: Option<i64>,
    /// `kafkaConsumerQueueCapacity` defines the capacity of the internal message queue used in the Kafka consumer client. Ignored when not using Kafka.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerQueueCapacity")]
    pub kafka_consumer_queue_capacity: Option<i64>,
    /// `kafkaConsumerReplicas` defines the number of replicas (pods) to start for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerReplicas")]
    pub kafka_consumer_replicas: Option<i32>,
    /// `logLevel` of the processor runtime
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorProcessorLogLevel>,
    /// `logTypes` defines the desired record types to generate. Possible values are:<br> - `FLOWS` (default) to export regular network flows<br> - `CONVERSATIONS` to generate events for started conversations, ended conversations as well as periodic "tick" updates<br> - `ENDED_CONVERSATIONS` to generate only ended conversations events<br> - `ALL` to generate both network flows and all conversations events<br>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logTypes")]
    pub log_types: Option<FlowCollectorProcessorLogTypes>,
    /// `Metrics` define the processor configuration regarding metrics
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<FlowCollectorProcessorMetrics>,
    /// Set `multiClusterDeployment` to `true` to enable multi clusters feature. This will add clusterName label to flows data
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "multiClusterDeployment")]
    pub multi_cluster_deployment: Option<bool>,
    /// Port of the flow collector (host port). By convention, some values are forbidden. It must be greater than 1024 and different from 4500, 4789 and 6081.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// `profilePort` allows setting up a Go pprof profiler listening to this port
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "profilePort")]
    pub profile_port: Option<i32>,
    /// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorProcessorResources>,
}

/// `debug` allows setting some aspects of the internal configuration of the flow processor. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Users setting its values do it at their own risk.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorDebug {
    /// `env` allows passing custom environment variables to underlying components. Useful for passing some very concrete performance-tuning options, such as `GOGC` and `GOMAXPROCS`, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<BTreeMap<String, String>>,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscaler {
    /// `maxReplicas` is the upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than MinReplicas.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxReplicas")]
    pub max_replicas: Option<i32>,
    /// Metrics used by the pod autoscaler
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetrics>>,
    /// `minReplicas` is the lower limit for the number of replicas to which the autoscaler can scale down. It defaults to 1 pod. minReplicas is allowed to be 0 if the alpha feature gate HPAScaleToZero is enabled and at least one Object or External metric is configured. Scaling is active as long as at least one metric value is available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minReplicas")]
    pub min_replicas: Option<i32>,
    /// `status` describes the desired status regarding deploying an horizontal pod autoscaler.<br> - `DISABLED` does not deploy an horizontal pod autoscaler.<br> - `ENABLED` deploys an horizontal pod autoscaler.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub status: Option<FlowCollectorProcessorKafkaConsumerAutoscalerStatus>,
}

/// MetricSpec specifies how to scale based on a single metric (only `type` and one other matching field should be set at once).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetrics {
    /// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerResource")]
    pub container_resource: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResource>,
    /// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub external: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternal>,
    /// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub object: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObject>,
    /// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pods: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPods>,
    /// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resource: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResource>,
    /// type is the type of metric source.  It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object. Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
    #[serde(rename = "type")]
    pub r#type: String,
}

/// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResource {
    /// container is the name of the container in the pods of the scaling target
    pub container: String,
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternal {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObject {
    /// describedObject specifies the descriptions of a object,such as kind,name apiVersion
    #[serde(rename = "describedObject")]
    pub described_object: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectDescribedObject,
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectTarget,
}

/// describedObject specifies the descriptions of a object,such as kind,name apiVersion
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectDescribedObject {
    /// apiVersion is the API version of the referent
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// kind is the kind of the referent; More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
    pub kind: String,
    /// name is the name of the referent; More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPods {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResource {
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorKafkaConsumerAutoscalerStatus {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "ENABLED")]
    Enabled,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorLogTypes {
    #[serde(rename = "FLOWS")]
    Flows,
    #[serde(rename = "CONVERSATIONS")]
    Conversations,
    #[serde(rename = "ENDED_CONVERSATIONS")]
    EndedConversations,
    #[serde(rename = "ALL")]
    All,
}

/// `Metrics` define the processor configuration regarding metrics
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetrics {
    /// `disableAlerts` is a list of alerts that should be disabled. Possible values are:<br> `NetObservNoFlows`, which is triggered when no flows are being observed for a certain period.<br> `NetObservLokiError`, which is triggered when flows are being dropped due to Loki errors.<br>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "disableAlerts")]
    pub disable_alerts: Option<Vec<String>>,
    /// `ignoreTags` [deprecated (*)] is a list of tags to specify which metrics to ignore. Each metric is associated with a list of tags. More details in https://github.com/netobserv/network-observability-operator/tree/main/controllers/flowlogspipeline/metrics_definitions . Available tags are: `egress`, `ingress`, `flows`, `bytes`, `packets`, `namespaces`, `nodes`, `workloads`, `nodes-flows`, `namespaces-flows`, `workloads-flows`. Namespace-based metrics are covered by both `workloads` and `namespaces` tags, hence it is recommended to always ignore one of them (`workloads` offering a finer granularity).<br> Deprecation notice: use `includeList` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ignoreTags")]
    pub ignore_tags: Option<Vec<String>>,
    /// `includeList` is a list of metric names to specify which ones to generate. The names correspond to the names in Prometheus without the prefix. For example, `namespace_egress_packets_total` will show up as `netobserv_namespace_egress_packets_total` in Prometheus. Note that the more metrics you add, the bigger is the impact on Prometheus workload resources. Metrics enabled by default are: `namespace_flows_total`, `node_ingress_bytes_total`, `workload_ingress_bytes_total`, `namespace_drop_packets_total` (when `PacketDrop` feature is enabled), `namespace_rtt_seconds` (when `FlowRTT` feature is enabled), `namespace_dns_latency_seconds` (when `DNSTracking` feature is enabled). More information, with full list of available metrics: https://github.com/netobserv/network-observability-operator/blob/main/docs/Metrics.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeList")]
    pub include_list: Option<Vec<String>>,
    /// Metrics server endpoint configuration for Prometheus scraper
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<FlowCollectorProcessorMetricsServer>,
}

/// Metrics server endpoint configuration for Prometheus scraper
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServer {
    /// The prometheus HTTP port
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// TLS configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorProcessorMetricsServerTls>,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTls {
    /// `insecureSkipVerify` allows skipping client-side verification of the provided certificate. If set to `true`, the `providedCaFile` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// TLS configuration when `type` is set to `PROVIDED`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub provided: Option<FlowCollectorProcessorMetricsServerTlsProvided>,
    /// Reference to the CA file when `type` is set to `PROVIDED`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providedCaFile")]
    pub provided_ca_file: Option<FlowCollectorProcessorMetricsServerTlsProvidedCaFile>,
    /// Select the type of TLS configuration:<br> - `DISABLED` (default) to not configure TLS for the endpoint. - `PROVIDED` to manually provide cert file and a key file. - `AUTO` to use OpenShift auto generated certificate using annotations.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsType>,
}

/// TLS configuration when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTlsProvided {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsProvidedType>,
}

/// TLS configuration when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsProvidedType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the CA file when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTlsProvidedCaFile {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsProvidedCaFileType>,
}

/// Reference to the CA file when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsProvidedCaFileType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsType {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "PROVIDED")]
    Provided,
    #[serde(rename = "AUTO")]
    Auto,
}

/// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container. 
    ///  This is an alpha field and requires enabling the DynamicResourceAllocation feature gate. 
    ///  This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorProcessorResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container.
    pub name: String,
}

/// `FlowCollectorStatus` defines the observed state of FlowCollector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorStatus {
    /// `conditions` represent the latest available observations of an object's state
    pub conditions: Vec<FlowCollectorStatusConditions>,
    /// Namespace where console plugin and flowlogs-pipeline have been deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions.  For example, 
///  	type FooStatus struct{ 	    // Represents the observations of a foo's current state. 	    // Known .status.conditions.type are: "Available", "Progressing", and "Degraded" 	    // +patchMergeKey=type 	    // +patchStrategy=merge 	    // +listType=map 	    // +listMapKey=type 	    Conditions []metav1.Condition `json:"conditions,omitempty" patchStrategy:"merge" patchMergeKey:"type" protobuf:"bytes,1,rep,name=conditions"` 
///  	    // other fields 	}
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorStatusConditions {
    /// lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable.
    #[serde(rename = "lastTransitionTime")]
    pub last_transition_time: String,
    /// message is a human readable message indicating details about the transition. This may be an empty string.
    pub message: String,
    /// observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    pub observed_generation: Option<i64>,
    /// reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty.
    pub reason: String,
    /// status of the condition, one of True, False, Unknown.
    pub status: FlowCollectorStatusConditionsStatus,
    /// type of condition in CamelCase or in foo.example.com/CamelCase. --- Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt)
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions.  For example, 
///  	type FooStatus struct{ 	    // Represents the observations of a foo's current state. 	    // Known .status.conditions.type are: "Available", "Progressing", and "Degraded" 	    // +patchMergeKey=type 	    // +patchStrategy=merge 	    // +listType=map 	    // +listMapKey=type 	    Conditions []metav1.Condition `json:"conditions,omitempty" patchStrategy:"merge" patchMergeKey:"type" protobuf:"bytes,1,rep,name=conditions"` 
///  	    // other fields 	}
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorStatusConditionsStatus {
    True,
    False,
    Unknown,
}

