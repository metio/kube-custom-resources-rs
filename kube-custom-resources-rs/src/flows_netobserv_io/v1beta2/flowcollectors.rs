// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --filename=./crd-catalog/netobserv/network-observability-operator/flows.netobserv.io/v1beta2/flowcollectors.yaml --derive=PartialEq
// kopium version: 0.16.5

use kube::CustomResource;
use serde::{Serialize, Deserialize};
use std::collections::BTreeMap;
use k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;

/// Defines the desired state of the FlowCollector resource. <br><br> *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature is not officially supported by Red Hat. It might have been, for example, contributed by the community and accepted without a formal agreement for maintenance. The product maintainers might provide some support for these features as a best effort only.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, PartialEq)]
#[kube(group = "flows.netobserv.io", version = "v1beta2", kind = "FlowCollector", plural = "flowcollectors")]
#[kube(status = "FlowCollectorStatus")]
#[kube(schema = "disabled")]
pub struct FlowCollectorSpec {
    /// Agent configuration for flows extraction.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub agent: Option<FlowCollectorAgent>,
    /// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consolePlugin")]
    pub console_plugin: Option<FlowCollectorConsolePlugin>,
    /// `deploymentModel` defines the desired type of deployment for flow processing. Possible values are:<br> - `Direct` (default) to make the flow processor listening directly from the agents.<br> - `Kafka` to make flows sent to a Kafka pipeline before consumption by the processor.<br> Kafka can provide better scalability, resiliency, and high availability (for more details, see https://www.redhat.com/en/topics/integration/what-is-apache-kafka).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deploymentModel")]
    pub deployment_model: Option<FlowCollectorDeploymentModel>,
    /// `exporters` define additional optional exporters for custom consumption or storage.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exporters: Option<Vec<FlowCollectorExporters>>,
    /// Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the `spec.deploymentModel` is `Kafka`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kafka: Option<FlowCollectorKafka>,
    /// `loki`, the flow store, client settings.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub loki: Option<FlowCollectorLoki>,
    /// Namespace where NetObserv pods are deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub processor: Option<FlowCollectorProcessor>,
}

/// Agent configuration for flows extraction.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgent {
    /// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `eBPF`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ebpf: Option<FlowCollectorAgentEbpf>,
    /// `ipfix` [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when `spec.agent.type` is set to `IPFIX`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ipfix: Option<FlowCollectorAgentIpfix>,
    /// `type` selects the flows tracing agent. Possible values are:<br> - `eBPF` (default) to use NetObserv eBPF agent.<br> - `IPFIX` [deprecated (*)] - to use the legacy IPFIX collector.<br> `eBPF` is recommended as it offers better performances and should work regardless of the CNI installed on the cluster. `IPFIX` works with OVN-Kubernetes CNI (other CNIs could work if they support exporting IPFIX, but they would require manual configuration).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentType>,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `eBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpf {
    /// `advanced` allows setting some aspects of the internal configuration of the eBPF agent. This section is aimed mostly for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub advanced: Option<FlowCollectorAgentEbpfAdvanced>,
    /// `cacheActiveTimeout` is the max period during which the reporter aggregates flows before sending. Increasing `cacheMaxFlows` and `cacheActiveTimeout` can decrease the network traffic overhead and the CPU load, however you can expect higher memory consumption and an increased latency in the flow collection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheActiveTimeout")]
    pub cache_active_timeout: Option<String>,
    /// `cacheMaxFlows` is the max number of flows in an aggregate; when reached, the reporter sends the flows. Increasing `cacheMaxFlows` and `cacheActiveTimeout` can decrease the network traffic overhead and the CPU load, however you can expect higher memory consumption and an increased latency in the flow collection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheMaxFlows")]
    pub cache_max_flows: Option<i32>,
    /// `excludeInterfaces` contains the interface names that are excluded from flow tracing. An entry enclosed by slashes, such as `/br-/`, is matched as a regular expression. Otherwise it is matched as a case-sensitive string.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "excludeInterfaces")]
    pub exclude_interfaces: Option<Vec<String>>,
    /// List of additional features to enable. They are all disabled by default. Enabling additional features might have performance impacts. Possible values are:<br> - `PacketDrop`: enable the packets drop flows logging feature. This feature requires mounting the kernel debug filesystem, so the eBPF pod has to run as privileged. If the `spec.agent.ebpf.privileged` parameter is not set, an error is reported.<br> - `DNSTracking`: enable the DNS tracking feature.<br> - `FlowRTT`: enable flow latency (RTT) calculations in the eBPF agent during TCP handshakes. This feature better works with `sampling` set to 1.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub features: Option<Vec<String>>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorAgentEbpfImagePullPolicy>,
    /// `interfaces` contains the interface names from where flows are collected. If empty, the agent fetches all the interfaces in the system, excepting the ones listed in ExcludeInterfaces. An entry enclosed by slashes, such as `/br-/`, is matched as a regular expression. Otherwise it is matched as a case-sensitive string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub interfaces: Option<Vec<String>>,
    /// `kafkaBatchSize` limits the maximum size of a request in bytes before being sent to a partition. Ignored when not using Kafka. Default: 1MB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaBatchSize")]
    pub kafka_batch_size: Option<i64>,
    /// `logLevel` defines the log level for the NetObserv eBPF Agent
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorAgentEbpfLogLevel>,
    /// `metrics` defines the eBPF agent configuration regarding metrics
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<FlowCollectorAgentEbpfMetrics>,
    /// Privileged mode for the eBPF Agent container. When ignored or set to `false`, the operator sets granular capabilities (BPF, PERFMON, NET_ADMIN, SYS_RESOURCE) to the container. If for some reason these capabilities cannot be set, such as if an old kernel version not knowing CAP_BPF is in use, then you can turn on this mode for more global privileges. Some agent features require the privileged mode, such as packet drops tracking (see `features`) and SR-IOV support.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorAgentEbpfResources>,
    /// Sampling rate of the flow reporter. 100 means one flow on 100 is sent. 0 or 1 means all flows are sampled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sampling: Option<i32>,
}

/// `advanced` allows setting some aspects of the internal configuration of the eBPF agent. This section is aimed mostly for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfAdvanced {
    /// `env` allows passing custom environment variables to underlying components. Useful for passing some very concrete performance-tuning options, such as `GOGC` and `GOMAXPROCS`, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<BTreeMap<String, String>>,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `eBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type` is set to `eBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `metrics` defines the eBPF agent configuration regarding metrics
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfMetrics {
    /// Set `enable` to `true` to enable eBPF agent metrics collection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// Metrics server endpoint configuration for Prometheus scraper
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<FlowCollectorAgentEbpfMetricsServer>,
}

/// Metrics server endpoint configuration for Prometheus scraper
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServer {
    /// The prometheus HTTP port
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// TLS configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorAgentEbpfMetricsServerTls>,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServerTls {
    /// `insecureSkipVerify` allows skipping client-side verification of the provided certificate. If set to `true`, the `providedCaFile` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// TLS configuration when `type` is set to `Provided`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub provided: Option<FlowCollectorAgentEbpfMetricsServerTlsProvided>,
    /// Reference to the CA file when `type` is set to `Provided`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providedCaFile")]
    pub provided_ca_file: Option<FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFile>,
    /// Select the type of TLS configuration:<br> - `Disabled` (default) to not configure TLS for the endpoint. - `Provided` to manually provide cert file and a key file. - `Auto` to use OpenShift auto generated certificate using annotations.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentEbpfMetricsServerTlsType>,
}

/// TLS configuration when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServerTlsProvided {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentEbpfMetricsServerTlsProvidedType>,
}

/// TLS configuration when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfMetricsServerTlsProvidedType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the CA file when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFile {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFileType>,
}

/// Reference to the CA file when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFileType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfMetricsServerTlsType {
    Disabled,
    Provided,
    Auto,
}

/// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container. 
    ///  This is an alpha field and requires enabling the DynamicResourceAllocation feature gate. 
    ///  This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorAgentEbpfResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container.
    pub name: String,
}

/// `ipfix` [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when `spec.agent.type` is set to `IPFIX`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentIpfix {
    /// `cacheActiveTimeout` is the max period during which the reporter aggregates flows before sending.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheActiveTimeout")]
    pub cache_active_timeout: Option<String>,
    /// `cacheMaxFlows` is the max number of flows in an aggregate; when reached, the reporter sends the flows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheMaxFlows")]
    pub cache_max_flows: Option<i32>,
    /// `clusterNetworkOperator` defines the settings related to the OpenShift Cluster Network Operator, when available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterNetworkOperator")]
    pub cluster_network_operator: Option<FlowCollectorAgentIpfixClusterNetworkOperator>,
    /// `forceSampleAll` allows disabling sampling in the IPFIX-based flow reporter. It is not recommended to sample all the traffic with IPFIX, as it might generate cluster instability. If you REALLY want to do that, set this flag to `true`. Use at your own risk. When it is set to `true`, the value of `sampling` is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "forceSampleAll")]
    pub force_sample_all: Option<bool>,
    /// `ovnKubernetes` defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN's IPFIX exports, without OpenShift. When using OpenShift, refer to the `clusterNetworkOperator` property instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ovnKubernetes")]
    pub ovn_kubernetes: Option<FlowCollectorAgentIpfixOvnKubernetes>,
    /// `sampling` is the sampling rate on the reporter. 100 means one flow on 100 is sent. To ensure cluster stability, it is not possible to set a value below 2. If you really want to sample every packet, which might impact the cluster stability, refer to `forceSampleAll`. Alternatively, you can use the eBPF Agent instead of IPFIX.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sampling: Option<i32>,
}

/// `clusterNetworkOperator` defines the settings related to the OpenShift Cluster Network Operator, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentIpfixClusterNetworkOperator {
    /// Namespace  where the config map is going to be deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// `ovnKubernetes` defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN's IPFIX exports, without OpenShift. When using OpenShift, refer to the `clusterNetworkOperator` property instead.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentIpfixOvnKubernetes {
    /// `containerName` defines the name of the container to configure for IPFIX.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// `daemonSetName` defines the name of the DaemonSet controlling the OVN-Kubernetes pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "daemonSetName")]
    pub daemon_set_name: Option<String>,
    /// Namespace where OVN-Kubernetes pods are deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// Agent configuration for flows extraction.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentType {
    #[serde(rename = "eBPF")]
    EBpf,
    #[serde(rename = "IPFIX")]
    Ipfix,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePlugin {
    /// `advanced` allows setting some aspects of the internal configuration of the console plugin. This section is aimed mostly for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub advanced: Option<FlowCollectorConsolePluginAdvanced>,
    /// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub autoscaler: Option<FlowCollectorConsolePluginAutoscaler>,
    /// Enables the console plugin deployment. `spec.loki.enable` must also be `true`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorConsolePluginImagePullPolicy>,
    /// `logLevel` for the console plugin backend
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorConsolePluginLogLevel>,
    /// `portNaming` defines the configuration of the port-to-service name translation
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portNaming")]
    pub port_naming: Option<FlowCollectorConsolePluginPortNaming>,
    /// `quickFilters` configures quick filter presets for the Console plugin
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "quickFilters")]
    pub quick_filters: Option<Vec<FlowCollectorConsolePluginQuickFilters>>,
    /// `replicas` defines the number of replicas (pods) to start.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub replicas: Option<i32>,
    /// `resources`, in terms of compute resources, required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorConsolePluginResources>,
}

/// `advanced` allows setting some aspects of the internal configuration of the console plugin. This section is aimed mostly for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAdvanced {
    /// `args` allows passing custom arguments to underlying components. Useful for overriding some parameters, such as an url or a configuration path, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// `env` allows passing custom environment variables to underlying components. Useful for passing some very concrete performance-tuning options, such as `GOGC` and `GOMAXPROCS`, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<BTreeMap<String, String>>,
    /// `port` is the plugin service port. Do not use 9002, which is reserved for metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// `register` allows, when set to `true`, to automatically register the provided console plugin with the OpenShift Console operator. When set to `false`, you can still register it manually by editing console.operator.openshift.io/cluster with the following command: `oc patch console.operator.openshift.io cluster --type='json' -p '[{"op": "add", "path": "/spec/plugins/-", "value": "netobserv-plugin"}]'`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub register: Option<bool>,
}

/// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscaler {
    /// `maxReplicas` is the upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than MinReplicas.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxReplicas")]
    pub max_replicas: Option<i32>,
    /// Metrics used by the pod autoscaler
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<FlowCollectorConsolePluginAutoscalerMetrics>>,
    /// `minReplicas` is the lower limit for the number of replicas to which the autoscaler can scale down. It defaults to 1 pod. minReplicas is allowed to be 0 if the alpha feature gate HPAScaleToZero is enabled and at least one Object or External metric is configured. Scaling is active as long as at least one metric value is available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minReplicas")]
    pub min_replicas: Option<i32>,
    /// `status` describes the desired status regarding deploying an horizontal pod autoscaler.<br> - `Disabled` does not deploy an horizontal pod autoscaler.<br> - `Enabled` deploys an horizontal pod autoscaler.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub status: Option<FlowCollectorConsolePluginAutoscalerStatus>,
}

/// MetricSpec specifies how to scale based on a single metric (only `type` and one other matching field should be set at once).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetrics {
    /// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerResource")]
    pub container_resource: Option<FlowCollectorConsolePluginAutoscalerMetricsContainerResource>,
    /// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub external: Option<FlowCollectorConsolePluginAutoscalerMetricsExternal>,
    /// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub object: Option<FlowCollectorConsolePluginAutoscalerMetricsObject>,
    /// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pods: Option<FlowCollectorConsolePluginAutoscalerMetricsPods>,
    /// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resource: Option<FlowCollectorConsolePluginAutoscalerMetricsResource>,
    /// type is the type of metric source.  It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object. Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
    #[serde(rename = "type")]
    pub r#type: String,
}

/// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsContainerResource {
    /// container is the name of the container in the pods of the scaling target
    pub container: String,
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsContainerResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsContainerResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternal {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsExternalMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsExternalTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObject {
    /// describedObject specifies the descriptions of a object,such as kind,name apiVersion
    #[serde(rename = "describedObject")]
    pub described_object: FlowCollectorConsolePluginAutoscalerMetricsObjectDescribedObject,
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsObjectMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsObjectTarget,
}

/// describedObject specifies the descriptions of a object,such as kind,name apiVersion
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectDescribedObject {
    /// apiVersion is the API version of the referent
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// kind is the kind of the referent; More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
    pub kind: String,
    /// name is the name of the referent; More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPods {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsPodsMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsPodsTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsResource {
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorConsolePluginAutoscalerMetricsResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginAutoscalerStatus {
    Disabled,
    Enabled,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `portNaming` defines the configuration of the port-to-service name translation
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginPortNaming {
    /// Enable the console plugin port-to-service name translation
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `portNames` defines additional port names to use in the console, for example, `portNames: {"3100": "loki"}`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portNames")]
    pub port_names: Option<BTreeMap<String, String>>,
}

/// `QuickFilter` defines preset configuration for Console's quick filters
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginQuickFilters {
    /// `default` defines whether this filter should be active by default or not
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub default: Option<bool>,
    /// `filter` is a set of keys and values to be set when this filter is selected. Each key can relate to a list of values using a coma-separated string, for example, `filter: {"src_namespace": "namespace1,namespace2"}`.
    pub filter: BTreeMap<String, String>,
    /// Name of the filter, that is displayed in the Console
    pub name: String,
}

/// `resources`, in terms of compute resources, required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container. 
    ///  This is an alpha field and requires enabling the DynamicResourceAllocation feature gate. 
    ///  This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorConsolePluginResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorConsolePluginResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container.
    pub name: String,
}

/// Defines the desired state of the FlowCollector resource. <br><br> *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature is not officially supported by Red Hat. It might have been, for example, contributed by the community and accepted without a formal agreement for maintenance. The product maintainers might provide some support for these features as a best effort only.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorDeploymentModel {
    Direct,
    Kafka,
}

/// `FlowCollectorExporter` defines an additional exporter to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExporters {
    /// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ipfix: Option<FlowCollectorExportersIpfix>,
    /// Kafka configuration, such as the address and topic, to send enriched flows to.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kafka: Option<FlowCollectorExportersKafka>,
    /// `type` selects the type of exporters. The available options are `Kafka` and `IPFIX`.
    #[serde(rename = "type")]
    pub r#type: FlowCollectorExportersType,
}

/// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersIpfix {
    /// Address of the IPFIX external receiver
    #[serde(rename = "targetHost")]
    pub target_host: String,
    /// Port for the IPFIX external receiver
    #[serde(rename = "targetPort")]
    pub target_port: i64,
    /// Transport protocol (`TCP` or `UDP`) to be used for the IPFIX connection, defaults to `TCP`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub transport: Option<FlowCollectorExportersIpfixTransport>,
}

/// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersIpfixTransport {
    #[serde(rename = "TCP")]
    Tcp,
    #[serde(rename = "UDP")]
    Udp,
}

/// Kafka configuration, such as the address and topic, to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafka {
    /// Address of the Kafka server
    pub address: String,
    /// SASL authentication configuration. [Unsupported (*)].
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl: Option<FlowCollectorExportersKafkaSasl>,
    /// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorExportersKafkaTls>,
    /// Kafka topic to use. It must exist. NetObserv does not create it.
    pub topic: String,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaSasl {
    /// Reference to the secret or config map containing the client ID
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientIDReference")]
    pub client_id_reference: Option<FlowCollectorExportersKafkaSaslClientIdReference>,
    /// Reference to the secret or config map containing the client secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientSecretReference")]
    pub client_secret_reference: Option<FlowCollectorExportersKafkaSaslClientSecretReference>,
    /// Type of SASL authentication to use, or `Disabled` if SASL is not used
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaSaslClientIdReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslClientIdReferenceType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslClientIdReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaSaslClientSecretReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslClientSecretReferenceType>,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslClientSecretReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslType {
    Disabled,
    Plain,
    #[serde(rename = "ScramSHA512")]
    ScramSha512,
}

/// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorExportersKafkaTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorExportersKafkaTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExportersKafkaTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `FlowCollectorExporter` defines an additional exporter to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersType {
    Kafka,
    #[serde(rename = "IPFIX")]
    Ipfix,
}

/// Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the `spec.deploymentModel` is `Kafka`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafka {
    /// Address of the Kafka server
    pub address: String,
    /// SASL authentication configuration. [Unsupported (*)].
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl: Option<FlowCollectorKafkaSasl>,
    /// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorKafkaTls>,
    /// Kafka topic to use. It must exist. NetObserv does not create it.
    pub topic: String,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaSasl {
    /// Reference to the secret or config map containing the client ID
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientIDReference")]
    pub client_id_reference: Option<FlowCollectorKafkaSaslClientIdReference>,
    /// Reference to the secret or config map containing the client secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientSecretReference")]
    pub client_secret_reference: Option<FlowCollectorKafkaSaslClientSecretReference>,
    /// Type of SASL authentication to use, or `Disabled` if SASL is not used
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaSaslClientIdReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslClientIdReferenceType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslClientIdReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaSaslClientSecretReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslClientSecretReferenceType>,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslClientSecretReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslType {
    Disabled,
    Plain,
    #[serde(rename = "ScramSHA512")]
    ScramSha512,
}

/// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorKafkaTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorKafkaTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorKafkaTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `loki`, the flow store, client settings.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLoki {
    /// `advanced` allows setting some aspects of the internal configuration of the Loki clients. This section is aimed mostly for debugging and fine-grained performance optimizations.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub advanced: Option<FlowCollectorLokiAdvanced>,
    /// Set `enable` to `true` to store flows in Loki. It is required for the OpenShift Console plugin installation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// Loki configuration for `LokiStack` mode. This is useful for an easy loki-operator configuration. It is ignored for other modes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lokiStack")]
    pub loki_stack: Option<FlowCollectorLokiLokiStack>,
    /// Loki configuration for `Manual` mode. This is the most flexible configuration. It is ignored for other modes.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub manual: Option<FlowCollectorLokiManual>,
    /// Loki configuration for `Microservices` mode. Use this option when Loki is installed using the microservices deployment mode (https://grafana.com/docs/loki/latest/fundamentals/architecture/deployment-modes/#microservices-mode). It is ignored for other modes.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub microservices: Option<FlowCollectorLokiMicroservices>,
    /// `mode` must be set according to the installation mode of Loki:<br> - Use `LokiStack` when Loki is managed using the Loki Operator<br> - Use `Monolithic` when Loki is installed as a monolithic workload<br> - Use `Microservices` when Loki is installed as microservices, but without Loki Operator<br> - Use `Manual` if none of the options above match your setup<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<FlowCollectorLokiMode>,
    /// Loki configuration for `Monolithic` mode. Use this option when Loki is installed using the monolithic deployment mode (https://grafana.com/docs/loki/latest/fundamentals/architecture/deployment-modes/#monolithic-mode). It is ignored for other modes.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub monolithic: Option<FlowCollectorLokiMonolithic>,
    /// `readTimeout` is the maximum console plugin loki query total time limit. A timeout of zero means no timeout.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readTimeout")]
    pub read_timeout: Option<String>,
    /// `writeBatchSize` is the maximum batch size (in bytes) of Loki logs to accumulate before sending.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeBatchSize")]
    pub write_batch_size: Option<i64>,
    /// `writeBatchWait` is the maximum time to wait before sending a Loki batch.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeBatchWait")]
    pub write_batch_wait: Option<String>,
    /// `writeTimeout` is the maximum Loki time connection / request limit. A timeout of zero means no timeout.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeTimeout")]
    pub write_timeout: Option<String>,
}

/// `advanced` allows setting some aspects of the internal configuration of the Loki clients. This section is aimed mostly for debugging and fine-grained performance optimizations.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiAdvanced {
    /// `staticLabels` is a map of common labels to set on each flow in Loki storage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "staticLabels")]
    pub static_labels: Option<BTreeMap<String, String>>,
    /// `writeMaxBackoff` is the maximum backoff time for Loki client connection between retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeMaxBackoff")]
    pub write_max_backoff: Option<String>,
    /// `writeMaxRetries` is the maximum number of retries for Loki client connections.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeMaxRetries")]
    pub write_max_retries: Option<i32>,
    /// `writeMinBackoff` is the initial backoff time for Loki client connection between retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeMinBackoff")]
    pub write_min_backoff: Option<String>,
}

/// Loki configuration for `LokiStack` mode. This is useful for an easy loki-operator configuration. It is ignored for other modes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiLokiStack {
    /// Name of an existing LokiStack resource to use.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace where this `LokiStack` resource is located. If omited, it is assumed to be the same as `spec.namespace`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// Loki configuration for `Manual` mode. This is the most flexible configuration. It is ignored for other modes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiManual {
    /// `authToken` describes the way to get a token to authenticate to Loki.<br> - `Disabled` does not send any token with the request.<br> - `Forward` forwards the user token for authorization.<br> - `Host` [deprecated (*)] - uses the local pod service account to authenticate to Loki.<br> When using the Loki Operator, this must be set to `Forward`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "authToken")]
    pub auth_token: Option<FlowCollectorLokiManualAuthToken>,
    /// `ingesterUrl` is the address of an existing Loki ingester service to push the flows to. When using the Loki Operator, set it to the Loki gateway service with the `network` tenant set in path, for example https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingesterUrl")]
    pub ingester_url: Option<String>,
    /// `querierUrl` specifies the address of the Loki querier service. When using the Loki Operator, set it to the Loki gateway service with the `network` tenant set in path, for example https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "querierUrl")]
    pub querier_url: Option<String>,
    /// TLS client configuration for Loki status URL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusTls")]
    pub status_tls: Option<FlowCollectorLokiManualStatusTls>,
    /// `statusUrl` specifies the address of the Loki `/ready`, `/metrics` and `/config` endpoints, in case it is different from the Loki querier URL. If empty, the `querierUrl` value is used. This is useful to show error messages and some context in the frontend. When using the Loki Operator, set it to the Loki HTTP query frontend service, for example https://loki-query-frontend-http.netobserv.svc:3100/. `statusTLS` configuration is used when `statusUrl` is set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusUrl")]
    pub status_url: Option<String>,
    /// `tenantID` is the Loki `X-Scope-OrgID` that identifies the tenant for each request. When using the Loki Operator, set it to `network`, which corresponds to a special tenant mode.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tenantID")]
    pub tenant_id: Option<String>,
    /// TLS client configuration for Loki URL.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorLokiManualTls>,
}

/// Loki configuration for `Manual` mode. This is the most flexible configuration. It is ignored for other modes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiManualAuthToken {
    Disabled,
    Host,
    Forward,
}

/// TLS client configuration for Loki status URL.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiManualStatusTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiManualStatusTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiManualStatusTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiManualStatusTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiManualStatusTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiManualStatusTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiManualStatusTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiManualStatusTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiManualStatusTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS client configuration for Loki URL.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiManualTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiManualTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiManualTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiManualTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiManualTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiManualTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiManualTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiManualTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiManualTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Loki configuration for `Microservices` mode. Use this option when Loki is installed using the microservices deployment mode (https://grafana.com/docs/loki/latest/fundamentals/architecture/deployment-modes/#microservices-mode). It is ignored for other modes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMicroservices {
    /// `ingesterUrl` is the address of an existing Loki ingester service to push the flows to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingesterUrl")]
    pub ingester_url: Option<String>,
    /// `querierURL` specifies the address of the Loki querier service.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "querierUrl")]
    pub querier_url: Option<String>,
    /// `tenantID` is the Loki `X-Scope-OrgID` header that identifies the tenant for each request.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tenantID")]
    pub tenant_id: Option<String>,
    /// TLS client configuration for Loki URL.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorLokiMicroservicesTls>,
}

/// TLS client configuration for Loki URL.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMicroservicesTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiMicroservicesTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiMicroservicesTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMicroservicesTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiMicroservicesTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiMicroservicesTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMicroservicesTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiMicroservicesTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiMicroservicesTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `loki`, the flow store, client settings.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiMode {
    Manual,
    LokiStack,
    Monolithic,
    Microservices,
}

/// Loki configuration for `Monolithic` mode. Use this option when Loki is installed using the monolithic deployment mode (https://grafana.com/docs/loki/latest/fundamentals/architecture/deployment-modes/#monolithic-mode). It is ignored for other modes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMonolithic {
    /// `tenantID` is the Loki `X-Scope-OrgID` header that identifies the tenant for each request.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tenantID")]
    pub tenant_id: Option<String>,
    /// TLS client configuration for Loki URL.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorLokiMonolithicTls>,
    /// `url` is the unique address of an existing Loki service that points to both the ingester and the querier.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub url: Option<String>,
}

/// TLS client configuration for Loki URL.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMonolithicTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiMonolithicTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate. If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiMonolithicTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMonolithicTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiMonolithicTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiMonolithicTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorLokiMonolithicTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiMonolithicTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiMonolithicTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessor {
    /// `addZone` allows availability zone awareness by labelling flows with their source and destination zones. This feature requires the "topology.kubernetes.io/zone" label to be set on nodes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "addZone")]
    pub add_zone: Option<bool>,
    /// `advanced` allows setting some aspects of the internal configuration of the flow processor. This section is aimed mostly for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub advanced: Option<FlowCollectorProcessorAdvanced>,
    /// `clusterName` is the name of the cluster to appear in the flows data. This is useful in a multi-cluster context. When using OpenShift, leave empty to make it automatically determined.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    pub cluster_name: Option<String>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorProcessorImagePullPolicy>,
    /// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerAutoscaler")]
    pub kafka_consumer_autoscaler: Option<FlowCollectorProcessorKafkaConsumerAutoscaler>,
    /// `kafkaConsumerBatchSize` indicates to the broker the maximum batch size, in bytes, that the consumer accepts. Ignored when not using Kafka. Default: 10MB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerBatchSize")]
    pub kafka_consumer_batch_size: Option<i64>,
    /// `kafkaConsumerQueueCapacity` defines the capacity of the internal message queue used in the Kafka consumer client. Ignored when not using Kafka.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerQueueCapacity")]
    pub kafka_consumer_queue_capacity: Option<i64>,
    /// `kafkaConsumerReplicas` defines the number of replicas (pods) to start for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerReplicas")]
    pub kafka_consumer_replicas: Option<i32>,
    /// `logLevel` of the processor runtime
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorProcessorLogLevel>,
    /// `logTypes` defines the desired record types to generate. Possible values are:<br> - `Flows` (default) to export regular network flows<br> - `Conversations` to generate events for started conversations, ended conversations as well as periodic "tick" updates<br> - `EndedConversations` to generate only ended conversations events<br> - `All` to generate both network flows and all conversations events<br>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logTypes")]
    pub log_types: Option<FlowCollectorProcessorLogTypes>,
    /// `Metrics` define the processor configuration regarding metrics
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<FlowCollectorProcessorMetrics>,
    /// Set `multiClusterDeployment` to `true` to enable multi clusters feature. This adds `clusterName` label to flows data
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "multiClusterDeployment")]
    pub multi_cluster_deployment: Option<bool>,
    /// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorProcessorResources>,
}

/// `advanced` allows setting some aspects of the internal configuration of the flow processor. This section is aimed mostly for debugging and fine-grained performance optimizations, such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorAdvanced {
    /// `conversationEndTimeout` is the time to wait after a network flow is received, to consider the conversation ended. This delay is ignored when a FIN packet is collected for TCP flows (see `conversationTerminatingTimeout` instead).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationEndTimeout")]
    pub conversation_end_timeout: Option<String>,
    /// `conversationHeartbeatInterval` is the time to wait between "tick" events of a conversation
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationHeartbeatInterval")]
    pub conversation_heartbeat_interval: Option<String>,
    /// `conversationTerminatingTimeout` is the time to wait from detected FIN flag to end a conversation. Only relevant for TCP flows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationTerminatingTimeout")]
    pub conversation_terminating_timeout: Option<String>,
    /// `dropUnusedFields` allows, when set to `true`, to drop fields that are known to be unused by OVS, to save storage space.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dropUnusedFields")]
    pub drop_unused_fields: Option<bool>,
    /// `enableKubeProbes` is a flag to enable or disable Kubernetes liveness and readiness probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKubeProbes")]
    pub enable_kube_probes: Option<bool>,
    /// `env` allows passing custom environment variables to underlying components. Useful for passing some very concrete performance-tuning options, such as `GOGC` and `GOMAXPROCS`, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<BTreeMap<String, String>>,
    /// `healthPort` is a collector HTTP port in the Pod that exposes the health check API
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "healthPort")]
    pub health_port: Option<i32>,
    /// Port of the flow collector (host port). By convention, some values are forbidden. It must be greater than 1024 and different from 4500, 4789 and 6081.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// `profilePort` allows setting up a Go pprof profiler listening to this port
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "profilePort")]
    pub profile_port: Option<i32>,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscaler {
    /// `maxReplicas` is the upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than MinReplicas.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxReplicas")]
    pub max_replicas: Option<i32>,
    /// Metrics used by the pod autoscaler
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetrics>>,
    /// `minReplicas` is the lower limit for the number of replicas to which the autoscaler can scale down. It defaults to 1 pod. minReplicas is allowed to be 0 if the alpha feature gate HPAScaleToZero is enabled and at least one Object or External metric is configured. Scaling is active as long as at least one metric value is available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minReplicas")]
    pub min_replicas: Option<i32>,
    /// `status` describes the desired status regarding deploying an horizontal pod autoscaler.<br> - `Disabled` does not deploy an horizontal pod autoscaler.<br> - `Enabled` deploys an horizontal pod autoscaler.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub status: Option<FlowCollectorProcessorKafkaConsumerAutoscalerStatus>,
}

/// MetricSpec specifies how to scale based on a single metric (only `type` and one other matching field should be set at once).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetrics {
    /// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerResource")]
    pub container_resource: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResource>,
    /// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub external: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternal>,
    /// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub object: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObject>,
    /// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pods: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPods>,
    /// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resource: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResource>,
    /// type is the type of metric source.  It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object. Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
    #[serde(rename = "type")]
    pub r#type: String,
}

/// containerResource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing a single container in each pod of the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source. This is an alpha feature and can be enabled by the HPAContainerMetrics feature flag.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResource {
    /// container is the name of the container in the pods of the scaling target
    pub container: String,
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// external refers to a global metric that is not associated with any Kubernetes object. It allows autoscaling based on information coming from components running outside of cluster (for example length of queue in cloud messaging service, or QPS from loadbalancer running outside of cluster).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternal {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// object refers to a metric describing a single kubernetes object (for example, hits-per-second on an Ingress object).
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObject {
    /// describedObject specifies the descriptions of a object,such as kind,name apiVersion
    #[serde(rename = "describedObject")]
    pub described_object: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectDescribedObject,
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectTarget,
}

/// describedObject specifies the descriptions of a object,such as kind,name apiVersion
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectDescribedObject {
    /// apiVersion is the API version of the referent
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// kind is the kind of the referent; More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
    pub kind: String,
    /// name is the name of the referent; More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// pods refers to a metric describing each pod in the current scale target (for example, transactions-processed-per-second).  The values will be averaged together before being compared to the target value.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPods {
    /// metric identifies the target metric by name and selector
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetric,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsTarget,
}

/// metric identifies the target metric by name and selector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetric {
    /// name is the name of the given metric
    pub name: String,
    /// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelector>,
}

/// selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// resource refers to a resource metric (such as those specified in requests and limits) known to Kubernetes describing each pod in the current scale target (e.g. CPU or memory). Such metrics are built in to Kubernetes, and have special scaling options on top of those available to normal per-pod metrics using the "pods" source.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResource {
    /// name is the name of the resource in question.
    pub name: String,
    /// target specifies the target value for the given metric
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResourceTarget,
}

/// target specifies the target value for the given metric
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResourceTarget {
    /// averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    /// averageValue is the target value of the average of the metric across all relevant pods (as a quantity)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    /// type represents whether the metric type is Utilization, Value, or AverageValue
    #[serde(rename = "type")]
    pub r#type: String,
    /// value is the target value of the metric (as a quantity).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorKafkaConsumerAutoscalerStatus {
    Disabled,
    Enabled,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `processor` defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorLogTypes {
    Flows,
    Conversations,
    EndedConversations,
    All,
}

/// `Metrics` define the processor configuration regarding metrics
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetrics {
    /// `disableAlerts` is a list of alerts that should be disabled. Possible values are:<br> `NetObservNoFlows`, which is triggered when no flows are being observed for a certain period.<br> `NetObservLokiError`, which is triggered when flows are being dropped due to Loki errors.<br>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "disableAlerts")]
    pub disable_alerts: Option<Vec<String>>,
    /// `includeList` is a list of metric names to specify which ones to generate. The names correspond to the names in Prometheus without the prefix. For example, `namespace_egress_packets_total` shows up as `netobserv_namespace_egress_packets_total` in Prometheus. Note that the more metrics you add, the bigger is the impact on Prometheus workload resources. Metrics enabled by default are: `namespace_flows_total`, `node_ingress_bytes_total`, `workload_ingress_bytes_total`, `namespace_drop_packets_total` (when `PacketDrop` feature is enabled), `namespace_rtt_seconds` (when `FlowRTT` feature is enabled), `namespace_dns_latency_seconds` (when `DNSTracking` feature is enabled). More information, with full list of available metrics: https://github.com/netobserv/network-observability-operator/blob/main/docs/Metrics.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeList")]
    pub include_list: Option<Vec<String>>,
    /// Metrics server endpoint configuration for Prometheus scraper
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<FlowCollectorProcessorMetricsServer>,
}

/// Metrics server endpoint configuration for Prometheus scraper
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServer {
    /// The prometheus HTTP port
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// TLS configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorProcessorMetricsServerTls>,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTls {
    /// `insecureSkipVerify` allows skipping client-side verification of the provided certificate. If set to `true`, the `providedCaFile` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// TLS configuration when `type` is set to `Provided`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub provided: Option<FlowCollectorProcessorMetricsServerTlsProvided>,
    /// Reference to the CA file when `type` is set to `Provided`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providedCaFile")]
    pub provided_ca_file: Option<FlowCollectorProcessorMetricsServerTlsProvidedCaFile>,
    /// Select the type of TLS configuration:<br> - `Disabled` (default) to not configure TLS for the endpoint. - `Provided` to manually provide cert file and a key file. - `Auto` to use OpenShift auto generated certificate using annotations.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsType>,
}

/// TLS configuration when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTlsProvided {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsProvidedType>,
}

/// TLS configuration when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsProvidedType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the CA file when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTlsProvidedCaFile {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsProvidedCaFileType>,
}

/// Reference to the CA file when `type` is set to `Provided`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsProvidedCaFileType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsType {
    Disabled,
    Provided,
    Auto,
}

/// `resources` are the compute resources required by this container. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container. 
    ///  This is an alpha field and requires enabling the DynamicResourceAllocation feature gate. 
    ///  This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorProcessorResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container.
    pub name: String,
}

/// `FlowCollectorStatus` defines the observed state of FlowCollector
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorStatus {
    /// `conditions` represent the latest available observations of an object's state
    pub conditions: Vec<FlowCollectorStatusConditions>,
    /// Namespace where console plugin and flowlogs-pipeline have been deployed. Deprecated: annotations are used instead
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions.  For example, 
///  	type FooStatus struct{ 	    // Represents the observations of a foo's current state. 	    // Known .status.conditions.type are: "Available", "Progressing", and "Degraded" 	    // +patchMergeKey=type 	    // +patchStrategy=merge 	    // +listType=map 	    // +listMapKey=type 	    Conditions []metav1.Condition `json:"conditions,omitempty" patchStrategy:"merge" patchMergeKey:"type" protobuf:"bytes,1,rep,name=conditions"` 
///  	    // other fields 	}
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorStatusConditions {
    /// lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable.
    #[serde(rename = "lastTransitionTime")]
    pub last_transition_time: String,
    /// message is a human readable message indicating details about the transition. This may be an empty string.
    pub message: String,
    /// observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    pub observed_generation: Option<i64>,
    /// reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty.
    pub reason: String,
    /// status of the condition, one of True, False, Unknown.
    pub status: FlowCollectorStatusConditionsStatus,
    /// type of condition in CamelCase or in foo.example.com/CamelCase. --- Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt)
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions.  For example, 
///  	type FooStatus struct{ 	    // Represents the observations of a foo's current state. 	    // Known .status.conditions.type are: "Available", "Progressing", and "Degraded" 	    // +patchMergeKey=type 	    // +patchStrategy=merge 	    // +listType=map 	    // +listMapKey=type 	    Conditions []metav1.Condition `json:"conditions,omitempty" patchStrategy:"merge" patchMergeKey:"type" protobuf:"bytes,1,rep,name=conditions"` 
///  	    // other fields 	}
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorStatusConditionsStatus {
    True,
    False,
    Unknown,
}

