// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --filename=./crd-catalog/rook/rook/ceph.rook.io/v1/cephobjectzones.yaml --derive=Default --derive=PartialEq --smart-derive-elision
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// ObjectZoneSpec represent the spec of an ObjectZone
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
#[kube(group = "ceph.rook.io", version = "v1", kind = "CephObjectZone", plural = "cephobjectzones")]
#[kube(namespaced)]
#[kube(status = "CephObjectZoneStatus")]
#[kube(schema = "disabled")]
#[kube(derive="Default")]
#[kube(derive="PartialEq")]
pub struct CephObjectZoneSpec {
    /// If this zone cannot be accessed from other peer Ceph clusters via the ClusterIP Service
    /// endpoint created by Rook, you must set this to the externally reachable endpoint(s). You may
    /// include the port in the definition. For example: "https://my-object-store.my-domain.net:443".
    /// In many cases, you should set this to the endpoint of the ingress resource that makes the
    /// CephObjectStore associated with this CephObjectStoreZone reachable to peer clusters.
    /// The list can have one or more endpoints pointing to different RGW servers in the zone.
    /// 
    /// If a CephObjectStore endpoint is omitted from this list, that object store's gateways will
    /// not receive multisite replication data
    /// (see CephObjectStore.spec.gateway.disableMultisiteSyncTraffic).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customEndpoints")]
    pub custom_endpoints: Option<Vec<String>>,
    /// The data pool settings
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataPool")]
    pub data_pool: Option<CephObjectZoneDataPool>,
    /// The metadata pool settings
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metadataPool")]
    pub metadata_pool: Option<CephObjectZoneMetadataPool>,
    /// Preserve pools on object zone deletion
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preservePoolsOnDelete")]
    pub preserve_pools_on_delete: Option<bool>,
    /// The pool information when configuring RADOS namespaces in existing pools.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sharedPools")]
    pub shared_pools: Option<CephObjectZoneSharedPools>,
    /// The display name for the ceph users
    #[serde(rename = "zoneGroup")]
    pub zone_group: String,
}

/// The data pool settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPool {
    /// The application name to set on the pool. Only expected to be set for rgw pools.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub application: Option<String>,
    /// DEPRECATED: use Parameters instead, e.g., Parameters["compression_mode"] = "force"
    /// The inline compression mode in Bluestore OSD to set to (options are: none, passive, aggressive, force)
    /// Do NOT set a default value for kubebuilder as this will override the Parameters
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "compressionMode")]
    pub compression_mode: Option<CephObjectZoneDataPoolCompressionMode>,
    /// The root of the crush hierarchy utilized by the pool
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crushRoot")]
    pub crush_root: Option<String>,
    /// The device class the OSD should set to for use in the pool
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceClass")]
    pub device_class: Option<String>,
    /// Allow rook operator to change the pool CRUSH tunables once the pool is created
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableCrushUpdates")]
    pub enable_crush_updates: Option<bool>,
    /// EnableRBDStats is used to enable gathering of statistics for all RBD images in the pool
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableRBDStats")]
    pub enable_rbd_stats: Option<bool>,
    /// The erasure code settings
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "erasureCoded")]
    pub erasure_coded: Option<CephObjectZoneDataPoolErasureCoded>,
    /// The failure domain: osd/host/(region or zone if available) - technically also any type in the crush map
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureDomain")]
    pub failure_domain: Option<String>,
    /// The mirroring settings
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mirroring: Option<CephObjectZoneDataPoolMirroring>,
    /// Parameters is a list of properties to enable on a given pool
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<BTreeMap<String, String>>,
    /// The quota settings
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub quotas: Option<CephObjectZoneDataPoolQuotas>,
    /// The replication settings
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub replicated: Option<CephObjectZoneDataPoolReplicated>,
    /// The mirroring statusCheck
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusCheck")]
    pub status_check: Option<CephObjectZoneDataPoolStatusCheck>,
}

/// The data pool settings
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum CephObjectZoneDataPoolCompressionMode {
    #[serde(rename = "none")]
    None,
    #[serde(rename = "passive")]
    Passive,
    #[serde(rename = "aggressive")]
    Aggressive,
    #[serde(rename = "force")]
    Force,
    #[serde(rename = "")]
    KopiumEmpty,
}

/// The erasure code settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolErasureCoded {
    /// The algorithm for erasure coding
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub algorithm: Option<String>,
    /// Number of coding chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
    /// This is the number of OSDs that can be lost simultaneously before data cannot be recovered.
    #[serde(rename = "codingChunks")]
    pub coding_chunks: i64,
    /// Number of data chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
    /// The number of chunks required to recover an object when any single OSD is lost is the same
    /// as dataChunks so be aware that the larger the number of data chunks, the higher the cost of recovery.
    #[serde(rename = "dataChunks")]
    pub data_chunks: i64,
}

/// The mirroring settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolMirroring {
    /// Enabled whether this pool is mirrored or not
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enabled: Option<bool>,
    /// Mode is the mirroring mode: either pool or image
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<String>,
    /// Peers represents the peers spec
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub peers: Option<CephObjectZoneDataPoolMirroringPeers>,
    /// SnapshotSchedules is the scheduling of snapshot for mirrored images/pools
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "snapshotSchedules")]
    pub snapshot_schedules: Option<Vec<CephObjectZoneDataPoolMirroringSnapshotSchedules>>,
}

/// Peers represents the peers spec
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolMirroringPeers {
    /// SecretNames represents the Kubernetes Secret names to add rbd-mirror or cephfs-mirror peers
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretNames")]
    pub secret_names: Option<Vec<String>>,
}

/// SnapshotScheduleSpec represents the snapshot scheduling settings of a mirrored pool
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolMirroringSnapshotSchedules {
    /// Interval represent the periodicity of the snapshot.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub interval: Option<String>,
    /// Path is the path to snapshot, only valid for CephFS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// StartTime indicates when to start the snapshot
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startTime")]
    pub start_time: Option<String>,
}

/// The quota settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolQuotas {
    /// MaxBytes represents the quota in bytes
    /// Deprecated in favor of MaxSize
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxBytes")]
    pub max_bytes: Option<i64>,
    /// MaxObjects represents the quota in objects
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxObjects")]
    pub max_objects: Option<i64>,
    /// MaxSize represents the quota in bytes as a string
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxSize")]
    pub max_size: Option<String>,
}

/// The replication settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolReplicated {
    /// HybridStorage represents hybrid storage tier settings
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hybridStorage")]
    pub hybrid_storage: Option<CephObjectZoneDataPoolReplicatedHybridStorage>,
    /// ReplicasPerFailureDomain the number of replica in the specified failure domain
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "replicasPerFailureDomain")]
    pub replicas_per_failure_domain: Option<i64>,
    /// RequireSafeReplicaSize if false allows you to set replica 1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requireSafeReplicaSize")]
    pub require_safe_replica_size: Option<bool>,
    /// Size - Number of copies per object in a replicated storage pool, including the object itself (required for replicated pool type)
    pub size: i64,
    /// SubFailureDomain the name of the sub-failure domain
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subFailureDomain")]
    pub sub_failure_domain: Option<String>,
    /// TargetSizeRatio gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetSizeRatio")]
    pub target_size_ratio: Option<f64>,
}

/// HybridStorage represents hybrid storage tier settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolReplicatedHybridStorage {
    /// PrimaryDeviceClass represents high performance tier (for example SSD or NVME) for Primary OSD
    #[serde(rename = "primaryDeviceClass")]
    pub primary_device_class: String,
    /// SecondaryDeviceClass represents low performance tier (for example HDDs) for remaining OSDs
    #[serde(rename = "secondaryDeviceClass")]
    pub secondary_device_class: String,
}

/// The mirroring statusCheck
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolStatusCheck {
    /// HealthCheckSpec represents the health check of an object store bucket
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mirror: Option<CephObjectZoneDataPoolStatusCheckMirror>,
}

/// HealthCheckSpec represents the health check of an object store bucket
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneDataPoolStatusCheckMirror {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub disabled: Option<bool>,
    /// Interval is the internal in second or minute for the health check to run like 60s for 60 seconds
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub interval: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timeout: Option<String>,
}

/// The metadata pool settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPool {
    /// The application name to set on the pool. Only expected to be set for rgw pools.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub application: Option<String>,
    /// DEPRECATED: use Parameters instead, e.g., Parameters["compression_mode"] = "force"
    /// The inline compression mode in Bluestore OSD to set to (options are: none, passive, aggressive, force)
    /// Do NOT set a default value for kubebuilder as this will override the Parameters
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "compressionMode")]
    pub compression_mode: Option<CephObjectZoneMetadataPoolCompressionMode>,
    /// The root of the crush hierarchy utilized by the pool
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crushRoot")]
    pub crush_root: Option<String>,
    /// The device class the OSD should set to for use in the pool
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceClass")]
    pub device_class: Option<String>,
    /// Allow rook operator to change the pool CRUSH tunables once the pool is created
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableCrushUpdates")]
    pub enable_crush_updates: Option<bool>,
    /// EnableRBDStats is used to enable gathering of statistics for all RBD images in the pool
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableRBDStats")]
    pub enable_rbd_stats: Option<bool>,
    /// The erasure code settings
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "erasureCoded")]
    pub erasure_coded: Option<CephObjectZoneMetadataPoolErasureCoded>,
    /// The failure domain: osd/host/(region or zone if available) - technically also any type in the crush map
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureDomain")]
    pub failure_domain: Option<String>,
    /// The mirroring settings
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mirroring: Option<CephObjectZoneMetadataPoolMirroring>,
    /// Parameters is a list of properties to enable on a given pool
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<BTreeMap<String, String>>,
    /// The quota settings
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub quotas: Option<CephObjectZoneMetadataPoolQuotas>,
    /// The replication settings
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub replicated: Option<CephObjectZoneMetadataPoolReplicated>,
    /// The mirroring statusCheck
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusCheck")]
    pub status_check: Option<CephObjectZoneMetadataPoolStatusCheck>,
}

/// The metadata pool settings
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum CephObjectZoneMetadataPoolCompressionMode {
    #[serde(rename = "none")]
    None,
    #[serde(rename = "passive")]
    Passive,
    #[serde(rename = "aggressive")]
    Aggressive,
    #[serde(rename = "force")]
    Force,
    #[serde(rename = "")]
    KopiumEmpty,
}

/// The erasure code settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolErasureCoded {
    /// The algorithm for erasure coding
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub algorithm: Option<String>,
    /// Number of coding chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
    /// This is the number of OSDs that can be lost simultaneously before data cannot be recovered.
    #[serde(rename = "codingChunks")]
    pub coding_chunks: i64,
    /// Number of data chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
    /// The number of chunks required to recover an object when any single OSD is lost is the same
    /// as dataChunks so be aware that the larger the number of data chunks, the higher the cost of recovery.
    #[serde(rename = "dataChunks")]
    pub data_chunks: i64,
}

/// The mirroring settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolMirroring {
    /// Enabled whether this pool is mirrored or not
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enabled: Option<bool>,
    /// Mode is the mirroring mode: either pool or image
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<String>,
    /// Peers represents the peers spec
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub peers: Option<CephObjectZoneMetadataPoolMirroringPeers>,
    /// SnapshotSchedules is the scheduling of snapshot for mirrored images/pools
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "snapshotSchedules")]
    pub snapshot_schedules: Option<Vec<CephObjectZoneMetadataPoolMirroringSnapshotSchedules>>,
}

/// Peers represents the peers spec
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolMirroringPeers {
    /// SecretNames represents the Kubernetes Secret names to add rbd-mirror or cephfs-mirror peers
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretNames")]
    pub secret_names: Option<Vec<String>>,
}

/// SnapshotScheduleSpec represents the snapshot scheduling settings of a mirrored pool
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolMirroringSnapshotSchedules {
    /// Interval represent the periodicity of the snapshot.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub interval: Option<String>,
    /// Path is the path to snapshot, only valid for CephFS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// StartTime indicates when to start the snapshot
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startTime")]
    pub start_time: Option<String>,
}

/// The quota settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolQuotas {
    /// MaxBytes represents the quota in bytes
    /// Deprecated in favor of MaxSize
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxBytes")]
    pub max_bytes: Option<i64>,
    /// MaxObjects represents the quota in objects
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxObjects")]
    pub max_objects: Option<i64>,
    /// MaxSize represents the quota in bytes as a string
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxSize")]
    pub max_size: Option<String>,
}

/// The replication settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolReplicated {
    /// HybridStorage represents hybrid storage tier settings
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hybridStorage")]
    pub hybrid_storage: Option<CephObjectZoneMetadataPoolReplicatedHybridStorage>,
    /// ReplicasPerFailureDomain the number of replica in the specified failure domain
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "replicasPerFailureDomain")]
    pub replicas_per_failure_domain: Option<i64>,
    /// RequireSafeReplicaSize if false allows you to set replica 1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requireSafeReplicaSize")]
    pub require_safe_replica_size: Option<bool>,
    /// Size - Number of copies per object in a replicated storage pool, including the object itself (required for replicated pool type)
    pub size: i64,
    /// SubFailureDomain the name of the sub-failure domain
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subFailureDomain")]
    pub sub_failure_domain: Option<String>,
    /// TargetSizeRatio gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetSizeRatio")]
    pub target_size_ratio: Option<f64>,
}

/// HybridStorage represents hybrid storage tier settings
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolReplicatedHybridStorage {
    /// PrimaryDeviceClass represents high performance tier (for example SSD or NVME) for Primary OSD
    #[serde(rename = "primaryDeviceClass")]
    pub primary_device_class: String,
    /// SecondaryDeviceClass represents low performance tier (for example HDDs) for remaining OSDs
    #[serde(rename = "secondaryDeviceClass")]
    pub secondary_device_class: String,
}

/// The mirroring statusCheck
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolStatusCheck {
    /// HealthCheckSpec represents the health check of an object store bucket
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mirror: Option<CephObjectZoneMetadataPoolStatusCheckMirror>,
}

/// HealthCheckSpec represents the health check of an object store bucket
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneMetadataPoolStatusCheckMirror {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub disabled: Option<bool>,
    /// Interval is the internal in second or minute for the health check to run like 60s for 60 seconds
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub interval: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timeout: Option<String>,
}

/// The pool information when configuring RADOS namespaces in existing pools.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneSharedPools {
    /// The data pool used for creating RADOS namespaces in the object store
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataPoolName")]
    pub data_pool_name: Option<String>,
    /// The metadata pool used for creating RADOS namespaces in the object store
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metadataPoolName")]
    pub metadata_pool_name: Option<String>,
    /// PoolPlacements control which Pools are associated with a particular RGW bucket.
    /// Once PoolPlacements are defined, RGW client will be able to associate pool
    /// with ObjectStore bucket by providing "<LocationConstraint>" during s3 bucket creation
    /// or "X-Storage-Policy" header during swift container creation.
    /// See: https://docs.ceph.com/en/latest/radosgw/placement/#placement-targets
    /// PoolPlacement with name: "default" will be used as a default pool if no option
    /// is provided during bucket creation.
    /// If default placement is not provided, spec.sharedPools.dataPoolName and spec.sharedPools.MetadataPoolName will be used as default pools.
    /// If spec.sharedPools are also empty, then RGW pools (spec.dataPool and spec.metadataPool) will be used as defaults.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "poolPlacements")]
    pub pool_placements: Option<Vec<CephObjectZoneSharedPoolsPoolPlacements>>,
    /// Whether the RADOS namespaces should be preserved on deletion of the object store
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preserveRadosNamespaceDataOnDelete")]
    pub preserve_rados_namespace_data_on_delete: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneSharedPoolsPoolPlacements {
    /// The data pool used to store ObjectStore data that cannot use erasure coding (ex: multi-part uploads).
    /// If dataPoolName is not erasure coded, then there is no need for dataNonECPoolName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataNonECPoolName")]
    pub data_non_ec_pool_name: Option<String>,
    /// The data pool used to store ObjectStore objects data.
    #[serde(rename = "dataPoolName")]
    pub data_pool_name: String,
    /// Sets given placement as default. Only one placement in the list can be marked as default.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub default: Option<bool>,
    /// The metadata pool used to store ObjectStore bucket index.
    #[serde(rename = "metadataPoolName")]
    pub metadata_pool_name: String,
    /// Pool placement name. Name can be arbitrary. Placement with name "default" will be used as default.
    pub name: String,
    /// StorageClasses can be selected by user to override dataPoolName during object creation.
    /// Each placement has default STANDARD StorageClass pointing to dataPoolName.
    /// This list allows defining additional StorageClasses on top of default STANDARD storage class.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storageClasses")]
    pub storage_classes: Option<Vec<CephObjectZoneSharedPoolsPoolPlacementsStorageClasses>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneSharedPoolsPoolPlacementsStorageClasses {
    /// DataPoolName is the data pool used to store ObjectStore objects data.
    #[serde(rename = "dataPoolName")]
    pub data_pool_name: String,
    /// Name is the StorageClass name. Ceph allows arbitrary name for StorageClasses,
    /// however most clients/libs insist on AWS names so it is recommended to use
    /// one of the valid x-amz-storage-class values for better compatibility:
    /// REDUCED_REDUNDANCY | STANDARD_IA | ONEZONE_IA | INTELLIGENT_TIERING | GLACIER | DEEP_ARCHIVE | OUTPOSTS | GLACIER_IR | SNOW | EXPRESS_ONEZONE
    /// See AWS docs: https://aws.amazon.com/de/s3/storage-classes/
    pub name: String,
}

/// Status represents the status of an object
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct CephObjectZoneStatus {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Condition>>,
    /// ObservedGeneration is the latest generation observed by the controller.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    pub observed_generation: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub phase: Option<String>,
}

