// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --filename=./crd-catalog/GoogleCloudPlatform/spark-on-k8s-operator/sparkoperator.k8s.io/v1beta2/scheduledsparkapplications.yaml --derive=Default --derive=PartialEq --smart-derive-elision
// kopium version: 0.21.1

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
    pub use k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;
}
use self::prelude::*;

/// ScheduledSparkApplicationSpec defines the desired state of ScheduledSparkApplication.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, PartialEq)]
#[kube(group = "sparkoperator.k8s.io", version = "v1beta2", kind = "ScheduledSparkApplication", plural = "scheduledsparkapplications")]
#[kube(namespaced)]
#[kube(status = "ScheduledSparkApplicationStatus")]
#[kube(schema = "disabled")]
#[kube(derive="PartialEq")]
pub struct ScheduledSparkApplicationSpec {
    /// ConcurrencyPolicy is the policy governing concurrent SparkApplication runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "concurrencyPolicy")]
    pub concurrency_policy: Option<String>,
    /// FailedRunHistoryLimit is the number of past failed runs of the application to keep.
    /// Defaults to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failedRunHistoryLimit")]
    pub failed_run_history_limit: Option<i32>,
    /// Schedule is a cron schedule on which the application should run.
    pub schedule: String,
    /// SuccessfulRunHistoryLimit is the number of past successful runs of the application to keep.
    /// Defaults to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successfulRunHistoryLimit")]
    pub successful_run_history_limit: Option<i32>,
    /// Suspend is a flag telling the controller to suspend subsequent runs of the application if set to true.
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub suspend: Option<bool>,
    /// Template is a template from which SparkApplication instances can be created.
    pub template: ScheduledSparkApplicationTemplate,
}

/// Template is a template from which SparkApplication instances can be created.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct ScheduledSparkApplicationTemplate {
    /// Arguments is a list of arguments to be passed to the application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arguments: Option<Vec<String>>,
    /// BatchScheduler configures which batch scheduler will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchScheduler")]
    pub batch_scheduler: Option<String>,
    /// BatchSchedulerOptions provides fine-grained control on how to batch scheduling.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSchedulerOptions")]
    pub batch_scheduler_options: Option<ScheduledSparkApplicationTemplateBatchSchedulerOptions>,
    /// Deps captures all possible types of dependencies of a Spark application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub deps: Option<ScheduledSparkApplicationTemplateDeps>,
    /// Driver is the driver specification.
    pub driver: ScheduledSparkApplicationTemplateDriver,
    /// DriverIngressOptions allows configuring the Service and the Ingress to expose ports inside Spark Driver
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverIngressOptions")]
    pub driver_ingress_options: Option<Vec<ScheduledSparkApplicationTemplateDriverIngressOptions>>,
    /// DynamicAllocation configures dynamic allocation that becomes available for the Kubernetes
    /// scheduler backend since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dynamicAllocation")]
    pub dynamic_allocation: Option<ScheduledSparkApplicationTemplateDynamicAllocation>,
    /// Executor is the executor specification.
    pub executor: ScheduledSparkApplicationTemplateExecutor,
    /// FailureRetries is the number of times to retry a failed application before giving up.
    /// This is best effort and actual retry attempts can be >= the value specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureRetries")]
    pub failure_retries: Option<i32>,
    /// HadoopConf carries user-specified Hadoop configuration properties as they would use the  the "--conf" option
    /// in spark-submit.  The SparkApplication controller automatically adds prefix "spark.hadoop." to Hadoop
    /// configuration properties.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConf")]
    pub hadoop_conf: Option<BTreeMap<String, String>>,
    /// HadoopConfigMap carries the name of the ConfigMap containing Hadoop configuration files such as core-site.xml.
    /// The controller will add environment variable HADOOP_CONF_DIR to the path where the ConfigMap is mounted to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConfigMap")]
    pub hadoop_config_map: Option<String>,
    /// Image is the container image for the driver, executor, and init-container. Any custom container images for the
    /// driver, executor, or init-container takes precedence over this.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// ImagePullPolicy is the image pull policy for the driver, executor, and init-container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// ImagePullSecrets is the list of image-pull secrets.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullSecrets")]
    pub image_pull_secrets: Option<Vec<String>>,
    /// MainFile is the path to a bundled JAR, Python, or R file of the application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainApplicationFile")]
    pub main_application_file: Option<String>,
    /// MainClass is the fully-qualified main class of the Spark application.
    /// This only applies to Java/Scala Spark applications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// This sets the Memory Overhead Factor that will allocate memory to non-JVM memory.
    /// For JVM-based jobs this value will default to 0.10, for non-JVM jobs 0.40. Value of this field will
    /// be overridden by `Spec.Driver.MemoryOverhead` and `Spec.Executor.MemoryOverhead` if they are set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverheadFactor")]
    pub memory_overhead_factor: Option<String>,
    /// Mode is the deployment mode of the Spark application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<ScheduledSparkApplicationTemplateMode>,
    /// Monitoring configures how monitoring is handled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub monitoring: Option<ScheduledSparkApplicationTemplateMonitoring>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at podSpec level (driver or executor).
    /// This field will be deprecated in future versions (at SparkApplicationSpec level).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// ProxyUser specifies the user to impersonate when submitting the application.
    /// It maps to the command-line flag "--proxy-user" in spark-submit.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "proxyUser")]
    pub proxy_user: Option<String>,
    /// This sets the major Python version of the docker
    /// image used to run the driver and executor containers. Can either be 2 or 3, default 2.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonVersion")]
    pub python_version: Option<ScheduledSparkApplicationTemplatePythonVersion>,
    /// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<ScheduledSparkApplicationTemplateRestartPolicy>,
    /// RetryInterval is the unit of intervals in seconds between submission retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryInterval")]
    pub retry_interval: Option<i64>,
    /// SparkConf carries user-specified Spark configuration properties as they would use the  "--conf" option in
    /// spark-submit.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConf")]
    pub spark_conf: Option<BTreeMap<String, String>>,
    /// SparkConfigMap carries the name of the ConfigMap containing Spark configuration files such as log4j.properties.
    /// The controller will add environment variable SPARK_CONF_DIR to the path where the ConfigMap is mounted to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConfigMap")]
    pub spark_config_map: Option<String>,
    /// SparkUIOptions allows configuring the Service and the Ingress to expose the sparkUI
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkUIOptions")]
    pub spark_ui_options: Option<ScheduledSparkApplicationTemplateSparkUiOptions>,
    /// SparkVersion is the version of Spark the application uses.
    #[serde(rename = "sparkVersion")]
    pub spark_version: String,
    /// TimeToLiveSeconds defines the Time-To-Live (TTL) duration in seconds for this SparkApplication
    /// after its termination.
    /// The SparkApplication object will be garbage collected if the current time is more than the
    /// TimeToLiveSeconds since its termination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeToLiveSeconds")]
    pub time_to_live_seconds: Option<i64>,
    /// Type tells the type of the Spark application.
    #[serde(rename = "type")]
    pub r#type: ScheduledSparkApplicationTemplateType,
    /// Volumes is the list of Kubernetes volumes that can be mounted by the driver and/or executors.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub volumes: Option<Vec<ScheduledSparkApplicationTemplateVolumes>>,
}

/// BatchSchedulerOptions provides fine-grained control on how to batch scheduling.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateBatchSchedulerOptions {
    /// PriorityClassName stands for the name of k8s PriorityClass resource, it's being used in Volcano batch scheduler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// Queue stands for the resource queue which the application belongs to, it's being used in Volcano batch scheduler.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queue: Option<String>,
    /// Resources stands for the resource list custom request for. Usually it is used to define the lower-bound limit.
    /// If specified, volcano scheduler will consider it as the resources requested.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<BTreeMap<String, IntOrString>>,
}

/// Deps captures all possible types of dependencies of a Spark application.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDeps {
    /// ExcludePackages is a list of "groupId:artifactId", to exclude while resolving the
    /// dependencies provided in Packages to avoid dependency conflicts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "excludePackages")]
    pub exclude_packages: Option<Vec<String>>,
    /// Files is a list of files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub files: Option<Vec<String>>,
    /// Jars is a list of JAR files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub jars: Option<Vec<String>>,
    /// Packages is a list of maven coordinates of jars to include on the driver and executor
    /// classpaths. This will search the local maven repo, then maven central and any additional
    /// remote repositories given by the "repositories" option.
    /// Each package should be of the form "groupId:artifactId:version".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub packages: Option<Vec<String>>,
    /// PyFiles is a list of Python files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pyFiles")]
    pub py_files: Option<Vec<String>>,
    /// Repositories is a list of additional remote repositories to search for the maven coordinate
    /// given with the "packages" option.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub repositories: Option<Vec<String>>,
}

/// Driver is the driver specification.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriver {
    /// Affinity specifies the affinity/anti-affinity settings for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<ScheduledSparkApplicationTemplateDriverAffinity>,
    /// Annotations are the Kubernetes annotations to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<BTreeMap<String, String>>,
    /// ConfigMaps carries information of other ConfigMaps to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMaps")]
    pub config_maps: Option<Vec<ScheduledSparkApplicationTemplateDriverConfigMaps>>,
    /// CoreLimit specifies a hard limit on CPU cores for the pod.
    /// Optional
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreLimit")]
    pub core_limit: Option<String>,
    /// CoreRequest is the physical CPU core request for the driver.
    /// Maps to `spark.kubernetes.driver.request.cores` that is available since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreRequest")]
    pub core_request: Option<String>,
    /// Cores maps to `spark.driver.cores` or `spark.executor.cores` for the driver and executors, respectively.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cores: Option<i32>,
    /// DnsConfig dns settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dnsConfig")]
    pub dns_config: Option<ScheduledSparkApplicationTemplateDriverDnsConfig>,
    /// Env carries the environment variables to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<ScheduledSparkApplicationTemplateDriverEnv>>,
    /// EnvFrom is a list of sources to populate environment variables in the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<ScheduledSparkApplicationTemplateDriverEnvFrom>>,
    /// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envSecretKeyRefs")]
    pub env_secret_key_refs: Option<BTreeMap<String, ScheduledSparkApplicationTemplateDriverEnvSecretKeyRefs>>,
    /// EnvVars carries the environment variables to add to the pod.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envVars")]
    pub env_vars: Option<BTreeMap<String, String>>,
    /// GPU specifies GPU requirement for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu: Option<ScheduledSparkApplicationTemplateDriverGpu>,
    /// HostAliases settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostAliases")]
    pub host_aliases: Option<Vec<ScheduledSparkApplicationTemplateDriverHostAliases>>,
    /// HostNetwork indicates whether to request host networking for the pod or not.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostNetwork")]
    pub host_network: Option<bool>,
    /// Image is the container image to use. Overrides Spec.Image if set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// InitContainers is a list of init-containers that run to completion before the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initContainers")]
    pub init_containers: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainers>>,
    /// JavaOptions is a string of extra JVM options to pass to the driver. For instance,
    /// GC settings or other logging.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "javaOptions")]
    pub java_options: Option<String>,
    /// KubernetesMaster is the URL of the Kubernetes master used by the driver to manage executor pods and
    /// other Kubernetes resources. Default to https://kubernetes.default.svc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesMaster")]
    pub kubernetes_master: Option<String>,
    /// Labels are the Kubernetes labels to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<BTreeMap<String, String>>,
    /// Lifecycle for running preStop or postStart commands
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<ScheduledSparkApplicationTemplateDriverLifecycle>,
    /// Memory is the amount of memory to request for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// MemoryOverhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverhead")]
    pub memory_overhead: Option<String>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at SparkApplication level (which will be deprecated).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// PodName is the name of the driver pod that the user creates. This is used for the
    /// in-cluster client mode in which the user creates a client pod where the driver of
    /// the user application runs. It's an error to set this field if Mode is not
    /// in-cluster-client.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podName")]
    pub pod_name: Option<String>,
    /// PodSecurityContext specifies the PodSecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podSecurityContext")]
    pub pod_security_context: Option<ScheduledSparkApplicationTemplateDriverPodSecurityContext>,
    /// Ports settings for the pods, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<ScheduledSparkApplicationTemplateDriverPorts>>,
    /// PriorityClassName is the name of the PriorityClass for the driver pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// SchedulerName specifies the scheduler that will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "schedulerName")]
    pub scheduler_name: Option<String>,
    /// Secrets carries information of secrets to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secrets: Option<Vec<ScheduledSparkApplicationTemplateDriverSecrets>>,
    /// SecurityContext specifies the container's SecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<ScheduledSparkApplicationTemplateDriverSecurityContext>,
    /// ServiceAccount is the name of the custom Kubernetes service account used by the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// ServiceAnnotations defines the annotations to be added to the Kubernetes headless service used by
    /// executors to connect to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels defines the labels to be added to the Kubernetes headless service used by
    /// executors to connect to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ShareProcessNamespace settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shareProcessNamespace")]
    pub share_process_namespace: Option<bool>,
    /// Sidecars is a list of sidecar containers that run along side the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sidecars: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecars>>,
    /// Termination grace period seconds for the pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Tolerations specifies the tolerations listed in ".spec.tolerations" to be applied to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tolerations: Option<Vec<ScheduledSparkApplicationTemplateDriverTolerations>>,
    /// VolumeMounts specifies the volumes listed in ".spec.volumes" to mount into the main container's filesystem.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<ScheduledSparkApplicationTemplateDriverVolumeMounts>>,
}

/// Affinity specifies the affinity/anti-affinity settings for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinity {
    /// Describes node affinity scheduling rules for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinity>,
    /// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAffinity>,
    /// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinity>,
}

/// Describes node affinity scheduling rules for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node matches the corresponding matchExpressions; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to an update), the system
    /// may or may not try to eventually evict the pod from its node.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution>,
}

/// An empty preferred scheduling term matches all objects with implicit weight 0
/// (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// A node selector term, associated with the corresponding weight.
    pub preference: ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference,
    /// Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.
    pub weight: i32,
}

/// A node selector term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// If the affinity requirements specified by this field are not met at
/// scheduling time, the pod will not be scheduled onto the node.
/// If the affinity requirements specified by this field cease to be met
/// at some point during pod execution (e.g. due to an update), the system
/// may or may not try to eventually evict the pod from its node.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// Required. A list of node selector terms. The terms are ORed.
    #[serde(rename = "nodeSelectorTerms")]
    pub node_selector_terms: Vec<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms>,
}

/// A null or empty node selector term matches no objects. The requirements of
/// them are ANDed.
/// The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the anti-affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling anti-affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the anti-affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the anti-affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// NamePath is a pair of a name and a path to which the named objects should be mounted to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverConfigMaps {
    pub name: String,
    pub path: String,
}

/// DnsConfig dns settings for the pod, following the Kubernetes specifications.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverDnsConfig {
    /// A list of DNS name server IP addresses.
    /// This will be appended to the base nameservers generated from DNSPolicy.
    /// Duplicated nameservers will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nameservers: Option<Vec<String>>,
    /// A list of DNS resolver options.
    /// This will be merged with the base options generated from DNSPolicy.
    /// Duplicated entries will be removed. Resolution options given in Options
    /// will override those that appear in the base DNSPolicy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<Vec<ScheduledSparkApplicationTemplateDriverDnsConfigOptions>>,
    /// A list of DNS search domains for host-name lookup.
    /// This will be appended to the base search paths generated from DNSPolicy.
    /// Duplicated search paths will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub searches: Option<Vec<String>>,
}

/// PodDNSConfigOption defines DNS resolver options of a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverDnsConfigOptions {
    /// Required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<ScheduledSparkApplicationTemplateDriverEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<ScheduledSparkApplicationTemplateDriverEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateDriverEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateDriverEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<ScheduledSparkApplicationTemplateDriverEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<ScheduledSparkApplicationTemplateDriverEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateDriverEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
/// Deprecated. Consider using `env` instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverEnvSecretKeyRefs {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// GPU specifies GPU requirement for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverGpu {
    /// Name is GPU resource name, such as: nvidia.com/gpu or amd.com/gpu
    pub name: String,
    /// Quantity is the number of GPUs to request for driver or executor.
    pub quantity: i64,
}

/// HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the
/// pod's hosts file.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverHostAliases {
    /// Hostnames for the above IP address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hostnames: Option<Vec<String>>,
    /// IP address of the host file entry.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ip: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainers {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<ScheduledSparkApplicationTemplateDriverInitContainersResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<ScheduledSparkApplicationTemplateDriverInitContainersSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<ScheduledSparkApplicationTemplateDriverInitContainersStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<ScheduledSparkApplicationTemplateDriverInitContainersEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateDriverInitContainersEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverInitContainersVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// Lifecycle for running preStop or postStart commands
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<ScheduledSparkApplicationTemplateDriverLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<ScheduledSparkApplicationTemplateDriverLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateDriverLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateDriverLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PodSecurityContext specifies the PodSecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverPodSecurityContext {
    /// A special supplemental group that applies to all containers in a pod.
    /// Some volume types allow the Kubelet to change the ownership of that volume
    /// to be owned by the pod:
    /// 
    /// 
    /// 1. The owning GID will be the FSGroup
    /// 2. The setgid bit is set (new files created in the volume will be owned by FSGroup)
    /// 3. The permission bits are OR'd with rw-rw----
    /// 
    /// 
    /// If unset, the Kubelet will not modify the ownership and permissions of any volume.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroup")]
    pub fs_group: Option<i64>,
    /// fsGroupChangePolicy defines behavior of changing ownership and permission of the volume
    /// before being exposed inside Pod. This field will only apply to
    /// volume types which support fsGroup based ownership(and permissions).
    /// It will have no effect on ephemeral volume types such as: secret, configmaps
    /// and emptydir.
    /// Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroupChangePolicy")]
    pub fs_group_change_policy: Option<String>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to all containers.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in SecurityContext.  If set in
    /// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
    /// takes precedence for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateDriverPodSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateDriverPodSecurityContextSeccompProfile>,
    /// A list of groups applied to the first process run in each container, in addition
    /// to the container's primary GID, the fsGroup (if specified), and group memberships
    /// defined in the container image for the uid of the container process. If unspecified,
    /// no additional groups are added to any container. Note that group memberships
    /// defined in the container image for the uid of the container process are still effective,
    /// even if they are not included in this list.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroups")]
    pub supplemental_groups: Option<Vec<i64>>,
    /// Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported
    /// sysctls (by the container runtime) might fail to launch.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sysctls: Option<Vec<ScheduledSparkApplicationTemplateDriverPodSecurityContextSysctls>>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options within a container's SecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateDriverPodSecurityContextWindowsOptions>,
}

/// The SELinux context to be applied to all containers.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in SecurityContext.  If set in
/// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
/// takes precedence for that container.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverPodSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverPodSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Sysctl defines a kernel parameter to be set
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverPodSecurityContextSysctls {
    /// Name of a property to set
    pub name: String,
    /// Value of a property to set
    pub value: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options within a container's SecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverPodSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// Port represents the port definition in the pods objects.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverPorts {
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    pub name: String,
    pub protocol: String,
}

/// SecretInfo captures information of a secret.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSecrets {
    pub name: String,
    pub path: String,
    /// SecretType tells the type of a secret.
    #[serde(rename = "secretType")]
    pub secret_type: String,
}

/// SecurityContext specifies the container's SecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<ScheduledSparkApplicationTemplateDriverSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateDriverSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateDriverSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateDriverSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecars {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<ScheduledSparkApplicationTemplateDriverSidecarsResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<ScheduledSparkApplicationTemplateDriverSidecarsSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<ScheduledSparkApplicationTemplateDriverSidecarsStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<ScheduledSparkApplicationTemplateDriverSidecarsEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateDriverSidecarsEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverSidecarsVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// The pod this Toleration is attached to tolerates any taint that matches
/// the triple <key,value,effect> using the matching operator <operator>.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverTolerations {
    /// Effect indicates the taint effect to match. Empty means match all taint effects.
    /// When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub effect: Option<String>,
    /// Key is the taint key that the toleration applies to. Empty means match all taint keys.
    /// If the key is empty, operator must be Exists; this combination means to match all values and all keys.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Operator represents a key's relationship to the value.
    /// Valid operators are Exists and Equal. Defaults to Equal.
    /// Exists is equivalent to wildcard for value, so that a pod can
    /// tolerate all taints of a particular category.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub operator: Option<String>,
    /// TolerationSeconds represents the period of time the toleration (which must be
    /// of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default,
    /// it is not set, which means tolerate the taint forever (do not evict). Zero and
    /// negative values will be treated as 0 (evict immediately) by the system.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tolerationSeconds")]
    pub toleration_seconds: Option<i64>,
    /// Value is the taint value the toleration matches to.
    /// If the operator is Exists, the value should be empty, otherwise just a regular string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// DriverIngressConfiguration is for driver ingress specific configuration parameters.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverIngressOptions {
    /// IngressAnnotations is a map of key,value pairs of annotations that might be added to the ingress object. i.e. specify nginx as ingress.class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressAnnotations")]
    pub ingress_annotations: Option<BTreeMap<String, String>>,
    /// TlsHosts is useful If we need to declare SSL certificates to the ingress object
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressTLS")]
    pub ingress_tls: Option<Vec<ScheduledSparkApplicationTemplateDriverIngressOptionsIngressTls>>,
    /// IngressURLFormat is the URL for the ingress.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressURLFormat")]
    pub ingress_url_format: Option<String>,
    /// ServiceAnnotations is a map of key,value pairs of annotations that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels is a map of key,value pairs of labels that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ServicePort allows configuring the port at service level that might be different from the targetPort.
    #[serde(rename = "servicePort")]
    pub service_port: i32,
    /// ServicePortName allows configuring the name of the service port.
    /// This may be useful for sidecar proxies like Envoy injected by Istio which require specific ports names to treat traffic as proper HTTP.
    #[serde(rename = "servicePortName")]
    pub service_port_name: String,
    /// ServiceType allows configuring the type of the service. Defaults to ClusterIP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceType")]
    pub service_type: Option<String>,
}

/// IngressTLS describes the transport layer security associated with an ingress.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDriverIngressOptionsIngressTls {
    /// hosts is a list of hosts included in the TLS certificate. The values in
    /// this list must match the name/s used in the tlsSecret. Defaults to the
    /// wildcard host setting for the loadbalancer controller fulfilling this
    /// Ingress, if left unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hosts: Option<Vec<String>>,
    /// secretName is the name of the secret used to terminate TLS traffic on
    /// port 443. Field is left optional to allow TLS routing based on SNI
    /// hostname alone. If the SNI host in a listener conflicts with the "Host"
    /// header field used by an IngressRule, the SNI host is used for termination
    /// and value of the "Host" header is used for routing.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// DynamicAllocation configures dynamic allocation that becomes available for the Kubernetes
/// scheduler backend since Spark 3.0.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateDynamicAllocation {
    /// Enabled controls whether dynamic allocation is enabled or not.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enabled: Option<bool>,
    /// InitialExecutors is the initial number of executors to request. If .spec.executor.instances
    /// is also set, the initial number of executors is set to the bigger of that and this option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialExecutors")]
    pub initial_executors: Option<i32>,
    /// MaxExecutors is the upper bound for the number of executors if dynamic allocation is enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxExecutors")]
    pub max_executors: Option<i32>,
    /// MinExecutors is the lower bound for the number of executors if dynamic allocation is enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minExecutors")]
    pub min_executors: Option<i32>,
    /// ShuffleTrackingTimeout controls the timeout in milliseconds for executors that are holding
    /// shuffle data if shuffle tracking is enabled (true by default if dynamic allocation is enabled).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shuffleTrackingTimeout")]
    pub shuffle_tracking_timeout: Option<i64>,
}

/// Executor is the executor specification.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutor {
    /// Affinity specifies the affinity/anti-affinity settings for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<ScheduledSparkApplicationTemplateExecutorAffinity>,
    /// Annotations are the Kubernetes annotations to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<BTreeMap<String, String>>,
    /// ConfigMaps carries information of other ConfigMaps to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMaps")]
    pub config_maps: Option<Vec<ScheduledSparkApplicationTemplateExecutorConfigMaps>>,
    /// CoreLimit specifies a hard limit on CPU cores for the pod.
    /// Optional
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreLimit")]
    pub core_limit: Option<String>,
    /// CoreRequest is the physical CPU core request for the executors.
    /// Maps to `spark.kubernetes.executor.request.cores` that is available since Spark 2.4.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreRequest")]
    pub core_request: Option<String>,
    /// Cores maps to `spark.driver.cores` or `spark.executor.cores` for the driver and executors, respectively.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cores: Option<i32>,
    /// DeleteOnTermination specify whether executor pods should be deleted in case of failure or normal termination.
    /// Maps to `spark.kubernetes.executor.deleteOnTermination` that is available since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deleteOnTermination")]
    pub delete_on_termination: Option<bool>,
    /// DnsConfig dns settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dnsConfig")]
    pub dns_config: Option<ScheduledSparkApplicationTemplateExecutorDnsConfig>,
    /// Env carries the environment variables to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<ScheduledSparkApplicationTemplateExecutorEnv>>,
    /// EnvFrom is a list of sources to populate environment variables in the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<ScheduledSparkApplicationTemplateExecutorEnvFrom>>,
    /// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envSecretKeyRefs")]
    pub env_secret_key_refs: Option<BTreeMap<String, ScheduledSparkApplicationTemplateExecutorEnvSecretKeyRefs>>,
    /// EnvVars carries the environment variables to add to the pod.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envVars")]
    pub env_vars: Option<BTreeMap<String, String>>,
    /// GPU specifies GPU requirement for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu: Option<ScheduledSparkApplicationTemplateExecutorGpu>,
    /// HostAliases settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostAliases")]
    pub host_aliases: Option<Vec<ScheduledSparkApplicationTemplateExecutorHostAliases>>,
    /// HostNetwork indicates whether to request host networking for the pod or not.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostNetwork")]
    pub host_network: Option<bool>,
    /// Image is the container image to use. Overrides Spec.Image if set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// InitContainers is a list of init-containers that run to completion before the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initContainers")]
    pub init_containers: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainers>>,
    /// Instances is the number of executor instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub instances: Option<i32>,
    /// JavaOptions is a string of extra JVM options to pass to the executors. For instance,
    /// GC settings or other logging.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "javaOptions")]
    pub java_options: Option<String>,
    /// Labels are the Kubernetes labels to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<BTreeMap<String, String>>,
    /// Lifecycle for running preStop or postStart commands
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<ScheduledSparkApplicationTemplateExecutorLifecycle>,
    /// Memory is the amount of memory to request for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// MemoryOverhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverhead")]
    pub memory_overhead: Option<String>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at SparkApplication level (which will be deprecated).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// PodSecurityContext specifies the PodSecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podSecurityContext")]
    pub pod_security_context: Option<ScheduledSparkApplicationTemplateExecutorPodSecurityContext>,
    /// Ports settings for the pods, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<ScheduledSparkApplicationTemplateExecutorPorts>>,
    /// PriorityClassName is the name of the PriorityClass for the executor pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// SchedulerName specifies the scheduler that will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "schedulerName")]
    pub scheduler_name: Option<String>,
    /// Secrets carries information of secrets to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secrets: Option<Vec<ScheduledSparkApplicationTemplateExecutorSecrets>>,
    /// SecurityContext specifies the container's SecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<ScheduledSparkApplicationTemplateExecutorSecurityContext>,
    /// ServiceAccount is the name of the custom Kubernetes service account used by the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// ShareProcessNamespace settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shareProcessNamespace")]
    pub share_process_namespace: Option<bool>,
    /// Sidecars is a list of sidecar containers that run along side the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sidecars: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecars>>,
    /// Termination grace period seconds for the pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Tolerations specifies the tolerations listed in ".spec.tolerations" to be applied to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tolerations: Option<Vec<ScheduledSparkApplicationTemplateExecutorTolerations>>,
    /// VolumeMounts specifies the volumes listed in ".spec.volumes" to mount into the main container's filesystem.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<ScheduledSparkApplicationTemplateExecutorVolumeMounts>>,
}

/// Affinity specifies the affinity/anti-affinity settings for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinity {
    /// Describes node affinity scheduling rules for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinity>,
    /// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinity>,
    /// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinity>,
}

/// Describes node affinity scheduling rules for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node matches the corresponding matchExpressions; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to an update), the system
    /// may or may not try to eventually evict the pod from its node.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution>,
}

/// An empty preferred scheduling term matches all objects with implicit weight 0
/// (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// A node selector term, associated with the corresponding weight.
    pub preference: ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference,
    /// Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.
    pub weight: i32,
}

/// A node selector term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// If the affinity requirements specified by this field are not met at
/// scheduling time, the pod will not be scheduled onto the node.
/// If the affinity requirements specified by this field cease to be met
/// at some point during pod execution (e.g. due to an update), the system
/// may or may not try to eventually evict the pod from its node.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// Required. A list of node selector terms. The terms are ORed.
    #[serde(rename = "nodeSelectorTerms")]
    pub node_selector_terms: Vec<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms>,
}

/// A null or empty node selector term matches no objects. The requirements of
/// them are ANDed.
/// The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the anti-affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling anti-affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the anti-affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the anti-affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// NamePath is a pair of a name and a path to which the named objects should be mounted to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorConfigMaps {
    pub name: String,
    pub path: String,
}

/// DnsConfig dns settings for the pod, following the Kubernetes specifications.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorDnsConfig {
    /// A list of DNS name server IP addresses.
    /// This will be appended to the base nameservers generated from DNSPolicy.
    /// Duplicated nameservers will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nameservers: Option<Vec<String>>,
    /// A list of DNS resolver options.
    /// This will be merged with the base options generated from DNSPolicy.
    /// Duplicated entries will be removed. Resolution options given in Options
    /// will override those that appear in the base DNSPolicy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<Vec<ScheduledSparkApplicationTemplateExecutorDnsConfigOptions>>,
    /// A list of DNS search domains for host-name lookup.
    /// This will be appended to the base search paths generated from DNSPolicy.
    /// Duplicated search paths will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub searches: Option<Vec<String>>,
}

/// PodDNSConfigOption defines DNS resolver options of a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorDnsConfigOptions {
    /// Required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<ScheduledSparkApplicationTemplateExecutorEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<ScheduledSparkApplicationTemplateExecutorEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateExecutorEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateExecutorEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<ScheduledSparkApplicationTemplateExecutorEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<ScheduledSparkApplicationTemplateExecutorEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateExecutorEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
/// Deprecated. Consider using `env` instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorEnvSecretKeyRefs {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// GPU specifies GPU requirement for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorGpu {
    /// Name is GPU resource name, such as: nvidia.com/gpu or amd.com/gpu
    pub name: String,
    /// Quantity is the number of GPUs to request for driver or executor.
    pub quantity: i64,
}

/// HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the
/// pod's hosts file.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorHostAliases {
    /// Hostnames for the above IP address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hostnames: Option<Vec<String>>,
    /// IP address of the host file entry.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ip: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainers {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<ScheduledSparkApplicationTemplateExecutorInitContainersResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<ScheduledSparkApplicationTemplateExecutorInitContainersEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateExecutorInitContainersEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorInitContainersVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// Lifecycle for running preStop or postStart commands
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PodSecurityContext specifies the PodSecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorPodSecurityContext {
    /// A special supplemental group that applies to all containers in a pod.
    /// Some volume types allow the Kubelet to change the ownership of that volume
    /// to be owned by the pod:
    /// 
    /// 
    /// 1. The owning GID will be the FSGroup
    /// 2. The setgid bit is set (new files created in the volume will be owned by FSGroup)
    /// 3. The permission bits are OR'd with rw-rw----
    /// 
    /// 
    /// If unset, the Kubelet will not modify the ownership and permissions of any volume.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroup")]
    pub fs_group: Option<i64>,
    /// fsGroupChangePolicy defines behavior of changing ownership and permission of the volume
    /// before being exposed inside Pod. This field will only apply to
    /// volume types which support fsGroup based ownership(and permissions).
    /// It will have no effect on ephemeral volume types such as: secret, configmaps
    /// and emptydir.
    /// Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroupChangePolicy")]
    pub fs_group_change_policy: Option<String>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to all containers.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in SecurityContext.  If set in
    /// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
    /// takes precedence for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateExecutorPodSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateExecutorPodSecurityContextSeccompProfile>,
    /// A list of groups applied to the first process run in each container, in addition
    /// to the container's primary GID, the fsGroup (if specified), and group memberships
    /// defined in the container image for the uid of the container process. If unspecified,
    /// no additional groups are added to any container. Note that group memberships
    /// defined in the container image for the uid of the container process are still effective,
    /// even if they are not included in this list.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroups")]
    pub supplemental_groups: Option<Vec<i64>>,
    /// Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported
    /// sysctls (by the container runtime) might fail to launch.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sysctls: Option<Vec<ScheduledSparkApplicationTemplateExecutorPodSecurityContextSysctls>>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options within a container's SecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateExecutorPodSecurityContextWindowsOptions>,
}

/// The SELinux context to be applied to all containers.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in SecurityContext.  If set in
/// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
/// takes precedence for that container.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorPodSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorPodSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Sysctl defines a kernel parameter to be set
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorPodSecurityContextSysctls {
    /// Name of a property to set
    pub name: String,
    /// Value of a property to set
    pub value: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options within a container's SecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorPodSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// Port represents the port definition in the pods objects.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorPorts {
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    pub name: String,
    pub protocol: String,
}

/// SecretInfo captures information of a secret.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSecrets {
    pub name: String,
    pub path: String,
    /// SecretType tells the type of a secret.
    #[serde(rename = "secretType")]
    pub secret_type: String,
}

/// SecurityContext specifies the container's SecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<ScheduledSparkApplicationTemplateExecutorSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateExecutorSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateExecutorSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateExecutorSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecars {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<ScheduledSparkApplicationTemplateExecutorSidecarsResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<ScheduledSparkApplicationTemplateExecutorSidecarsEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateExecutorSidecarsEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorSidecarsVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// The pod this Toleration is attached to tolerates any taint that matches
/// the triple <key,value,effect> using the matching operator <operator>.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorTolerations {
    /// Effect indicates the taint effect to match. Empty means match all taint effects.
    /// When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub effect: Option<String>,
    /// Key is the taint key that the toleration applies to. Empty means match all taint keys.
    /// If the key is empty, operator must be Exists; this combination means to match all values and all keys.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Operator represents a key's relationship to the value.
    /// Valid operators are Exists and Equal. Defaults to Equal.
    /// Exists is equivalent to wildcard for value, so that a pod can
    /// tolerate all taints of a particular category.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub operator: Option<String>,
    /// TolerationSeconds represents the period of time the toleration (which must be
    /// of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default,
    /// it is not set, which means tolerate the taint forever (do not evict). Zero and
    /// negative values will be treated as 0 (evict immediately) by the system.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tolerationSeconds")]
    pub toleration_seconds: Option<i64>,
    /// Value is the taint value the toleration matches to.
    /// If the operator is Exists, the value should be empty, otherwise just a regular string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateExecutorVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// Template is a template from which SparkApplication instances can be created.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum ScheduledSparkApplicationTemplateMode {
    #[serde(rename = "cluster")]
    Cluster,
    #[serde(rename = "client")]
    Client,
}

/// Monitoring configures how monitoring is handled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateMonitoring {
    /// ExposeDriverMetrics specifies whether to expose metrics on the driver.
    #[serde(rename = "exposeDriverMetrics")]
    pub expose_driver_metrics: bool,
    /// ExposeExecutorMetrics specifies whether to expose metrics on the executors.
    #[serde(rename = "exposeExecutorMetrics")]
    pub expose_executor_metrics: bool,
    /// MetricsProperties is the content of a custom metrics.properties for configuring the Spark metric system.
    /// If not specified, the content in spark-docker/conf/metrics.properties will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricsProperties")]
    pub metrics_properties: Option<String>,
    /// MetricsPropertiesFile is the container local path of file metrics.properties for configuring
    /// the Spark metric system. If not specified, value /etc/metrics/conf/metrics.properties will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricsPropertiesFile")]
    pub metrics_properties_file: Option<String>,
    /// Prometheus is for configuring the Prometheus JMX exporter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prometheus: Option<ScheduledSparkApplicationTemplateMonitoringPrometheus>,
}

/// Prometheus is for configuring the Prometheus JMX exporter.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateMonitoringPrometheus {
    /// ConfigFile is the path to the custom Prometheus configuration file provided in the Spark image.
    /// ConfigFile takes precedence over Configuration, which is shown below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configFile")]
    pub config_file: Option<String>,
    /// Configuration is the content of the Prometheus configuration needed by the Prometheus JMX exporter.
    /// If not specified, the content in spark-docker/conf/prometheus.yaml will be used.
    /// Configuration has no effect if ConfigFile is set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub configuration: Option<String>,
    /// JmxExporterJar is the path to the Prometheus JMX exporter jar in the container.
    #[serde(rename = "jmxExporterJar")]
    pub jmx_exporter_jar: String,
    /// Port is the port of the HTTP server run by the Prometheus JMX exporter.
    /// If not specified, 8090 will be used as the default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// PortName is the port name of prometheus JMX exporter port.
    /// If not specified, jmx-exporter will be used as the default.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portName")]
    pub port_name: Option<String>,
}

/// Template is a template from which SparkApplication instances can be created.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum ScheduledSparkApplicationTemplatePythonVersion {
    #[serde(rename = "2")]
    r#_2,
    #[serde(rename = "3")]
    r#_3,
}

/// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateRestartPolicy {
    /// OnFailureRetries the number of times to retry running an application before giving up.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onFailureRetries")]
    pub on_failure_retries: Option<i32>,
    /// OnFailureRetryInterval is the interval in seconds between retries on failed runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onFailureRetryInterval")]
    pub on_failure_retry_interval: Option<i64>,
    /// OnSubmissionFailureRetries is the number of times to retry submitting an application before giving up.
    /// This is best effort and actual retry attempts can be >= the value specified due to caching.
    /// These are required if RestartPolicy is OnFailure.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onSubmissionFailureRetries")]
    pub on_submission_failure_retries: Option<i32>,
    /// OnSubmissionFailureRetryInterval is the interval in seconds between retries on failed submissions.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onSubmissionFailureRetryInterval")]
    pub on_submission_failure_retry_interval: Option<i64>,
    /// Type specifies the RestartPolicyType.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<ScheduledSparkApplicationTemplateRestartPolicyType>,
}

/// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum ScheduledSparkApplicationTemplateRestartPolicyType {
    Never,
    Always,
    OnFailure,
}

/// SparkUIOptions allows configuring the Service and the Ingress to expose the sparkUI
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateSparkUiOptions {
    /// IngressAnnotations is a map of key,value pairs of annotations that might be added to the ingress object. i.e. specify nginx as ingress.class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressAnnotations")]
    pub ingress_annotations: Option<BTreeMap<String, String>>,
    /// TlsHosts is useful If we need to declare SSL certificates to the ingress object
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressTLS")]
    pub ingress_tls: Option<Vec<ScheduledSparkApplicationTemplateSparkUiOptionsIngressTls>>,
    /// ServiceAnnotations is a map of key,value pairs of annotations that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels is a map of key,value pairs of labels that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ServicePort allows configuring the port at service level that might be different from the targetPort.
    /// TargetPort should be the same as the one defined in spark.ui.port
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "servicePort")]
    pub service_port: Option<i32>,
    /// ServicePortName allows configuring the name of the service port.
    /// This may be useful for sidecar proxies like Envoy injected by Istio which require specific ports names to treat traffic as proper HTTP.
    /// Defaults to spark-driver-ui-port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "servicePortName")]
    pub service_port_name: Option<String>,
    /// ServiceType allows configuring the type of the service. Defaults to ClusterIP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceType")]
    pub service_type: Option<String>,
}

/// IngressTLS describes the transport layer security associated with an ingress.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateSparkUiOptionsIngressTls {
    /// hosts is a list of hosts included in the TLS certificate. The values in
    /// this list must match the name/s used in the tlsSecret. Defaults to the
    /// wildcard host setting for the loadbalancer controller fulfilling this
    /// Ingress, if left unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hosts: Option<Vec<String>>,
    /// secretName is the name of the secret used to terminate TLS traffic on
    /// port 443. Field is left optional to allow TLS routing based on SNI
    /// hostname alone. If the SNI host in a listener conflicts with the "Host"
    /// header field used by an IngressRule, the SNI host is used for termination
    /// and value of the "Host" header is used for routing.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// Template is a template from which SparkApplication instances can be created.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum ScheduledSparkApplicationTemplateType {
    Java,
    Python,
    Scala,
    R,
}

/// Volume represents a named volume in a pod that may be accessed by any container in the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumes {
    /// awsElasticBlockStore represents an AWS Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "awsElasticBlockStore")]
    pub aws_elastic_block_store: Option<ScheduledSparkApplicationTemplateVolumesAwsElasticBlockStore>,
    /// azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "azureDisk")]
    pub azure_disk: Option<ScheduledSparkApplicationTemplateVolumesAzureDisk>,
    /// azureFile represents an Azure File Service mount on the host and bind mount to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "azureFile")]
    pub azure_file: Option<ScheduledSparkApplicationTemplateVolumesAzureFile>,
    /// cephFS represents a Ceph FS mount on the host that shares a pod's lifetime
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cephfs: Option<ScheduledSparkApplicationTemplateVolumesCephfs>,
    /// cinder represents a cinder volume attached and mounted on kubelets host machine.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cinder: Option<ScheduledSparkApplicationTemplateVolumesCinder>,
    /// configMap represents a configMap that should populate this volume
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<ScheduledSparkApplicationTemplateVolumesConfigMap>,
    /// csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers (Beta feature).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub csi: Option<ScheduledSparkApplicationTemplateVolumesCsi>,
    /// downwardAPI represents downward API about the pod that should populate this volume
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "downwardAPI")]
    pub downward_api: Option<ScheduledSparkApplicationTemplateVolumesDownwardApi>,
    /// emptyDir represents a temporary directory that shares a pod's lifetime.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "emptyDir")]
    pub empty_dir: Option<ScheduledSparkApplicationTemplateVolumesEmptyDir>,
    /// ephemeral represents a volume that is handled by a cluster storage driver.
    /// The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts,
    /// and deleted when the pod is removed.
    /// 
    /// 
    /// Use this if:
    /// a) the volume is only needed while the pod runs,
    /// b) features of normal volumes like restoring from snapshot or capacity
    ///    tracking are needed,
    /// c) the storage driver is specified through a storage class, and
    /// d) the storage driver supports dynamic volume provisioning through
    ///    a PersistentVolumeClaim (see EphemeralVolumeSource for more
    ///    information on the connection between this volume type
    ///    and PersistentVolumeClaim).
    /// 
    /// 
    /// Use PersistentVolumeClaim or one of the vendor-specific
    /// APIs for volumes that persist for longer than the lifecycle
    /// of an individual pod.
    /// 
    /// 
    /// Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to
    /// be used that way - see the documentation of the driver for
    /// more information.
    /// 
    /// 
    /// A pod can use both types of ephemeral volumes and
    /// persistent volumes at the same time.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ephemeral: Option<ScheduledSparkApplicationTemplateVolumesEphemeral>,
    /// fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub fc: Option<ScheduledSparkApplicationTemplateVolumesFc>,
    /// flexVolume represents a generic volume resource that is
    /// provisioned/attached using an exec based plugin.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "flexVolume")]
    pub flex_volume: Option<ScheduledSparkApplicationTemplateVolumesFlexVolume>,
    /// flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub flocker: Option<ScheduledSparkApplicationTemplateVolumesFlocker>,
    /// gcePersistentDisk represents a GCE Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gcePersistentDisk")]
    pub gce_persistent_disk: Option<ScheduledSparkApplicationTemplateVolumesGcePersistentDisk>,
    /// gitRepo represents a git repository at a particular revision.
    /// DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an
    /// EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir
    /// into the Pod's container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gitRepo")]
    pub git_repo: Option<ScheduledSparkApplicationTemplateVolumesGitRepo>,
    /// glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub glusterfs: Option<ScheduledSparkApplicationTemplateVolumesGlusterfs>,
    /// hostPath represents a pre-existing file or directory on the host
    /// machine that is directly exposed to the container. This is generally
    /// used for system agents or other privileged things that are allowed
    /// to see the host machine. Most containers will NOT need this.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    /// ---
    /// TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not
    /// mount host directories as read/write.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPath")]
    pub host_path: Option<ScheduledSparkApplicationTemplateVolumesHostPath>,
    /// iscsi represents an ISCSI Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// More info: https://examples.k8s.io/volumes/iscsi/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub iscsi: Option<ScheduledSparkApplicationTemplateVolumesIscsi>,
    /// name of the volume.
    /// Must be a DNS_LABEL and unique within the pod.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
    /// nfs represents an NFS mount on the host that shares a pod's lifetime
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nfs: Option<ScheduledSparkApplicationTemplateVolumesNfs>,
    /// persistentVolumeClaimVolumeSource represents a reference to a
    /// PersistentVolumeClaim in the same namespace.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "persistentVolumeClaim")]
    pub persistent_volume_claim: Option<ScheduledSparkApplicationTemplateVolumesPersistentVolumeClaim>,
    /// photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "photonPersistentDisk")]
    pub photon_persistent_disk: Option<ScheduledSparkApplicationTemplateVolumesPhotonPersistentDisk>,
    /// portworxVolume represents a portworx volume attached and mounted on kubelets host machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portworxVolume")]
    pub portworx_volume: Option<ScheduledSparkApplicationTemplateVolumesPortworxVolume>,
    /// projected items for all in one resources secrets, configmaps, and downward API
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub projected: Option<ScheduledSparkApplicationTemplateVolumesProjected>,
    /// quobyte represents a Quobyte mount on the host that shares a pod's lifetime
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub quobyte: Option<ScheduledSparkApplicationTemplateVolumesQuobyte>,
    /// rbd represents a Rados Block Device mount on the host that shares a pod's lifetime.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub rbd: Option<ScheduledSparkApplicationTemplateVolumesRbd>,
    /// scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scaleIO")]
    pub scale_io: Option<ScheduledSparkApplicationTemplateVolumesScaleIo>,
    /// secret represents a secret that should populate this volume.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secret: Option<ScheduledSparkApplicationTemplateVolumesSecret>,
    /// storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storageos: Option<ScheduledSparkApplicationTemplateVolumesStorageos>,
    /// vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vsphereVolume")]
    pub vsphere_volume: Option<ScheduledSparkApplicationTemplateVolumesVsphereVolume>,
}

/// awsElasticBlockStore represents an AWS Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesAwsElasticBlockStore {
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// partition is the partition in the volume that you want to mount.
    /// If omitted, the default is to mount by volume name.
    /// Examples: For volume /dev/sda1, you specify the partition as "1".
    /// Similarly, the volume partition for /dev/sda is "0" (or you can leave the property empty).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub partition: Option<i32>,
    /// readOnly value true will force the readOnly setting in VolumeMounts.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeID is unique ID of the persistent disk resource in AWS (Amazon EBS volume).
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesAzureDisk {
    /// cachingMode is the Host Caching mode: None, Read Only, Read Write.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cachingMode")]
    pub caching_mode: Option<String>,
    /// diskName is the Name of the data disk in the blob storage
    #[serde(rename = "diskName")]
    pub disk_name: String,
    /// diskURI is the URI of data disk in the blob storage
    #[serde(rename = "diskURI")]
    pub disk_uri: String,
    /// fsType is Filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// kind expected values are Shared: multiple blob disks per storage account  Dedicated: single blob disk per storage account  Managed: azure managed data disk (only in managed availability set). defaults to shared
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kind: Option<String>,
    /// readOnly Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// azureFile represents an Azure File Service mount on the host and bind mount to the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesAzureFile {
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretName is the  name of secret that contains Azure Storage Account Name and Key
    #[serde(rename = "secretName")]
    pub secret_name: String,
    /// shareName is the azure share Name
    #[serde(rename = "shareName")]
    pub share_name: String,
}

/// cephFS represents a Ceph FS mount on the host that shares a pod's lifetime
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesCephfs {
    /// monitors is Required: Monitors is a collection of Ceph monitors
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    pub monitors: Vec<String>,
    /// path is Optional: Used as the mounted root, rather than the full Ceph tree, default is /
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// readOnly is Optional: Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretFile is Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretFile")]
    pub secret_file: Option<String>,
    /// secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty.
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateVolumesCephfsSecretRef>,
    /// user is optional: User is the rados user name, default is admin
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty.
/// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesCephfsSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// cinder represents a cinder volume attached and mounted on kubelets host machine.
/// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesCinder {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is optional: points to a secret object containing parameters used to connect
    /// to OpenStack.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateVolumesCinderSecretRef>,
    /// volumeID used to identify the volume in cinder.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// secretRef is optional: points to a secret object containing parameters used to connect
/// to OpenStack.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesCinderSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// configMap represents a configMap that should populate this volume
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesConfigMap {
    /// defaultMode is optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// ConfigMap will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the ConfigMap,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<ScheduledSparkApplicationTemplateVolumesConfigMapItems>>,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional specify whether the ConfigMap or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesConfigMapItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers (Beta feature).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesCsi {
    /// driver is the name of the CSI driver that handles this volume.
    /// Consult with your admin for the correct name as registered in the cluster.
    pub driver: String,
    /// fsType to mount. Ex. "ext4", "xfs", "ntfs".
    /// If not provided, the empty value is passed to the associated CSI driver
    /// which will determine the default filesystem to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// nodePublishSecretRef is a reference to the secret object containing
    /// sensitive information to pass to the CSI driver to complete the CSI
    /// NodePublishVolume and NodeUnpublishVolume calls.
    /// This field is optional, and  may be empty if no secret is required. If the
    /// secret object contains more than one secret, all secret references are passed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePublishSecretRef")]
    pub node_publish_secret_ref: Option<ScheduledSparkApplicationTemplateVolumesCsiNodePublishSecretRef>,
    /// readOnly specifies a read-only configuration for the volume.
    /// Defaults to false (read/write).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeAttributes stores driver-specific properties that are passed to the CSI
    /// driver. Consult your driver's documentation for supported values.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeAttributes")]
    pub volume_attributes: Option<BTreeMap<String, String>>,
}

/// nodePublishSecretRef is a reference to the secret object containing
/// sensitive information to pass to the CSI driver to complete the CSI
/// NodePublishVolume and NodeUnpublishVolume calls.
/// This field is optional, and  may be empty if no secret is required. If the
/// secret object contains more than one secret, all secret references are passed.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesCsiNodePublishSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// downwardAPI represents downward API about the pod that should populate this volume
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesDownwardApi {
    /// Optional: mode bits to use on created files by default. Must be a
    /// Optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// Items is a list of downward API volume file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<ScheduledSparkApplicationTemplateVolumesDownwardApiItems>>,
}

/// DownwardAPIVolumeFile represents information to create the file containing the pod field
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesDownwardApiItems {
    /// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateVolumesDownwardApiItemsFieldRef>,
    /// Optional: mode bits used to set permissions on this file, must be an octal value
    /// between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'
    pub path: String,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateVolumesDownwardApiItemsResourceFieldRef>,
}

/// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesDownwardApiItemsFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesDownwardApiItemsResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// emptyDir represents a temporary directory that shares a pod's lifetime.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEmptyDir {
    /// medium represents what type of storage medium should back this directory.
    /// The default is "" which means to use the node's default medium.
    /// Must be an empty string (default) or Memory.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub medium: Option<String>,
    /// sizeLimit is the total amount of local storage required for this EmptyDir volume.
    /// The size limit is also applicable for memory medium.
    /// The maximum usage on memory medium EmptyDir would be the minimum value between
    /// the SizeLimit specified here and the sum of memory limits of all containers in a pod.
    /// The default is nil which means that the limit is undefined.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sizeLimit")]
    pub size_limit: Option<IntOrString>,
}

/// ephemeral represents a volume that is handled by a cluster storage driver.
/// The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts,
/// and deleted when the pod is removed.
/// 
/// 
/// Use this if:
/// a) the volume is only needed while the pod runs,
/// b) features of normal volumes like restoring from snapshot or capacity
///    tracking are needed,
/// c) the storage driver is specified through a storage class, and
/// d) the storage driver supports dynamic volume provisioning through
///    a PersistentVolumeClaim (see EphemeralVolumeSource for more
///    information on the connection between this volume type
///    and PersistentVolumeClaim).
/// 
/// 
/// Use PersistentVolumeClaim or one of the vendor-specific
/// APIs for volumes that persist for longer than the lifecycle
/// of an individual pod.
/// 
/// 
/// Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to
/// be used that way - see the documentation of the driver for
/// more information.
/// 
/// 
/// A pod can use both types of ephemeral volumes and
/// persistent volumes at the same time.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeral {
    /// Will be used to create a stand-alone PVC to provision the volume.
    /// The pod in which this EphemeralVolumeSource is embedded will be the
    /// owner of the PVC, i.e. the PVC will be deleted together with the
    /// pod.  The name of the PVC will be `<pod name>-<volume name>` where
    /// `<volume name>` is the name from the `PodSpec.Volumes` array
    /// entry. Pod validation will reject the pod if the concatenated name
    /// is not valid for a PVC (for example, too long).
    /// 
    /// 
    /// An existing PVC with that name that is not owned by the pod
    /// will *not* be used for the pod to avoid using an unrelated
    /// volume by mistake. Starting the pod is then blocked until
    /// the unrelated PVC is removed. If such a pre-created PVC is
    /// meant to be used by the pod, the PVC has to updated with an
    /// owner reference to the pod once the pod exists. Normally
    /// this should not be necessary, but it may be useful when
    /// manually reconstructing a broken cluster.
    /// 
    /// 
    /// This field is read-only and no changes will be made by Kubernetes
    /// to the PVC after it has been created.
    /// 
    /// 
    /// Required, must not be nil.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeClaimTemplate")]
    pub volume_claim_template: Option<ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplate>,
}

/// Will be used to create a stand-alone PVC to provision the volume.
/// The pod in which this EphemeralVolumeSource is embedded will be the
/// owner of the PVC, i.e. the PVC will be deleted together with the
/// pod.  The name of the PVC will be `<pod name>-<volume name>` where
/// `<volume name>` is the name from the `PodSpec.Volumes` array
/// entry. Pod validation will reject the pod if the concatenated name
/// is not valid for a PVC (for example, too long).
/// 
/// 
/// An existing PVC with that name that is not owned by the pod
/// will *not* be used for the pod to avoid using an unrelated
/// volume by mistake. Starting the pod is then blocked until
/// the unrelated PVC is removed. If such a pre-created PVC is
/// meant to be used by the pod, the PVC has to updated with an
/// owner reference to the pod once the pod exists. Normally
/// this should not be necessary, but it may be useful when
/// manually reconstructing a broken cluster.
/// 
/// 
/// This field is read-only and no changes will be made by Kubernetes
/// to the PVC after it has been created.
/// 
/// 
/// Required, must not be nil.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplate {
    /// May contain labels and annotations that will be copied into the PVC
    /// when creating it. No other fields are allowed and will be rejected during
    /// validation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateMetadata>,
    /// The specification for the PersistentVolumeClaim. The entire content is
    /// copied unchanged into the PVC that gets created from this
    /// template. The same fields as in a PersistentVolumeClaim
    /// are also valid here.
    pub spec: ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpec,
}

/// May contain labels and annotations that will be copied into the PVC
/// when creating it. No other fields are allowed and will be rejected during
/// validation.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateMetadata {
}

/// The specification for the PersistentVolumeClaim. The entire content is
/// copied unchanged into the PVC that gets created from this
/// template. The same fields as in a PersistentVolumeClaim
/// are also valid here.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpec {
    /// accessModes contains the desired access modes the volume should have.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "accessModes")]
    pub access_modes: Option<Vec<String>>,
    /// dataSource field can be used to specify either:
    /// * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)
    /// * An existing PVC (PersistentVolumeClaim)
    /// If the provisioner or an external controller can support the specified data source,
    /// it will create a new volume based on the contents of the specified data source.
    /// When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,
    /// and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.
    /// If the namespace is specified, then dataSourceRef will not be copied to dataSource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSource")]
    pub data_source: Option<ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecDataSource>,
    /// dataSourceRef specifies the object from which to populate the volume with data, if a non-empty
    /// volume is desired. This may be any object from a non-empty API group (non
    /// core object) or a PersistentVolumeClaim object.
    /// When this field is specified, volume binding will only succeed if the type of
    /// the specified object matches some installed volume populator or dynamic
    /// provisioner.
    /// This field will replace the functionality of the dataSource field and as such
    /// if both fields are non-empty, they must have the same value. For backwards
    /// compatibility, when namespace isn't specified in dataSourceRef,
    /// both fields (dataSource and dataSourceRef) will be set to the same
    /// value automatically if one of them is empty and the other is non-empty.
    /// When namespace is specified in dataSourceRef,
    /// dataSource isn't set to the same value and must be empty.
    /// There are three important differences between dataSource and dataSourceRef:
    /// * While dataSource only allows two specific types of objects, dataSourceRef
    ///   allows any non-core object, as well as PersistentVolumeClaim objects.
    /// * While dataSource ignores disallowed values (dropping them), dataSourceRef
    ///   preserves all values, and generates an error if a disallowed value is
    ///   specified.
    /// * While dataSource only allows local objects, dataSourceRef allows objects
    ///   in any namespaces.
    /// (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.
    /// (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSourceRef")]
    pub data_source_ref: Option<ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecDataSourceRef>,
    /// resources represents the minimum resources the volume should have.
    /// If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements
    /// that are lower than previous value but must still be higher than capacity recorded in the
    /// status field of the claim.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecResources>,
    /// selector is a label query over volumes to consider for binding.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecSelector>,
    /// storageClassName is the name of the StorageClass required by the claim.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storageClassName")]
    pub storage_class_name: Option<String>,
    /// volumeAttributesClassName may be used to set the VolumeAttributesClass used by this claim.
    /// If specified, the CSI driver will create or update the volume with the attributes defined
    /// in the corresponding VolumeAttributesClass. This has a different purpose than storageClassName,
    /// it can be changed after the claim is created. An empty string value means that no VolumeAttributesClass
    /// will be applied to the claim but it's not allowed to reset this field to empty string once it is set.
    /// If unspecified and the PersistentVolumeClaim is unbound, the default VolumeAttributesClass
    /// will be set by the persistentvolume controller if it exists.
    /// If the resource referred to by volumeAttributesClass does not exist, this PersistentVolumeClaim will be
    /// set to a Pending state, as reflected by the modifyVolumeStatus field, until such as a resource
    /// exists.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#volumeattributesclass
    /// (Alpha) Using this field requires the VolumeAttributesClass feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeAttributesClassName")]
    pub volume_attributes_class_name: Option<String>,
    /// volumeMode defines what type of volume is required by the claim.
    /// Value of Filesystem is implied when not included in claim spec.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMode")]
    pub volume_mode: Option<String>,
    /// volumeName is the binding reference to the PersistentVolume backing this claim.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
}

/// dataSource field can be used to specify either:
/// * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)
/// * An existing PVC (PersistentVolumeClaim)
/// If the provisioner or an external controller can support the specified data source,
/// it will create a new volume based on the contents of the specified data source.
/// When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,
/// and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.
/// If the namespace is specified, then dataSourceRef will not be copied to dataSource.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecDataSource {
    /// APIGroup is the group for the resource being referenced.
    /// If APIGroup is not specified, the specified Kind must be in the core API group.
    /// For any other third-party types, APIGroup is required.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiGroup")]
    pub api_group: Option<String>,
    /// Kind is the type of resource being referenced
    pub kind: String,
    /// Name is the name of resource being referenced
    pub name: String,
}

/// dataSourceRef specifies the object from which to populate the volume with data, if a non-empty
/// volume is desired. This may be any object from a non-empty API group (non
/// core object) or a PersistentVolumeClaim object.
/// When this field is specified, volume binding will only succeed if the type of
/// the specified object matches some installed volume populator or dynamic
/// provisioner.
/// This field will replace the functionality of the dataSource field and as such
/// if both fields are non-empty, they must have the same value. For backwards
/// compatibility, when namespace isn't specified in dataSourceRef,
/// both fields (dataSource and dataSourceRef) will be set to the same
/// value automatically if one of them is empty and the other is non-empty.
/// When namespace is specified in dataSourceRef,
/// dataSource isn't set to the same value and must be empty.
/// There are three important differences between dataSource and dataSourceRef:
/// * While dataSource only allows two specific types of objects, dataSourceRef
///   allows any non-core object, as well as PersistentVolumeClaim objects.
/// * While dataSource ignores disallowed values (dropping them), dataSourceRef
///   preserves all values, and generates an error if a disallowed value is
///   specified.
/// * While dataSource only allows local objects, dataSourceRef allows objects
///   in any namespaces.
/// (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.
/// (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecDataSourceRef {
    /// APIGroup is the group for the resource being referenced.
    /// If APIGroup is not specified, the specified Kind must be in the core API group.
    /// For any other third-party types, APIGroup is required.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiGroup")]
    pub api_group: Option<String>,
    /// Kind is the type of resource being referenced
    pub kind: String,
    /// Name is the name of resource being referenced
    pub name: String,
    /// Namespace is the namespace of resource being referenced
    /// Note that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details.
    /// (Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// resources represents the minimum resources the volume should have.
/// If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements
/// that are lower than previous value but must still be higher than capacity recorded in the
/// status field of the claim.
/// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecResources {
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// selector is a label query over volumes to consider for binding.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesEphemeralVolumeClaimTemplateSpecSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesFc {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// lun is Optional: FC target lun number
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lun: Option<i32>,
    /// readOnly is Optional: Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// targetWWNs is Optional: FC target worldwide names (WWNs)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetWWNs")]
    pub target_ww_ns: Option<Vec<String>>,
    /// wwids Optional: FC volume world wide identifiers (wwids)
    /// Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub wwids: Option<Vec<String>>,
}

/// flexVolume represents a generic volume resource that is
/// provisioned/attached using an exec based plugin.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesFlexVolume {
    /// driver is the name of the driver to use for this volume.
    pub driver: String,
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". The default filesystem depends on FlexVolume script.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// options is Optional: this field holds extra command options if any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<BTreeMap<String, String>>,
    /// readOnly is Optional: defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is Optional: secretRef is reference to the secret object containing
    /// sensitive information to pass to the plugin scripts. This may be
    /// empty if no secret object is specified. If the secret object
    /// contains more than one secret, all secrets are passed to the plugin
    /// scripts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateVolumesFlexVolumeSecretRef>,
}

/// secretRef is Optional: secretRef is reference to the secret object containing
/// sensitive information to pass to the plugin scripts. This may be
/// empty if no secret object is specified. If the secret object
/// contains more than one secret, all secrets are passed to the plugin
/// scripts.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesFlexVolumeSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesFlocker {
    /// datasetName is Name of the dataset stored as metadata -> name on the dataset for Flocker
    /// should be considered as deprecated
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datasetName")]
    pub dataset_name: Option<String>,
    /// datasetUUID is the UUID of the dataset. This is unique identifier of a Flocker dataset
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datasetUUID")]
    pub dataset_uuid: Option<String>,
}

/// gcePersistentDisk represents a GCE Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesGcePersistentDisk {
    /// fsType is filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// partition is the partition in the volume that you want to mount.
    /// If omitted, the default is to mount by volume name.
    /// Examples: For volume /dev/sda1, you specify the partition as "1".
    /// Similarly, the volume partition for /dev/sda is "0" (or you can leave the property empty).
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub partition: Option<i32>,
    /// pdName is unique name of the PD resource in GCE. Used to identify the disk in GCE.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(rename = "pdName")]
    pub pd_name: String,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// gitRepo represents a git repository at a particular revision.
/// DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an
/// EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir
/// into the Pod's container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesGitRepo {
    /// directory is the target directory name.
    /// Must not contain or start with '..'.  If '.' is supplied, the volume directory will be the
    /// git repository.  Otherwise, if specified, the volume will contain the git repository in
    /// the subdirectory with the given name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub directory: Option<String>,
    /// repository is the URL
    pub repository: String,
    /// revision is the commit hash for the specified revision.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub revision: Option<String>,
}

/// glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime.
/// More info: https://examples.k8s.io/volumes/glusterfs/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesGlusterfs {
    /// endpoints is the endpoint name that details Glusterfs topology.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    pub endpoints: String,
    /// path is the Glusterfs volume path.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    pub path: String,
    /// readOnly here will force the Glusterfs volume to be mounted with read-only permissions.
    /// Defaults to false.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// hostPath represents a pre-existing file or directory on the host
/// machine that is directly exposed to the container. This is generally
/// used for system agents or other privileged things that are allowed
/// to see the host machine. Most containers will NOT need this.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
/// ---
/// TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not
/// mount host directories as read/write.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesHostPath {
    /// path of the directory on the host.
    /// If the path is a symlink, it will follow the link to the real path.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    pub path: String,
    /// type for HostPath Volume
    /// Defaults to ""
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// iscsi represents an ISCSI Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// More info: https://examples.k8s.io/volumes/iscsi/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesIscsi {
    /// chapAuthDiscovery defines whether support iSCSI Discovery CHAP authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "chapAuthDiscovery")]
    pub chap_auth_discovery: Option<bool>,
    /// chapAuthSession defines whether support iSCSI Session CHAP authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "chapAuthSession")]
    pub chap_auth_session: Option<bool>,
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// initiatorName is the custom iSCSI Initiator Name.
    /// If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface
    /// <target portal>:<volume name> will be created for the connection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initiatorName")]
    pub initiator_name: Option<String>,
    /// iqn is the target iSCSI Qualified Name.
    pub iqn: String,
    /// iscsiInterface is the interface Name that uses an iSCSI transport.
    /// Defaults to 'default' (tcp).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "iscsiInterface")]
    pub iscsi_interface: Option<String>,
    /// lun represents iSCSI Target Lun number.
    pub lun: i32,
    /// portals is the iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port
    /// is other than default (typically TCP ports 860 and 3260).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub portals: Option<Vec<String>>,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is the CHAP Secret for iSCSI target and initiator authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateVolumesIscsiSecretRef>,
    /// targetPortal is iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port
    /// is other than default (typically TCP ports 860 and 3260).
    #[serde(rename = "targetPortal")]
    pub target_portal: String,
}

/// secretRef is the CHAP Secret for iSCSI target and initiator authentication
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesIscsiSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// nfs represents an NFS mount on the host that shares a pod's lifetime
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesNfs {
    /// path that is exported by the NFS server.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    pub path: String,
    /// readOnly here will force the NFS export to be mounted with read-only permissions.
    /// Defaults to false.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// server is the hostname or IP address of the NFS server.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    pub server: String,
}

/// persistentVolumeClaimVolumeSource represents a reference to a
/// PersistentVolumeClaim in the same namespace.
/// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesPersistentVolumeClaim {
    /// claimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
    #[serde(rename = "claimName")]
    pub claim_name: String,
    /// readOnly Will force the ReadOnly setting in VolumeMounts.
    /// Default false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesPhotonPersistentDisk {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// pdID is the ID that identifies Photon Controller persistent disk
    #[serde(rename = "pdID")]
    pub pd_id: String,
}

/// portworxVolume represents a portworx volume attached and mounted on kubelets host machine
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesPortworxVolume {
    /// fSType represents the filesystem type to mount
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeID uniquely identifies a Portworx volume
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// projected items for all in one resources secrets, configmaps, and downward API
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjected {
    /// defaultMode are the mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// sources is the list of volume projections
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sources: Option<Vec<ScheduledSparkApplicationTemplateVolumesProjectedSources>>,
}

/// Projection that may be projected along with other supported volume types
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSources {
    /// ClusterTrustBundle allows a pod to access the `.spec.trustBundle` field
    /// of ClusterTrustBundle objects in an auto-updating file.
    /// 
    /// 
    /// Alpha, gated by the ClusterTrustBundleProjection feature gate.
    /// 
    /// 
    /// ClusterTrustBundle objects can either be selected by name, or by the
    /// combination of signer name and a label selector.
    /// 
    /// 
    /// Kubelet performs aggressive normalization of the PEM contents written
    /// into the pod filesystem.  Esoteric PEM features such as inter-block
    /// comments and block headers are stripped.  Certificates are deduplicated.
    /// The ordering of certificates within the file is arbitrary, and Kubelet
    /// may change the order over time.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterTrustBundle")]
    pub cluster_trust_bundle: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesClusterTrustBundle>,
    /// configMap information about the configMap data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesConfigMap>,
    /// downwardAPI information about the downwardAPI data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "downwardAPI")]
    pub downward_api: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApi>,
    /// secret information about the secret data to project
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secret: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesSecret>,
    /// serviceAccountToken is information about the serviceAccountToken data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountToken")]
    pub service_account_token: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesServiceAccountToken>,
}

/// ClusterTrustBundle allows a pod to access the `.spec.trustBundle` field
/// of ClusterTrustBundle objects in an auto-updating file.
/// 
/// 
/// Alpha, gated by the ClusterTrustBundleProjection feature gate.
/// 
/// 
/// ClusterTrustBundle objects can either be selected by name, or by the
/// combination of signer name and a label selector.
/// 
/// 
/// Kubelet performs aggressive normalization of the PEM contents written
/// into the pod filesystem.  Esoteric PEM features such as inter-block
/// comments and block headers are stripped.  Certificates are deduplicated.
/// The ordering of certificates within the file is arbitrary, and Kubelet
/// may change the order over time.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesClusterTrustBundle {
    /// Select all ClusterTrustBundles that match this label selector.  Only has
    /// effect if signerName is set.  Mutually-exclusive with name.  If unset,
    /// interpreted as "match nothing".  If set but empty, interpreted as "match
    /// everything".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesClusterTrustBundleLabelSelector>,
    /// Select a single ClusterTrustBundle by object name.  Mutually-exclusive
    /// with signerName and labelSelector.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// If true, don't block pod startup if the referenced ClusterTrustBundle(s)
    /// aren't available.  If using name, then the named ClusterTrustBundle is
    /// allowed not to exist.  If using signerName, then the combination of
    /// signerName and labelSelector is allowed to match zero
    /// ClusterTrustBundles.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
    /// Relative path from the volume root to write the bundle.
    pub path: String,
    /// Select all ClusterTrustBundles that match this signer name.
    /// Mutually-exclusive with name.  The contents of all selected
    /// ClusterTrustBundles will be unified and deduplicated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "signerName")]
    pub signer_name: Option<String>,
}

/// Select all ClusterTrustBundles that match this label selector.  Only has
/// effect if signerName is set.  Mutually-exclusive with name.  If unset,
/// interpreted as "match nothing".  If set but empty, interpreted as "match
/// everything".
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesClusterTrustBundleLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<ScheduledSparkApplicationTemplateVolumesProjectedSourcesClusterTrustBundleLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesClusterTrustBundleLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// configMap information about the configMap data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesConfigMap {
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// ConfigMap will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the ConfigMap,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<ScheduledSparkApplicationTemplateVolumesProjectedSourcesConfigMapItems>>,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional specify whether the ConfigMap or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesConfigMapItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// downwardAPI information about the downwardAPI data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApi {
    /// Items is a list of DownwardAPIVolume file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApiItems>>,
}

/// DownwardAPIVolumeFile represents information to create the file containing the pod field
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApiItems {
    /// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApiItemsFieldRef>,
    /// Optional: mode bits used to set permissions on this file, must be an octal value
    /// between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'
    pub path: String,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApiItemsResourceFieldRef>,
}

/// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApiItemsFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesDownwardApiItemsResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// secret information about the secret data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesSecret {
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// Secret will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the Secret,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<ScheduledSparkApplicationTemplateVolumesProjectedSourcesSecretItems>>,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional field specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesSecretItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// serviceAccountToken is information about the serviceAccountToken data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesProjectedSourcesServiceAccountToken {
    /// audience is the intended audience of the token. A recipient of a token
    /// must identify itself with an identifier specified in the audience of the
    /// token, and otherwise should reject the token. The audience defaults to the
    /// identifier of the apiserver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub audience: Option<String>,
    /// expirationSeconds is the requested duration of validity of the service
    /// account token. As the token approaches expiration, the kubelet volume
    /// plugin will proactively rotate the service account token. The kubelet will
    /// start trying to rotate the token if the token is older than 80 percent of
    /// its time to live or if the token is older than 24 hours.Defaults to 1 hour
    /// and must be at least 10 minutes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "expirationSeconds")]
    pub expiration_seconds: Option<i64>,
    /// path is the path relative to the mount point of the file to project the
    /// token into.
    pub path: String,
}

/// quobyte represents a Quobyte mount on the host that shares a pod's lifetime
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesQuobyte {
    /// group to map volume access to
    /// Default is no group
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub group: Option<String>,
    /// readOnly here will force the Quobyte volume to be mounted with read-only permissions.
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// registry represents a single or multiple Quobyte Registry services
    /// specified as a string as host:port pair (multiple entries are separated with commas)
    /// which acts as the central registry for volumes
    pub registry: String,
    /// tenant owning the given Quobyte volume in the Backend
    /// Used with dynamically provisioned Quobyte volumes, value is set by the plugin
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tenant: Option<String>,
    /// user to map volume access to
    /// Defaults to serivceaccount user
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
    /// volume is a string that references an already created Quobyte volume by name.
    pub volume: String,
}

/// rbd represents a Rados Block Device mount on the host that shares a pod's lifetime.
/// More info: https://examples.k8s.io/volumes/rbd/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesRbd {
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// image is the rados image name.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    pub image: String,
    /// keyring is the path to key ring for RBDUser.
    /// Default is /etc/ceph/keyring.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub keyring: Option<String>,
    /// monitors is a collection of Ceph monitors.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    pub monitors: Vec<String>,
    /// pool is the rados pool name.
    /// Default is rbd.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pool: Option<String>,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is name of the authentication secret for RBDUser. If provided
    /// overrides keyring.
    /// Default is nil.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateVolumesRbdSecretRef>,
    /// user is the rados user name.
    /// Default is admin.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// secretRef is name of the authentication secret for RBDUser. If provided
/// overrides keyring.
/// Default is nil.
/// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesRbdSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesScaleIo {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs".
    /// Default is "xfs".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// gateway is the host address of the ScaleIO API Gateway.
    pub gateway: String,
    /// protectionDomain is the name of the ScaleIO Protection Domain for the configured storage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "protectionDomain")]
    pub protection_domain: Option<String>,
    /// readOnly Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef references to the secret for ScaleIO user and other
    /// sensitive information. If this is not provided, Login operation will fail.
    #[serde(rename = "secretRef")]
    pub secret_ref: ScheduledSparkApplicationTemplateVolumesScaleIoSecretRef,
    /// sslEnabled Flag enable/disable SSL communication with Gateway, default false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sslEnabled")]
    pub ssl_enabled: Option<bool>,
    /// storageMode indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned.
    /// Default is ThinProvisioned.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storageMode")]
    pub storage_mode: Option<String>,
    /// storagePool is the ScaleIO Storage Pool associated with the protection domain.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePool")]
    pub storage_pool: Option<String>,
    /// system is the name of the storage system as configured in ScaleIO.
    pub system: String,
    /// volumeName is the name of a volume already created in the ScaleIO system
    /// that is associated with this volume source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
}

/// secretRef references to the secret for ScaleIO user and other
/// sensitive information. If this is not provided, Login operation will fail.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesScaleIoSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// secret represents a secret that should populate this volume.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesSecret {
    /// defaultMode is Optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values
    /// for mode bits. Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// items If unspecified, each key-value pair in the Data field of the referenced
    /// Secret will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the Secret,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<ScheduledSparkApplicationTemplateVolumesSecretItems>>,
    /// optional field specify whether the Secret or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
    /// secretName is the name of the secret in the pod's namespace to use.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesSecretItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesStorageos {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef specifies the secret to use for obtaining the StorageOS API
    /// credentials.  If not specified, default values will be attempted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<ScheduledSparkApplicationTemplateVolumesStorageosSecretRef>,
    /// volumeName is the human-readable name of the StorageOS volume.  Volume
    /// names are only unique within a namespace.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
    /// volumeNamespace specifies the scope of the volume within StorageOS.  If no
    /// namespace is specified then the Pod's namespace will be used.  This allows the
    /// Kubernetes name scoping to be mirrored within StorageOS for tighter integration.
    /// Set VolumeName to any name to override the default behaviour.
    /// Set to "default" if you are not using namespaces within StorageOS.
    /// Namespaces that do not pre-exist within StorageOS will be created.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeNamespace")]
    pub volume_namespace: Option<String>,
}

/// secretRef specifies the secret to use for obtaining the StorageOS API
/// credentials.  If not specified, default values will be attempted.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesStorageosSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationTemplateVolumesVsphereVolume {
    /// fsType is filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// storagePolicyID is the storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePolicyID")]
    pub storage_policy_id: Option<String>,
    /// storagePolicyName is the storage Policy Based Management (SPBM) profile name.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePolicyName")]
    pub storage_policy_name: Option<String>,
    /// volumePath is the path that identifies vSphere volume vmdk
    #[serde(rename = "volumePath")]
    pub volume_path: String,
}

/// ScheduledSparkApplicationStatus defines the observed state of ScheduledSparkApplication.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct ScheduledSparkApplicationStatus {
    /// LastRun is the time when the last run of the application started.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lastRun")]
    pub last_run: Option<String>,
    /// LastRunName is the name of the SparkApplication for the most recent run of the application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lastRunName")]
    pub last_run_name: Option<String>,
    /// NextRun is the time when the next run of the application will start.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nextRun")]
    pub next_run: Option<String>,
    /// PastFailedRunNames keeps the names of SparkApplications for past failed runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pastFailedRunNames")]
    pub past_failed_run_names: Option<Vec<String>>,
    /// PastSuccessfulRunNames keeps the names of SparkApplications for past successful runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pastSuccessfulRunNames")]
    pub past_successful_run_names: Option<Vec<String>>,
    /// Reason tells why the ScheduledSparkApplication is in the particular ScheduleState.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reason: Option<String>,
    /// ScheduleState is the current scheduling state of the application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleState")]
    pub schedule_state: Option<String>,
}

