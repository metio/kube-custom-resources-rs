// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --filename=./crd-catalog/GoogleCloudPlatform/spark-on-k8s-operator/sparkoperator.k8s.io/v1beta2/sparkapplications.yaml --derive=Default --derive=PartialEq --smart-derive-elision
// kopium version: 0.21.1

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
    pub use k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;
}
use self::prelude::*;

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, PartialEq)]
#[kube(group = "sparkoperator.k8s.io", version = "v1beta2", kind = "SparkApplication", plural = "sparkapplications")]
#[kube(namespaced)]
#[kube(status = "SparkApplicationStatus")]
#[kube(schema = "disabled")]
#[kube(derive="PartialEq")]
pub struct SparkApplicationSpec {
    /// Arguments is a list of arguments to be passed to the application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arguments: Option<Vec<String>>,
    /// BatchScheduler configures which batch scheduler will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchScheduler")]
    pub batch_scheduler: Option<String>,
    /// BatchSchedulerOptions provides fine-grained control on how to batch scheduling.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSchedulerOptions")]
    pub batch_scheduler_options: Option<SparkApplicationBatchSchedulerOptions>,
    /// Deps captures all possible types of dependencies of a Spark application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub deps: Option<SparkApplicationDeps>,
    /// Driver is the driver specification.
    pub driver: SparkApplicationDriver,
    /// DriverIngressOptions allows configuring the Service and the Ingress to expose ports inside Spark Driver
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverIngressOptions")]
    pub driver_ingress_options: Option<Vec<SparkApplicationDriverIngressOptions>>,
    /// DynamicAllocation configures dynamic allocation that becomes available for the Kubernetes
    /// scheduler backend since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dynamicAllocation")]
    pub dynamic_allocation: Option<SparkApplicationDynamicAllocation>,
    /// Executor is the executor specification.
    pub executor: SparkApplicationExecutor,
    /// FailureRetries is the number of times to retry a failed application before giving up.
    /// This is best effort and actual retry attempts can be >= the value specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureRetries")]
    pub failure_retries: Option<i32>,
    /// HadoopConf carries user-specified Hadoop configuration properties as they would use the  the "--conf" option
    /// in spark-submit.  The SparkApplication controller automatically adds prefix "spark.hadoop." to Hadoop
    /// configuration properties.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConf")]
    pub hadoop_conf: Option<BTreeMap<String, String>>,
    /// HadoopConfigMap carries the name of the ConfigMap containing Hadoop configuration files such as core-site.xml.
    /// The controller will add environment variable HADOOP_CONF_DIR to the path where the ConfigMap is mounted to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConfigMap")]
    pub hadoop_config_map: Option<String>,
    /// Image is the container image for the driver, executor, and init-container. Any custom container images for the
    /// driver, executor, or init-container takes precedence over this.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// ImagePullPolicy is the image pull policy for the driver, executor, and init-container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// ImagePullSecrets is the list of image-pull secrets.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullSecrets")]
    pub image_pull_secrets: Option<Vec<String>>,
    /// MainFile is the path to a bundled JAR, Python, or R file of the application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainApplicationFile")]
    pub main_application_file: Option<String>,
    /// MainClass is the fully-qualified main class of the Spark application.
    /// This only applies to Java/Scala Spark applications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// This sets the Memory Overhead Factor that will allocate memory to non-JVM memory.
    /// For JVM-based jobs this value will default to 0.10, for non-JVM jobs 0.40. Value of this field will
    /// be overridden by `Spec.Driver.MemoryOverhead` and `Spec.Executor.MemoryOverhead` if they are set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverheadFactor")]
    pub memory_overhead_factor: Option<String>,
    /// Mode is the deployment mode of the Spark application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<SparkApplicationMode>,
    /// Monitoring configures how monitoring is handled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub monitoring: Option<SparkApplicationMonitoring>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at podSpec level (driver or executor).
    /// This field will be deprecated in future versions (at SparkApplicationSpec level).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// ProxyUser specifies the user to impersonate when submitting the application.
    /// It maps to the command-line flag "--proxy-user" in spark-submit.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "proxyUser")]
    pub proxy_user: Option<String>,
    /// This sets the major Python version of the docker
    /// image used to run the driver and executor containers. Can either be 2 or 3, default 2.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonVersion")]
    pub python_version: Option<SparkApplicationPythonVersion>,
    /// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<SparkApplicationRestartPolicy>,
    /// RetryInterval is the unit of intervals in seconds between submission retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryInterval")]
    pub retry_interval: Option<i64>,
    /// SparkConf carries user-specified Spark configuration properties as they would use the  "--conf" option in
    /// spark-submit.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConf")]
    pub spark_conf: Option<BTreeMap<String, String>>,
    /// SparkConfigMap carries the name of the ConfigMap containing Spark configuration files such as log4j.properties.
    /// The controller will add environment variable SPARK_CONF_DIR to the path where the ConfigMap is mounted to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConfigMap")]
    pub spark_config_map: Option<String>,
    /// SparkUIOptions allows configuring the Service and the Ingress to expose the sparkUI
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkUIOptions")]
    pub spark_ui_options: Option<SparkApplicationSparkUiOptions>,
    /// SparkVersion is the version of Spark the application uses.
    #[serde(rename = "sparkVersion")]
    pub spark_version: String,
    /// TimeToLiveSeconds defines the Time-To-Live (TTL) duration in seconds for this SparkApplication
    /// after its termination.
    /// The SparkApplication object will be garbage collected if the current time is more than the
    /// TimeToLiveSeconds since its termination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeToLiveSeconds")]
    pub time_to_live_seconds: Option<i64>,
    /// Type tells the type of the Spark application.
    #[serde(rename = "type")]
    pub r#type: SparkApplicationType,
    /// Volumes is the list of Kubernetes volumes that can be mounted by the driver and/or executors.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub volumes: Option<Vec<SparkApplicationVolumes>>,
}

/// BatchSchedulerOptions provides fine-grained control on how to batch scheduling.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationBatchSchedulerOptions {
    /// PriorityClassName stands for the name of k8s PriorityClass resource, it's being used in Volcano batch scheduler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// Queue stands for the resource queue which the application belongs to, it's being used in Volcano batch scheduler.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queue: Option<String>,
    /// Resources stands for the resource list custom request for. Usually it is used to define the lower-bound limit.
    /// If specified, volcano scheduler will consider it as the resources requested.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<BTreeMap<String, IntOrString>>,
}

/// Deps captures all possible types of dependencies of a Spark application.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDeps {
    /// ExcludePackages is a list of "groupId:artifactId", to exclude while resolving the
    /// dependencies provided in Packages to avoid dependency conflicts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "excludePackages")]
    pub exclude_packages: Option<Vec<String>>,
    /// Files is a list of files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub files: Option<Vec<String>>,
    /// Jars is a list of JAR files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub jars: Option<Vec<String>>,
    /// Packages is a list of maven coordinates of jars to include on the driver and executor
    /// classpaths. This will search the local maven repo, then maven central and any additional
    /// remote repositories given by the "repositories" option.
    /// Each package should be of the form "groupId:artifactId:version".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub packages: Option<Vec<String>>,
    /// PyFiles is a list of Python files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pyFiles")]
    pub py_files: Option<Vec<String>>,
    /// Repositories is a list of additional remote repositories to search for the maven coordinate
    /// given with the "packages" option.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub repositories: Option<Vec<String>>,
}

/// Driver is the driver specification.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriver {
    /// Affinity specifies the affinity/anti-affinity settings for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<SparkApplicationDriverAffinity>,
    /// Annotations are the Kubernetes annotations to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<BTreeMap<String, String>>,
    /// ConfigMaps carries information of other ConfigMaps to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMaps")]
    pub config_maps: Option<Vec<SparkApplicationDriverConfigMaps>>,
    /// CoreLimit specifies a hard limit on CPU cores for the pod.
    /// Optional
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreLimit")]
    pub core_limit: Option<String>,
    /// CoreRequest is the physical CPU core request for the driver.
    /// Maps to `spark.kubernetes.driver.request.cores` that is available since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreRequest")]
    pub core_request: Option<String>,
    /// Cores maps to `spark.driver.cores` or `spark.executor.cores` for the driver and executors, respectively.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cores: Option<i32>,
    /// DnsConfig dns settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dnsConfig")]
    pub dns_config: Option<SparkApplicationDriverDnsConfig>,
    /// Env carries the environment variables to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationDriverEnv>>,
    /// EnvFrom is a list of sources to populate environment variables in the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationDriverEnvFrom>>,
    /// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envSecretKeyRefs")]
    pub env_secret_key_refs: Option<BTreeMap<String, SparkApplicationDriverEnvSecretKeyRefs>>,
    /// EnvVars carries the environment variables to add to the pod.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envVars")]
    pub env_vars: Option<BTreeMap<String, String>>,
    /// GPU specifies GPU requirement for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu: Option<SparkApplicationDriverGpu>,
    /// HostAliases settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostAliases")]
    pub host_aliases: Option<Vec<SparkApplicationDriverHostAliases>>,
    /// HostNetwork indicates whether to request host networking for the pod or not.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostNetwork")]
    pub host_network: Option<bool>,
    /// Image is the container image to use. Overrides Spec.Image if set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// InitContainers is a list of init-containers that run to completion before the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initContainers")]
    pub init_containers: Option<Vec<SparkApplicationDriverInitContainers>>,
    /// JavaOptions is a string of extra JVM options to pass to the driver. For instance,
    /// GC settings or other logging.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "javaOptions")]
    pub java_options: Option<String>,
    /// KubernetesMaster is the URL of the Kubernetes master used by the driver to manage executor pods and
    /// other Kubernetes resources. Default to https://kubernetes.default.svc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesMaster")]
    pub kubernetes_master: Option<String>,
    /// Labels are the Kubernetes labels to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<BTreeMap<String, String>>,
    /// Lifecycle for running preStop or postStart commands
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationDriverLifecycle>,
    /// Memory is the amount of memory to request for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// MemoryOverhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverhead")]
    pub memory_overhead: Option<String>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at SparkApplication level (which will be deprecated).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// PodName is the name of the driver pod that the user creates. This is used for the
    /// in-cluster client mode in which the user creates a client pod where the driver of
    /// the user application runs. It's an error to set this field if Mode is not
    /// in-cluster-client.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podName")]
    pub pod_name: Option<String>,
    /// PodSecurityContext specifies the PodSecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podSecurityContext")]
    pub pod_security_context: Option<SparkApplicationDriverPodSecurityContext>,
    /// Ports settings for the pods, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationDriverPorts>>,
    /// PriorityClassName is the name of the PriorityClass for the driver pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// SchedulerName specifies the scheduler that will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "schedulerName")]
    pub scheduler_name: Option<String>,
    /// Secrets carries information of secrets to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secrets: Option<Vec<SparkApplicationDriverSecrets>>,
    /// SecurityContext specifies the container's SecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationDriverSecurityContext>,
    /// ServiceAccount is the name of the custom Kubernetes service account used by the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// ServiceAnnotations defines the annotations to be added to the Kubernetes headless service used by
    /// executors to connect to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels defines the labels to be added to the Kubernetes headless service used by
    /// executors to connect to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ShareProcessNamespace settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shareProcessNamespace")]
    pub share_process_namespace: Option<bool>,
    /// Sidecars is a list of sidecar containers that run along side the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sidecars: Option<Vec<SparkApplicationDriverSidecars>>,
    /// Termination grace period seconds for the pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Tolerations specifies the tolerations listed in ".spec.tolerations" to be applied to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tolerations: Option<Vec<SparkApplicationDriverTolerations>>,
    /// VolumeMounts specifies the volumes listed in ".spec.volumes" to mount into the main container's filesystem.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationDriverVolumeMounts>>,
}

/// Affinity specifies the affinity/anti-affinity settings for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinity {
    /// Describes node affinity scheduling rules for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<SparkApplicationDriverAffinityNodeAffinity>,
    /// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<SparkApplicationDriverAffinityPodAffinity>,
    /// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<SparkApplicationDriverAffinityPodAntiAffinity>,
}

/// Describes node affinity scheduling rules for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node matches the corresponding matchExpressions; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to an update), the system
    /// may or may not try to eventually evict the pod from its node.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution>,
}

/// An empty preferred scheduling term matches all objects with implicit weight 0
/// (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// A node selector term, associated with the corresponding weight.
    pub preference: SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference,
    /// Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.
    pub weight: i32,
}

/// A node selector term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// If the affinity requirements specified by this field are not met at
/// scheduling time, the pod will not be scheduled onto the node.
/// If the affinity requirements specified by this field cease to be met
/// at some point during pod execution (e.g. due to an update), the system
/// may or may not try to eventually evict the pod from its node.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// Required. A list of node selector terms. The terms are ORed.
    #[serde(rename = "nodeSelectorTerms")]
    pub node_selector_terms: Vec<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms>,
}

/// A null or empty node selector term matches no objects. The requirements of
/// them are ANDed.
/// The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the anti-affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling anti-affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the anti-affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the anti-affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// NamePath is a pair of a name and a path to which the named objects should be mounted to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigMaps {
    pub name: String,
    pub path: String,
}

/// DnsConfig dns settings for the pod, following the Kubernetes specifications.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverDnsConfig {
    /// A list of DNS name server IP addresses.
    /// This will be appended to the base nameservers generated from DNSPolicy.
    /// Duplicated nameservers will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nameservers: Option<Vec<String>>,
    /// A list of DNS resolver options.
    /// This will be merged with the base options generated from DNSPolicy.
    /// Duplicated entries will be removed. Resolution options given in Options
    /// will override those that appear in the base DNSPolicy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<Vec<SparkApplicationDriverDnsConfigOptions>>,
    /// A list of DNS search domains for host-name lookup.
    /// This will be appended to the base search paths generated from DNSPolicy.
    /// Duplicated search paths will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub searches: Option<Vec<String>>,
}

/// PodDNSConfigOption defines DNS resolver options of a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverDnsConfigOptions {
    /// Required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationDriverEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationDriverEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationDriverEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationDriverEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationDriverEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationDriverEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationDriverEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
/// Deprecated. Consider using `env` instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvSecretKeyRefs {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// GPU specifies GPU requirement for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverGpu {
    /// Name is GPU resource name, such as: nvidia.com/gpu or amd.com/gpu
    pub name: String,
    /// Quantity is the number of GPUs to request for driver or executor.
    pub quantity: i64,
}

/// HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the
/// pod's hosts file.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverHostAliases {
    /// Hostnames for the above IP address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hostnames: Option<Vec<String>>,
    /// IP address of the host file entry.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ip: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainers {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationDriverInitContainersEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationDriverInitContainersEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationDriverInitContainersLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationDriverInitContainersLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationDriverInitContainersPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationDriverInitContainersReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationDriverInitContainersResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationDriverInitContainersResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationDriverInitContainersSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationDriverInitContainersStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationDriverInitContainersVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationDriverInitContainersVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationDriverInitContainersEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationDriverInitContainersEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationDriverInitContainersEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationDriverInitContainersEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationDriverInitContainersEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationDriverInitContainersEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationDriverInitContainersEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationDriverInitContainersLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationDriverInitContainersLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverInitContainersLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverInitContainersLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverInitContainersLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverInitContainersReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationDriverInitContainersResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationDriverInitContainersSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverInitContainersSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverInitContainersSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverInitContainersSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverInitContainersStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// Lifecycle for running preStop or postStart commands
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationDriverLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationDriverLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PodSecurityContext specifies the PodSecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContext {
    /// A special supplemental group that applies to all containers in a pod.
    /// Some volume types allow the Kubelet to change the ownership of that volume
    /// to be owned by the pod:
    /// 
    /// 
    /// 1. The owning GID will be the FSGroup
    /// 2. The setgid bit is set (new files created in the volume will be owned by FSGroup)
    /// 3. The permission bits are OR'd with rw-rw----
    /// 
    /// 
    /// If unset, the Kubelet will not modify the ownership and permissions of any volume.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroup")]
    pub fs_group: Option<i64>,
    /// fsGroupChangePolicy defines behavior of changing ownership and permission of the volume
    /// before being exposed inside Pod. This field will only apply to
    /// volume types which support fsGroup based ownership(and permissions).
    /// It will have no effect on ephemeral volume types such as: secret, configmaps
    /// and emptydir.
    /// Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroupChangePolicy")]
    pub fs_group_change_policy: Option<String>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to all containers.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in SecurityContext.  If set in
    /// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
    /// takes precedence for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverPodSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverPodSecurityContextSeccompProfile>,
    /// A list of groups applied to the first process run in each container, in addition
    /// to the container's primary GID, the fsGroup (if specified), and group memberships
    /// defined in the container image for the uid of the container process. If unspecified,
    /// no additional groups are added to any container. Note that group memberships
    /// defined in the container image for the uid of the container process are still effective,
    /// even if they are not included in this list.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroups")]
    pub supplemental_groups: Option<Vec<i64>>,
    /// Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported
    /// sysctls (by the container runtime) might fail to launch.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sysctls: Option<Vec<SparkApplicationDriverPodSecurityContextSysctls>>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options within a container's SecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverPodSecurityContextWindowsOptions>,
}

/// The SELinux context to be applied to all containers.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in SecurityContext.  If set in
/// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
/// takes precedence for that container.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Sysctl defines a kernel parameter to be set
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextSysctls {
    /// Name of a property to set
    pub name: String,
    /// Value of a property to set
    pub value: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options within a container's SecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// Port represents the port definition in the pods objects.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPorts {
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    pub name: String,
    pub protocol: String,
}

/// SecretInfo captures information of a secret.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecrets {
    pub name: String,
    pub path: String,
    /// SecretType tells the type of a secret.
    #[serde(rename = "secretType")]
    pub secret_type: String,
}

/// SecurityContext specifies the container's SecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationDriverSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecars {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationDriverSidecarsEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationDriverSidecarsEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationDriverSidecarsLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationDriverSidecarsLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationDriverSidecarsPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationDriverSidecarsReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationDriverSidecarsResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationDriverSidecarsResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationDriverSidecarsSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationDriverSidecarsStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationDriverSidecarsVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationDriverSidecarsVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationDriverSidecarsEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationDriverSidecarsEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationDriverSidecarsEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationDriverSidecarsEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationDriverSidecarsEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationDriverSidecarsEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationDriverSidecarsEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationDriverSidecarsLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationDriverSidecarsLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverSidecarsLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverSidecarsLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverSidecarsLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverSidecarsReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationDriverSidecarsResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationDriverSidecarsSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverSidecarsSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverSidecarsSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverSidecarsSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverSidecarsStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// The pod this Toleration is attached to tolerates any taint that matches
/// the triple <key,value,effect> using the matching operator <operator>.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverTolerations {
    /// Effect indicates the taint effect to match. Empty means match all taint effects.
    /// When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub effect: Option<String>,
    /// Key is the taint key that the toleration applies to. Empty means match all taint keys.
    /// If the key is empty, operator must be Exists; this combination means to match all values and all keys.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Operator represents a key's relationship to the value.
    /// Valid operators are Exists and Equal. Defaults to Equal.
    /// Exists is equivalent to wildcard for value, so that a pod can
    /// tolerate all taints of a particular category.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub operator: Option<String>,
    /// TolerationSeconds represents the period of time the toleration (which must be
    /// of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default,
    /// it is not set, which means tolerate the taint forever (do not evict). Zero and
    /// negative values will be treated as 0 (evict immediately) by the system.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tolerationSeconds")]
    pub toleration_seconds: Option<i64>,
    /// Value is the taint value the toleration matches to.
    /// If the operator is Exists, the value should be empty, otherwise just a regular string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// DriverIngressConfiguration is for driver ingress specific configuration parameters.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverIngressOptions {
    /// IngressAnnotations is a map of key,value pairs of annotations that might be added to the ingress object. i.e. specify nginx as ingress.class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressAnnotations")]
    pub ingress_annotations: Option<BTreeMap<String, String>>,
    /// TlsHosts is useful If we need to declare SSL certificates to the ingress object
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressTLS")]
    pub ingress_tls: Option<Vec<SparkApplicationDriverIngressOptionsIngressTls>>,
    /// IngressURLFormat is the URL for the ingress.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressURLFormat")]
    pub ingress_url_format: Option<String>,
    /// ServiceAnnotations is a map of key,value pairs of annotations that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels is a map of key,value pairs of labels that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ServicePort allows configuring the port at service level that might be different from the targetPort.
    #[serde(rename = "servicePort")]
    pub service_port: i32,
    /// ServicePortName allows configuring the name of the service port.
    /// This may be useful for sidecar proxies like Envoy injected by Istio which require specific ports names to treat traffic as proper HTTP.
    #[serde(rename = "servicePortName")]
    pub service_port_name: String,
    /// ServiceType allows configuring the type of the service. Defaults to ClusterIP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceType")]
    pub service_type: Option<String>,
}

/// IngressTLS describes the transport layer security associated with an ingress.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverIngressOptionsIngressTls {
    /// hosts is a list of hosts included in the TLS certificate. The values in
    /// this list must match the name/s used in the tlsSecret. Defaults to the
    /// wildcard host setting for the loadbalancer controller fulfilling this
    /// Ingress, if left unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hosts: Option<Vec<String>>,
    /// secretName is the name of the secret used to terminate TLS traffic on
    /// port 443. Field is left optional to allow TLS routing based on SNI
    /// hostname alone. If the SNI host in a listener conflicts with the "Host"
    /// header field used by an IngressRule, the SNI host is used for termination
    /// and value of the "Host" header is used for routing.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// DynamicAllocation configures dynamic allocation that becomes available for the Kubernetes
/// scheduler backend since Spark 3.0.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDynamicAllocation {
    /// Enabled controls whether dynamic allocation is enabled or not.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enabled: Option<bool>,
    /// InitialExecutors is the initial number of executors to request. If .spec.executor.instances
    /// is also set, the initial number of executors is set to the bigger of that and this option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialExecutors")]
    pub initial_executors: Option<i32>,
    /// MaxExecutors is the upper bound for the number of executors if dynamic allocation is enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxExecutors")]
    pub max_executors: Option<i32>,
    /// MinExecutors is the lower bound for the number of executors if dynamic allocation is enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minExecutors")]
    pub min_executors: Option<i32>,
    /// ShuffleTrackingTimeout controls the timeout in milliseconds for executors that are holding
    /// shuffle data if shuffle tracking is enabled (true by default if dynamic allocation is enabled).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shuffleTrackingTimeout")]
    pub shuffle_tracking_timeout: Option<i64>,
}

/// Executor is the executor specification.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutor {
    /// Affinity specifies the affinity/anti-affinity settings for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<SparkApplicationExecutorAffinity>,
    /// Annotations are the Kubernetes annotations to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<BTreeMap<String, String>>,
    /// ConfigMaps carries information of other ConfigMaps to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMaps")]
    pub config_maps: Option<Vec<SparkApplicationExecutorConfigMaps>>,
    /// CoreLimit specifies a hard limit on CPU cores for the pod.
    /// Optional
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreLimit")]
    pub core_limit: Option<String>,
    /// CoreRequest is the physical CPU core request for the executors.
    /// Maps to `spark.kubernetes.executor.request.cores` that is available since Spark 2.4.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreRequest")]
    pub core_request: Option<String>,
    /// Cores maps to `spark.driver.cores` or `spark.executor.cores` for the driver and executors, respectively.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cores: Option<i32>,
    /// DeleteOnTermination specify whether executor pods should be deleted in case of failure or normal termination.
    /// Maps to `spark.kubernetes.executor.deleteOnTermination` that is available since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deleteOnTermination")]
    pub delete_on_termination: Option<bool>,
    /// DnsConfig dns settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dnsConfig")]
    pub dns_config: Option<SparkApplicationExecutorDnsConfig>,
    /// Env carries the environment variables to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationExecutorEnv>>,
    /// EnvFrom is a list of sources to populate environment variables in the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationExecutorEnvFrom>>,
    /// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envSecretKeyRefs")]
    pub env_secret_key_refs: Option<BTreeMap<String, SparkApplicationExecutorEnvSecretKeyRefs>>,
    /// EnvVars carries the environment variables to add to the pod.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envVars")]
    pub env_vars: Option<BTreeMap<String, String>>,
    /// GPU specifies GPU requirement for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu: Option<SparkApplicationExecutorGpu>,
    /// HostAliases settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostAliases")]
    pub host_aliases: Option<Vec<SparkApplicationExecutorHostAliases>>,
    /// HostNetwork indicates whether to request host networking for the pod or not.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostNetwork")]
    pub host_network: Option<bool>,
    /// Image is the container image to use. Overrides Spec.Image if set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// InitContainers is a list of init-containers that run to completion before the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initContainers")]
    pub init_containers: Option<Vec<SparkApplicationExecutorInitContainers>>,
    /// Instances is the number of executor instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub instances: Option<i32>,
    /// JavaOptions is a string of extra JVM options to pass to the executors. For instance,
    /// GC settings or other logging.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "javaOptions")]
    pub java_options: Option<String>,
    /// Labels are the Kubernetes labels to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<BTreeMap<String, String>>,
    /// Lifecycle for running preStop or postStart commands
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationExecutorLifecycle>,
    /// Memory is the amount of memory to request for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// MemoryOverhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverhead")]
    pub memory_overhead: Option<String>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at SparkApplication level (which will be deprecated).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// PodSecurityContext specifies the PodSecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podSecurityContext")]
    pub pod_security_context: Option<SparkApplicationExecutorPodSecurityContext>,
    /// Ports settings for the pods, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationExecutorPorts>>,
    /// PriorityClassName is the name of the PriorityClass for the executor pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// SchedulerName specifies the scheduler that will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "schedulerName")]
    pub scheduler_name: Option<String>,
    /// Secrets carries information of secrets to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secrets: Option<Vec<SparkApplicationExecutorSecrets>>,
    /// SecurityContext specifies the container's SecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationExecutorSecurityContext>,
    /// ServiceAccount is the name of the custom Kubernetes service account used by the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// ShareProcessNamespace settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shareProcessNamespace")]
    pub share_process_namespace: Option<bool>,
    /// Sidecars is a list of sidecar containers that run along side the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sidecars: Option<Vec<SparkApplicationExecutorSidecars>>,
    /// Termination grace period seconds for the pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Tolerations specifies the tolerations listed in ".spec.tolerations" to be applied to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tolerations: Option<Vec<SparkApplicationExecutorTolerations>>,
    /// VolumeMounts specifies the volumes listed in ".spec.volumes" to mount into the main container's filesystem.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationExecutorVolumeMounts>>,
}

/// Affinity specifies the affinity/anti-affinity settings for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinity {
    /// Describes node affinity scheduling rules for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<SparkApplicationExecutorAffinityNodeAffinity>,
    /// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<SparkApplicationExecutorAffinityPodAffinity>,
    /// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<SparkApplicationExecutorAffinityPodAntiAffinity>,
}

/// Describes node affinity scheduling rules for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node matches the corresponding matchExpressions; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to an update), the system
    /// may or may not try to eventually evict the pod from its node.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution>,
}

/// An empty preferred scheduling term matches all objects with implicit weight 0
/// (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// A node selector term, associated with the corresponding weight.
    pub preference: SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference,
    /// Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.
    pub weight: i32,
}

/// A node selector term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// If the affinity requirements specified by this field are not met at
/// scheduling time, the pod will not be scheduled onto the node.
/// If the affinity requirements specified by this field cease to be met
/// at some point during pod execution (e.g. due to an update), the system
/// may or may not try to eventually evict the pod from its node.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// Required. A list of node selector terms. The terms are ORed.
    #[serde(rename = "nodeSelectorTerms")]
    pub node_selector_terms: Vec<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms>,
}

/// A null or empty node selector term matches no objects. The requirements of
/// them are ANDed.
/// The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the anti-affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling anti-affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the anti-affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the anti-affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.
    /// Also, MatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector.
    /// Also, MismatchLabelKeys cannot be set when LabelSelector isn't set.
    /// This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// NamePath is a pair of a name and a path to which the named objects should be mounted to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigMaps {
    pub name: String,
    pub path: String,
}

/// DnsConfig dns settings for the pod, following the Kubernetes specifications.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorDnsConfig {
    /// A list of DNS name server IP addresses.
    /// This will be appended to the base nameservers generated from DNSPolicy.
    /// Duplicated nameservers will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nameservers: Option<Vec<String>>,
    /// A list of DNS resolver options.
    /// This will be merged with the base options generated from DNSPolicy.
    /// Duplicated entries will be removed. Resolution options given in Options
    /// will override those that appear in the base DNSPolicy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<Vec<SparkApplicationExecutorDnsConfigOptions>>,
    /// A list of DNS search domains for host-name lookup.
    /// This will be appended to the base search paths generated from DNSPolicy.
    /// Duplicated search paths will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub searches: Option<Vec<String>>,
}

/// PodDNSConfigOption defines DNS resolver options of a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorDnsConfigOptions {
    /// Required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationExecutorEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationExecutorEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationExecutorEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationExecutorEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationExecutorEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationExecutorEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationExecutorEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
/// Deprecated. Consider using `env` instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvSecretKeyRefs {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// GPU specifies GPU requirement for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorGpu {
    /// Name is GPU resource name, such as: nvidia.com/gpu or amd.com/gpu
    pub name: String,
    /// Quantity is the number of GPUs to request for driver or executor.
    pub quantity: i64,
}

/// HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the
/// pod's hosts file.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorHostAliases {
    /// Hostnames for the above IP address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hostnames: Option<Vec<String>>,
    /// IP address of the host file entry.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ip: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainers {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationExecutorInitContainersEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationExecutorInitContainersEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationExecutorInitContainersLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationExecutorInitContainersLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationExecutorInitContainersPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationExecutorInitContainersReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationExecutorInitContainersResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationExecutorInitContainersResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationExecutorInitContainersSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationExecutorInitContainersStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationExecutorInitContainersVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationExecutorInitContainersVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationExecutorInitContainersEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationExecutorInitContainersEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationExecutorInitContainersEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationExecutorInitContainersLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationExecutorInitContainersLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorInitContainersLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorInitContainersLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorInitContainersLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorInitContainersReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationExecutorInitContainersResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationExecutorInitContainersSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorInitContainersSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorInitContainersSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorInitContainersSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorInitContainersStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// Lifecycle for running preStop or postStart commands
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationExecutorLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationExecutorLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PodSecurityContext specifies the PodSecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContext {
    /// A special supplemental group that applies to all containers in a pod.
    /// Some volume types allow the Kubelet to change the ownership of that volume
    /// to be owned by the pod:
    /// 
    /// 
    /// 1. The owning GID will be the FSGroup
    /// 2. The setgid bit is set (new files created in the volume will be owned by FSGroup)
    /// 3. The permission bits are OR'd with rw-rw----
    /// 
    /// 
    /// If unset, the Kubelet will not modify the ownership and permissions of any volume.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroup")]
    pub fs_group: Option<i64>,
    /// fsGroupChangePolicy defines behavior of changing ownership and permission of the volume
    /// before being exposed inside Pod. This field will only apply to
    /// volume types which support fsGroup based ownership(and permissions).
    /// It will have no effect on ephemeral volume types such as: secret, configmaps
    /// and emptydir.
    /// Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroupChangePolicy")]
    pub fs_group_change_policy: Option<String>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to all containers.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in SecurityContext.  If set in
    /// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
    /// takes precedence for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorPodSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorPodSecurityContextSeccompProfile>,
    /// A list of groups applied to the first process run in each container, in addition
    /// to the container's primary GID, the fsGroup (if specified), and group memberships
    /// defined in the container image for the uid of the container process. If unspecified,
    /// no additional groups are added to any container. Note that group memberships
    /// defined in the container image for the uid of the container process are still effective,
    /// even if they are not included in this list.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroups")]
    pub supplemental_groups: Option<Vec<i64>>,
    /// Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported
    /// sysctls (by the container runtime) might fail to launch.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sysctls: Option<Vec<SparkApplicationExecutorPodSecurityContextSysctls>>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options within a container's SecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorPodSecurityContextWindowsOptions>,
}

/// The SELinux context to be applied to all containers.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in SecurityContext.  If set in
/// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
/// takes precedence for that container.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Sysctl defines a kernel parameter to be set
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextSysctls {
    /// Name of a property to set
    pub name: String,
    /// Value of a property to set
    pub value: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options within a container's SecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// Port represents the port definition in the pods objects.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPorts {
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    pub name: String,
    pub protocol: String,
}

/// SecretInfo captures information of a secret.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecrets {
    pub name: String,
    pub path: String,
    /// SecretType tells the type of a secret.
    #[serde(rename = "secretType")]
    pub secret_type: String,
}

/// SecurityContext specifies the container's SecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationExecutorSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecars {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationExecutorSidecarsEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationExecutorSidecarsEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationExecutorSidecarsLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationExecutorSidecarsLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationExecutorSidecarsPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationExecutorSidecarsReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationExecutorSidecarsResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationExecutorSidecarsResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationExecutorSidecarsSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationExecutorSidecarsStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationExecutorSidecarsVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationExecutorSidecarsVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationExecutorSidecarsEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationExecutorSidecarsEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationExecutorSidecarsEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvFromConfigMapRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvFromSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationExecutorSidecarsLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationExecutorSidecarsLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStart {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsLifecyclePostStartExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsLifecyclePostStartHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorSidecarsLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsLifecyclePostStartTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStop {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsLifecyclePreStopExec>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsLifecyclePreStopHttpGet>,
    /// Sleep represents the duration that the container should sleep before being terminated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorSidecarsLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for the backward compatibility. There are no validation of this field and
    /// lifecycle hooks will fail in runtime when tcp handler is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsLifecyclePreStopTcpSocket>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents the duration that the container should sleep before being terminated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for the backward compatibility. There are no validation of this field and
/// lifecycle hooks will fail in runtime when tcp handler is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorSidecarsLivenessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorSidecarsReadinessProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationExecutorSidecarsResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationExecutorSidecarsSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default is DefaultProcMount which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorSidecarsSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorSidecarsSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorSidecarsSecurityContextWindowsOptions>,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbe {
    /// Exec specifies the action to take.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies an action involving a GRPC port.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorSidecarsStartupProbeGrpc>,
    /// HTTPGet specifies the http request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies an action involving a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies the action to take.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies an action involving a GRPC port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies the http request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies an action involving a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// The pod this Toleration is attached to tolerates any taint that matches
/// the triple <key,value,effect> using the matching operator <operator>.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorTolerations {
    /// Effect indicates the taint effect to match. Empty means match all taint effects.
    /// When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub effect: Option<String>,
    /// Key is the taint key that the toleration applies to. Empty means match all taint keys.
    /// If the key is empty, operator must be Exists; this combination means to match all values and all keys.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Operator represents a key's relationship to the value.
    /// Valid operators are Exists and Equal. Defaults to Equal.
    /// Exists is equivalent to wildcard for value, so that a pod can
    /// tolerate all taints of a particular category.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub operator: Option<String>,
    /// TolerationSeconds represents the period of time the toleration (which must be
    /// of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default,
    /// it is not set, which means tolerate the taint forever (do not evict). Zero and
    /// negative values will be treated as 0 (evict immediately) by the system.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tolerationSeconds")]
    pub toleration_seconds: Option<i64>,
    /// Value is the taint value the toleration matches to.
    /// If the operator is Exists, the value should be empty, otherwise just a regular string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationMode {
    #[serde(rename = "cluster")]
    Cluster,
    #[serde(rename = "client")]
    Client,
}

/// Monitoring configures how monitoring is handled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationMonitoring {
    /// ExposeDriverMetrics specifies whether to expose metrics on the driver.
    #[serde(rename = "exposeDriverMetrics")]
    pub expose_driver_metrics: bool,
    /// ExposeExecutorMetrics specifies whether to expose metrics on the executors.
    #[serde(rename = "exposeExecutorMetrics")]
    pub expose_executor_metrics: bool,
    /// MetricsProperties is the content of a custom metrics.properties for configuring the Spark metric system.
    /// If not specified, the content in spark-docker/conf/metrics.properties will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricsProperties")]
    pub metrics_properties: Option<String>,
    /// MetricsPropertiesFile is the container local path of file metrics.properties for configuring
    /// the Spark metric system. If not specified, value /etc/metrics/conf/metrics.properties will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricsPropertiesFile")]
    pub metrics_properties_file: Option<String>,
    /// Prometheus is for configuring the Prometheus JMX exporter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prometheus: Option<SparkApplicationMonitoringPrometheus>,
}

/// Prometheus is for configuring the Prometheus JMX exporter.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationMonitoringPrometheus {
    /// ConfigFile is the path to the custom Prometheus configuration file provided in the Spark image.
    /// ConfigFile takes precedence over Configuration, which is shown below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configFile")]
    pub config_file: Option<String>,
    /// Configuration is the content of the Prometheus configuration needed by the Prometheus JMX exporter.
    /// If not specified, the content in spark-docker/conf/prometheus.yaml will be used.
    /// Configuration has no effect if ConfigFile is set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub configuration: Option<String>,
    /// JmxExporterJar is the path to the Prometheus JMX exporter jar in the container.
    #[serde(rename = "jmxExporterJar")]
    pub jmx_exporter_jar: String,
    /// Port is the port of the HTTP server run by the Prometheus JMX exporter.
    /// If not specified, 8090 will be used as the default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// PortName is the port name of prometheus JMX exporter port.
    /// If not specified, jmx-exporter will be used as the default.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portName")]
    pub port_name: Option<String>,
}

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationPythonVersion {
    #[serde(rename = "2")]
    r#_2,
    #[serde(rename = "3")]
    r#_3,
}

/// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationRestartPolicy {
    /// OnFailureRetries the number of times to retry running an application before giving up.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onFailureRetries")]
    pub on_failure_retries: Option<i32>,
    /// OnFailureRetryInterval is the interval in seconds between retries on failed runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onFailureRetryInterval")]
    pub on_failure_retry_interval: Option<i64>,
    /// OnSubmissionFailureRetries is the number of times to retry submitting an application before giving up.
    /// This is best effort and actual retry attempts can be >= the value specified due to caching.
    /// These are required if RestartPolicy is OnFailure.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onSubmissionFailureRetries")]
    pub on_submission_failure_retries: Option<i32>,
    /// OnSubmissionFailureRetryInterval is the interval in seconds between retries on failed submissions.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onSubmissionFailureRetryInterval")]
    pub on_submission_failure_retry_interval: Option<i64>,
    /// Type specifies the RestartPolicyType.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<SparkApplicationRestartPolicyType>,
}

/// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationRestartPolicyType {
    Never,
    Always,
    OnFailure,
}

/// SparkUIOptions allows configuring the Service and the Ingress to expose the sparkUI
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationSparkUiOptions {
    /// IngressAnnotations is a map of key,value pairs of annotations that might be added to the ingress object. i.e. specify nginx as ingress.class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressAnnotations")]
    pub ingress_annotations: Option<BTreeMap<String, String>>,
    /// TlsHosts is useful If we need to declare SSL certificates to the ingress object
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressTLS")]
    pub ingress_tls: Option<Vec<SparkApplicationSparkUiOptionsIngressTls>>,
    /// ServiceAnnotations is a map of key,value pairs of annotations that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels is a map of key,value pairs of labels that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ServicePort allows configuring the port at service level that might be different from the targetPort.
    /// TargetPort should be the same as the one defined in spark.ui.port
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "servicePort")]
    pub service_port: Option<i32>,
    /// ServicePortName allows configuring the name of the service port.
    /// This may be useful for sidecar proxies like Envoy injected by Istio which require specific ports names to treat traffic as proper HTTP.
    /// Defaults to spark-driver-ui-port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "servicePortName")]
    pub service_port_name: Option<String>,
    /// ServiceType allows configuring the type of the service. Defaults to ClusterIP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceType")]
    pub service_type: Option<String>,
}

/// IngressTLS describes the transport layer security associated with an ingress.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationSparkUiOptionsIngressTls {
    /// hosts is a list of hosts included in the TLS certificate. The values in
    /// this list must match the name/s used in the tlsSecret. Defaults to the
    /// wildcard host setting for the loadbalancer controller fulfilling this
    /// Ingress, if left unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hosts: Option<Vec<String>>,
    /// secretName is the name of the secret used to terminate TLS traffic on
    /// port 443. Field is left optional to allow TLS routing based on SNI
    /// hostname alone. If the SNI host in a listener conflicts with the "Host"
    /// header field used by an IngressRule, the SNI host is used for termination
    /// and value of the "Host" header is used for routing.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationType {
    Java,
    Python,
    Scala,
    R,
}

/// Volume represents a named volume in a pod that may be accessed by any container in the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumes {
    /// awsElasticBlockStore represents an AWS Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "awsElasticBlockStore")]
    pub aws_elastic_block_store: Option<SparkApplicationVolumesAwsElasticBlockStore>,
    /// azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "azureDisk")]
    pub azure_disk: Option<SparkApplicationVolumesAzureDisk>,
    /// azureFile represents an Azure File Service mount on the host and bind mount to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "azureFile")]
    pub azure_file: Option<SparkApplicationVolumesAzureFile>,
    /// cephFS represents a Ceph FS mount on the host that shares a pod's lifetime
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cephfs: Option<SparkApplicationVolumesCephfs>,
    /// cinder represents a cinder volume attached and mounted on kubelets host machine.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cinder: Option<SparkApplicationVolumesCinder>,
    /// configMap represents a configMap that should populate this volume
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<SparkApplicationVolumesConfigMap>,
    /// csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers (Beta feature).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub csi: Option<SparkApplicationVolumesCsi>,
    /// downwardAPI represents downward API about the pod that should populate this volume
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "downwardAPI")]
    pub downward_api: Option<SparkApplicationVolumesDownwardApi>,
    /// emptyDir represents a temporary directory that shares a pod's lifetime.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "emptyDir")]
    pub empty_dir: Option<SparkApplicationVolumesEmptyDir>,
    /// ephemeral represents a volume that is handled by a cluster storage driver.
    /// The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts,
    /// and deleted when the pod is removed.
    /// 
    /// 
    /// Use this if:
    /// a) the volume is only needed while the pod runs,
    /// b) features of normal volumes like restoring from snapshot or capacity
    ///    tracking are needed,
    /// c) the storage driver is specified through a storage class, and
    /// d) the storage driver supports dynamic volume provisioning through
    ///    a PersistentVolumeClaim (see EphemeralVolumeSource for more
    ///    information on the connection between this volume type
    ///    and PersistentVolumeClaim).
    /// 
    /// 
    /// Use PersistentVolumeClaim or one of the vendor-specific
    /// APIs for volumes that persist for longer than the lifecycle
    /// of an individual pod.
    /// 
    /// 
    /// Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to
    /// be used that way - see the documentation of the driver for
    /// more information.
    /// 
    /// 
    /// A pod can use both types of ephemeral volumes and
    /// persistent volumes at the same time.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ephemeral: Option<SparkApplicationVolumesEphemeral>,
    /// fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub fc: Option<SparkApplicationVolumesFc>,
    /// flexVolume represents a generic volume resource that is
    /// provisioned/attached using an exec based plugin.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "flexVolume")]
    pub flex_volume: Option<SparkApplicationVolumesFlexVolume>,
    /// flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub flocker: Option<SparkApplicationVolumesFlocker>,
    /// gcePersistentDisk represents a GCE Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gcePersistentDisk")]
    pub gce_persistent_disk: Option<SparkApplicationVolumesGcePersistentDisk>,
    /// gitRepo represents a git repository at a particular revision.
    /// DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an
    /// EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir
    /// into the Pod's container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gitRepo")]
    pub git_repo: Option<SparkApplicationVolumesGitRepo>,
    /// glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub glusterfs: Option<SparkApplicationVolumesGlusterfs>,
    /// hostPath represents a pre-existing file or directory on the host
    /// machine that is directly exposed to the container. This is generally
    /// used for system agents or other privileged things that are allowed
    /// to see the host machine. Most containers will NOT need this.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    /// ---
    /// TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not
    /// mount host directories as read/write.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPath")]
    pub host_path: Option<SparkApplicationVolumesHostPath>,
    /// iscsi represents an ISCSI Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// More info: https://examples.k8s.io/volumes/iscsi/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub iscsi: Option<SparkApplicationVolumesIscsi>,
    /// name of the volume.
    /// Must be a DNS_LABEL and unique within the pod.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
    /// nfs represents an NFS mount on the host that shares a pod's lifetime
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nfs: Option<SparkApplicationVolumesNfs>,
    /// persistentVolumeClaimVolumeSource represents a reference to a
    /// PersistentVolumeClaim in the same namespace.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "persistentVolumeClaim")]
    pub persistent_volume_claim: Option<SparkApplicationVolumesPersistentVolumeClaim>,
    /// photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "photonPersistentDisk")]
    pub photon_persistent_disk: Option<SparkApplicationVolumesPhotonPersistentDisk>,
    /// portworxVolume represents a portworx volume attached and mounted on kubelets host machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portworxVolume")]
    pub portworx_volume: Option<SparkApplicationVolumesPortworxVolume>,
    /// projected items for all in one resources secrets, configmaps, and downward API
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub projected: Option<SparkApplicationVolumesProjected>,
    /// quobyte represents a Quobyte mount on the host that shares a pod's lifetime
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub quobyte: Option<SparkApplicationVolumesQuobyte>,
    /// rbd represents a Rados Block Device mount on the host that shares a pod's lifetime.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub rbd: Option<SparkApplicationVolumesRbd>,
    /// scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scaleIO")]
    pub scale_io: Option<SparkApplicationVolumesScaleIo>,
    /// secret represents a secret that should populate this volume.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secret: Option<SparkApplicationVolumesSecret>,
    /// storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storageos: Option<SparkApplicationVolumesStorageos>,
    /// vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vsphereVolume")]
    pub vsphere_volume: Option<SparkApplicationVolumesVsphereVolume>,
}

/// awsElasticBlockStore represents an AWS Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesAwsElasticBlockStore {
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// partition is the partition in the volume that you want to mount.
    /// If omitted, the default is to mount by volume name.
    /// Examples: For volume /dev/sda1, you specify the partition as "1".
    /// Similarly, the volume partition for /dev/sda is "0" (or you can leave the property empty).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub partition: Option<i32>,
    /// readOnly value true will force the readOnly setting in VolumeMounts.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeID is unique ID of the persistent disk resource in AWS (Amazon EBS volume).
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesAzureDisk {
    /// cachingMode is the Host Caching mode: None, Read Only, Read Write.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cachingMode")]
    pub caching_mode: Option<String>,
    /// diskName is the Name of the data disk in the blob storage
    #[serde(rename = "diskName")]
    pub disk_name: String,
    /// diskURI is the URI of data disk in the blob storage
    #[serde(rename = "diskURI")]
    pub disk_uri: String,
    /// fsType is Filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// kind expected values are Shared: multiple blob disks per storage account  Dedicated: single blob disk per storage account  Managed: azure managed data disk (only in managed availability set). defaults to shared
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kind: Option<String>,
    /// readOnly Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// azureFile represents an Azure File Service mount on the host and bind mount to the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesAzureFile {
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretName is the  name of secret that contains Azure Storage Account Name and Key
    #[serde(rename = "secretName")]
    pub secret_name: String,
    /// shareName is the azure share Name
    #[serde(rename = "shareName")]
    pub share_name: String,
}

/// cephFS represents a Ceph FS mount on the host that shares a pod's lifetime
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCephfs {
    /// monitors is Required: Monitors is a collection of Ceph monitors
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    pub monitors: Vec<String>,
    /// path is Optional: Used as the mounted root, rather than the full Ceph tree, default is /
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// readOnly is Optional: Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretFile is Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretFile")]
    pub secret_file: Option<String>,
    /// secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty.
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesCephfsSecretRef>,
    /// user is optional: User is the rados user name, default is admin
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty.
/// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCephfsSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// cinder represents a cinder volume attached and mounted on kubelets host machine.
/// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCinder {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is optional: points to a secret object containing parameters used to connect
    /// to OpenStack.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesCinderSecretRef>,
    /// volumeID used to identify the volume in cinder.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// secretRef is optional: points to a secret object containing parameters used to connect
/// to OpenStack.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCinderSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// configMap represents a configMap that should populate this volume
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesConfigMap {
    /// defaultMode is optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// ConfigMap will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the ConfigMap,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesConfigMapItems>>,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional specify whether the ConfigMap or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesConfigMapItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers (Beta feature).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCsi {
    /// driver is the name of the CSI driver that handles this volume.
    /// Consult with your admin for the correct name as registered in the cluster.
    pub driver: String,
    /// fsType to mount. Ex. "ext4", "xfs", "ntfs".
    /// If not provided, the empty value is passed to the associated CSI driver
    /// which will determine the default filesystem to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// nodePublishSecretRef is a reference to the secret object containing
    /// sensitive information to pass to the CSI driver to complete the CSI
    /// NodePublishVolume and NodeUnpublishVolume calls.
    /// This field is optional, and  may be empty if no secret is required. If the
    /// secret object contains more than one secret, all secret references are passed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePublishSecretRef")]
    pub node_publish_secret_ref: Option<SparkApplicationVolumesCsiNodePublishSecretRef>,
    /// readOnly specifies a read-only configuration for the volume.
    /// Defaults to false (read/write).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeAttributes stores driver-specific properties that are passed to the CSI
    /// driver. Consult your driver's documentation for supported values.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeAttributes")]
    pub volume_attributes: Option<BTreeMap<String, String>>,
}

/// nodePublishSecretRef is a reference to the secret object containing
/// sensitive information to pass to the CSI driver to complete the CSI
/// NodePublishVolume and NodeUnpublishVolume calls.
/// This field is optional, and  may be empty if no secret is required. If the
/// secret object contains more than one secret, all secret references are passed.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCsiNodePublishSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// downwardAPI represents downward API about the pod that should populate this volume
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApi {
    /// Optional: mode bits to use on created files by default. Must be a
    /// Optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// Items is a list of downward API volume file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesDownwardApiItems>>,
}

/// DownwardAPIVolumeFile represents information to create the file containing the pod field
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApiItems {
    /// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationVolumesDownwardApiItemsFieldRef>,
    /// Optional: mode bits used to set permissions on this file, must be an octal value
    /// between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'
    pub path: String,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationVolumesDownwardApiItemsResourceFieldRef>,
}

/// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApiItemsFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApiItemsResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// emptyDir represents a temporary directory that shares a pod's lifetime.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEmptyDir {
    /// medium represents what type of storage medium should back this directory.
    /// The default is "" which means to use the node's default medium.
    /// Must be an empty string (default) or Memory.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub medium: Option<String>,
    /// sizeLimit is the total amount of local storage required for this EmptyDir volume.
    /// The size limit is also applicable for memory medium.
    /// The maximum usage on memory medium EmptyDir would be the minimum value between
    /// the SizeLimit specified here and the sum of memory limits of all containers in a pod.
    /// The default is nil which means that the limit is undefined.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sizeLimit")]
    pub size_limit: Option<IntOrString>,
}

/// ephemeral represents a volume that is handled by a cluster storage driver.
/// The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts,
/// and deleted when the pod is removed.
/// 
/// 
/// Use this if:
/// a) the volume is only needed while the pod runs,
/// b) features of normal volumes like restoring from snapshot or capacity
///    tracking are needed,
/// c) the storage driver is specified through a storage class, and
/// d) the storage driver supports dynamic volume provisioning through
///    a PersistentVolumeClaim (see EphemeralVolumeSource for more
///    information on the connection between this volume type
///    and PersistentVolumeClaim).
/// 
/// 
/// Use PersistentVolumeClaim or one of the vendor-specific
/// APIs for volumes that persist for longer than the lifecycle
/// of an individual pod.
/// 
/// 
/// Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to
/// be used that way - see the documentation of the driver for
/// more information.
/// 
/// 
/// A pod can use both types of ephemeral volumes and
/// persistent volumes at the same time.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeral {
    /// Will be used to create a stand-alone PVC to provision the volume.
    /// The pod in which this EphemeralVolumeSource is embedded will be the
    /// owner of the PVC, i.e. the PVC will be deleted together with the
    /// pod.  The name of the PVC will be `<pod name>-<volume name>` where
    /// `<volume name>` is the name from the `PodSpec.Volumes` array
    /// entry. Pod validation will reject the pod if the concatenated name
    /// is not valid for a PVC (for example, too long).
    /// 
    /// 
    /// An existing PVC with that name that is not owned by the pod
    /// will *not* be used for the pod to avoid using an unrelated
    /// volume by mistake. Starting the pod is then blocked until
    /// the unrelated PVC is removed. If such a pre-created PVC is
    /// meant to be used by the pod, the PVC has to updated with an
    /// owner reference to the pod once the pod exists. Normally
    /// this should not be necessary, but it may be useful when
    /// manually reconstructing a broken cluster.
    /// 
    /// 
    /// This field is read-only and no changes will be made by Kubernetes
    /// to the PVC after it has been created.
    /// 
    /// 
    /// Required, must not be nil.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeClaimTemplate")]
    pub volume_claim_template: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplate>,
}

/// Will be used to create a stand-alone PVC to provision the volume.
/// The pod in which this EphemeralVolumeSource is embedded will be the
/// owner of the PVC, i.e. the PVC will be deleted together with the
/// pod.  The name of the PVC will be `<pod name>-<volume name>` where
/// `<volume name>` is the name from the `PodSpec.Volumes` array
/// entry. Pod validation will reject the pod if the concatenated name
/// is not valid for a PVC (for example, too long).
/// 
/// 
/// An existing PVC with that name that is not owned by the pod
/// will *not* be used for the pod to avoid using an unrelated
/// volume by mistake. Starting the pod is then blocked until
/// the unrelated PVC is removed. If such a pre-created PVC is
/// meant to be used by the pod, the PVC has to updated with an
/// owner reference to the pod once the pod exists. Normally
/// this should not be necessary, but it may be useful when
/// manually reconstructing a broken cluster.
/// 
/// 
/// This field is read-only and no changes will be made by Kubernetes
/// to the PVC after it has been created.
/// 
/// 
/// Required, must not be nil.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplate {
    /// May contain labels and annotations that will be copied into the PVC
    /// when creating it. No other fields are allowed and will be rejected during
    /// validation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateMetadata>,
    /// The specification for the PersistentVolumeClaim. The entire content is
    /// copied unchanged into the PVC that gets created from this
    /// template. The same fields as in a PersistentVolumeClaim
    /// are also valid here.
    pub spec: SparkApplicationVolumesEphemeralVolumeClaimTemplateSpec,
}

/// May contain labels and annotations that will be copied into the PVC
/// when creating it. No other fields are allowed and will be rejected during
/// validation.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateMetadata {
}

/// The specification for the PersistentVolumeClaim. The entire content is
/// copied unchanged into the PVC that gets created from this
/// template. The same fields as in a PersistentVolumeClaim
/// are also valid here.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpec {
    /// accessModes contains the desired access modes the volume should have.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "accessModes")]
    pub access_modes: Option<Vec<String>>,
    /// dataSource field can be used to specify either:
    /// * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)
    /// * An existing PVC (PersistentVolumeClaim)
    /// If the provisioner or an external controller can support the specified data source,
    /// it will create a new volume based on the contents of the specified data source.
    /// When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,
    /// and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.
    /// If the namespace is specified, then dataSourceRef will not be copied to dataSource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSource")]
    pub data_source: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSource>,
    /// dataSourceRef specifies the object from which to populate the volume with data, if a non-empty
    /// volume is desired. This may be any object from a non-empty API group (non
    /// core object) or a PersistentVolumeClaim object.
    /// When this field is specified, volume binding will only succeed if the type of
    /// the specified object matches some installed volume populator or dynamic
    /// provisioner.
    /// This field will replace the functionality of the dataSource field and as such
    /// if both fields are non-empty, they must have the same value. For backwards
    /// compatibility, when namespace isn't specified in dataSourceRef,
    /// both fields (dataSource and dataSourceRef) will be set to the same
    /// value automatically if one of them is empty and the other is non-empty.
    /// When namespace is specified in dataSourceRef,
    /// dataSource isn't set to the same value and must be empty.
    /// There are three important differences between dataSource and dataSourceRef:
    /// * While dataSource only allows two specific types of objects, dataSourceRef
    ///   allows any non-core object, as well as PersistentVolumeClaim objects.
    /// * While dataSource ignores disallowed values (dropping them), dataSourceRef
    ///   preserves all values, and generates an error if a disallowed value is
    ///   specified.
    /// * While dataSource only allows local objects, dataSourceRef allows objects
    ///   in any namespaces.
    /// (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.
    /// (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSourceRef")]
    pub data_source_ref: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSourceRef>,
    /// resources represents the minimum resources the volume should have.
    /// If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements
    /// that are lower than previous value but must still be higher than capacity recorded in the
    /// status field of the claim.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecResources>,
    /// selector is a label query over volumes to consider for binding.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelector>,
    /// storageClassName is the name of the StorageClass required by the claim.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storageClassName")]
    pub storage_class_name: Option<String>,
    /// volumeAttributesClassName may be used to set the VolumeAttributesClass used by this claim.
    /// If specified, the CSI driver will create or update the volume with the attributes defined
    /// in the corresponding VolumeAttributesClass. This has a different purpose than storageClassName,
    /// it can be changed after the claim is created. An empty string value means that no VolumeAttributesClass
    /// will be applied to the claim but it's not allowed to reset this field to empty string once it is set.
    /// If unspecified and the PersistentVolumeClaim is unbound, the default VolumeAttributesClass
    /// will be set by the persistentvolume controller if it exists.
    /// If the resource referred to by volumeAttributesClass does not exist, this PersistentVolumeClaim will be
    /// set to a Pending state, as reflected by the modifyVolumeStatus field, until such as a resource
    /// exists.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#volumeattributesclass
    /// (Alpha) Using this field requires the VolumeAttributesClass feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeAttributesClassName")]
    pub volume_attributes_class_name: Option<String>,
    /// volumeMode defines what type of volume is required by the claim.
    /// Value of Filesystem is implied when not included in claim spec.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMode")]
    pub volume_mode: Option<String>,
    /// volumeName is the binding reference to the PersistentVolume backing this claim.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
}

/// dataSource field can be used to specify either:
/// * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)
/// * An existing PVC (PersistentVolumeClaim)
/// If the provisioner or an external controller can support the specified data source,
/// it will create a new volume based on the contents of the specified data source.
/// When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,
/// and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.
/// If the namespace is specified, then dataSourceRef will not be copied to dataSource.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSource {
    /// APIGroup is the group for the resource being referenced.
    /// If APIGroup is not specified, the specified Kind must be in the core API group.
    /// For any other third-party types, APIGroup is required.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiGroup")]
    pub api_group: Option<String>,
    /// Kind is the type of resource being referenced
    pub kind: String,
    /// Name is the name of resource being referenced
    pub name: String,
}

/// dataSourceRef specifies the object from which to populate the volume with data, if a non-empty
/// volume is desired. This may be any object from a non-empty API group (non
/// core object) or a PersistentVolumeClaim object.
/// When this field is specified, volume binding will only succeed if the type of
/// the specified object matches some installed volume populator or dynamic
/// provisioner.
/// This field will replace the functionality of the dataSource field and as such
/// if both fields are non-empty, they must have the same value. For backwards
/// compatibility, when namespace isn't specified in dataSourceRef,
/// both fields (dataSource and dataSourceRef) will be set to the same
/// value automatically if one of them is empty and the other is non-empty.
/// When namespace is specified in dataSourceRef,
/// dataSource isn't set to the same value and must be empty.
/// There are three important differences between dataSource and dataSourceRef:
/// * While dataSource only allows two specific types of objects, dataSourceRef
///   allows any non-core object, as well as PersistentVolumeClaim objects.
/// * While dataSource ignores disallowed values (dropping them), dataSourceRef
///   preserves all values, and generates an error if a disallowed value is
///   specified.
/// * While dataSource only allows local objects, dataSourceRef allows objects
///   in any namespaces.
/// (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.
/// (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSourceRef {
    /// APIGroup is the group for the resource being referenced.
    /// If APIGroup is not specified, the specified Kind must be in the core API group.
    /// For any other third-party types, APIGroup is required.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiGroup")]
    pub api_group: Option<String>,
    /// Kind is the type of resource being referenced
    pub kind: String,
    /// Name is the name of resource being referenced
    pub name: String,
    /// Namespace is the namespace of resource being referenced
    /// Note that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details.
    /// (Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// resources represents the minimum resources the volume should have.
/// If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements
/// that are lower than previous value but must still be higher than capacity recorded in the
/// status field of the claim.
/// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecResources {
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// selector is a label query over volumes to consider for binding.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFc {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// lun is Optional: FC target lun number
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lun: Option<i32>,
    /// readOnly is Optional: Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// targetWWNs is Optional: FC target worldwide names (WWNs)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetWWNs")]
    pub target_ww_ns: Option<Vec<String>>,
    /// wwids Optional: FC volume world wide identifiers (wwids)
    /// Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub wwids: Option<Vec<String>>,
}

/// flexVolume represents a generic volume resource that is
/// provisioned/attached using an exec based plugin.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFlexVolume {
    /// driver is the name of the driver to use for this volume.
    pub driver: String,
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". The default filesystem depends on FlexVolume script.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// options is Optional: this field holds extra command options if any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<BTreeMap<String, String>>,
    /// readOnly is Optional: defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is Optional: secretRef is reference to the secret object containing
    /// sensitive information to pass to the plugin scripts. This may be
    /// empty if no secret object is specified. If the secret object
    /// contains more than one secret, all secrets are passed to the plugin
    /// scripts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesFlexVolumeSecretRef>,
}

/// secretRef is Optional: secretRef is reference to the secret object containing
/// sensitive information to pass to the plugin scripts. This may be
/// empty if no secret object is specified. If the secret object
/// contains more than one secret, all secrets are passed to the plugin
/// scripts.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFlexVolumeSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFlocker {
    /// datasetName is Name of the dataset stored as metadata -> name on the dataset for Flocker
    /// should be considered as deprecated
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datasetName")]
    pub dataset_name: Option<String>,
    /// datasetUUID is the UUID of the dataset. This is unique identifier of a Flocker dataset
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datasetUUID")]
    pub dataset_uuid: Option<String>,
}

/// gcePersistentDisk represents a GCE Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesGcePersistentDisk {
    /// fsType is filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// partition is the partition in the volume that you want to mount.
    /// If omitted, the default is to mount by volume name.
    /// Examples: For volume /dev/sda1, you specify the partition as "1".
    /// Similarly, the volume partition for /dev/sda is "0" (or you can leave the property empty).
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub partition: Option<i32>,
    /// pdName is unique name of the PD resource in GCE. Used to identify the disk in GCE.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(rename = "pdName")]
    pub pd_name: String,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// gitRepo represents a git repository at a particular revision.
/// DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an
/// EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir
/// into the Pod's container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesGitRepo {
    /// directory is the target directory name.
    /// Must not contain or start with '..'.  If '.' is supplied, the volume directory will be the
    /// git repository.  Otherwise, if specified, the volume will contain the git repository in
    /// the subdirectory with the given name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub directory: Option<String>,
    /// repository is the URL
    pub repository: String,
    /// revision is the commit hash for the specified revision.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub revision: Option<String>,
}

/// glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime.
/// More info: https://examples.k8s.io/volumes/glusterfs/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesGlusterfs {
    /// endpoints is the endpoint name that details Glusterfs topology.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    pub endpoints: String,
    /// path is the Glusterfs volume path.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    pub path: String,
    /// readOnly here will force the Glusterfs volume to be mounted with read-only permissions.
    /// Defaults to false.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// hostPath represents a pre-existing file or directory on the host
/// machine that is directly exposed to the container. This is generally
/// used for system agents or other privileged things that are allowed
/// to see the host machine. Most containers will NOT need this.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
/// ---
/// TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not
/// mount host directories as read/write.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesHostPath {
    /// path of the directory on the host.
    /// If the path is a symlink, it will follow the link to the real path.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    pub path: String,
    /// type for HostPath Volume
    /// Defaults to ""
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// iscsi represents an ISCSI Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// More info: https://examples.k8s.io/volumes/iscsi/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesIscsi {
    /// chapAuthDiscovery defines whether support iSCSI Discovery CHAP authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "chapAuthDiscovery")]
    pub chap_auth_discovery: Option<bool>,
    /// chapAuthSession defines whether support iSCSI Session CHAP authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "chapAuthSession")]
    pub chap_auth_session: Option<bool>,
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// initiatorName is the custom iSCSI Initiator Name.
    /// If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface
    /// <target portal>:<volume name> will be created for the connection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initiatorName")]
    pub initiator_name: Option<String>,
    /// iqn is the target iSCSI Qualified Name.
    pub iqn: String,
    /// iscsiInterface is the interface Name that uses an iSCSI transport.
    /// Defaults to 'default' (tcp).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "iscsiInterface")]
    pub iscsi_interface: Option<String>,
    /// lun represents iSCSI Target Lun number.
    pub lun: i32,
    /// portals is the iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port
    /// is other than default (typically TCP ports 860 and 3260).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub portals: Option<Vec<String>>,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is the CHAP Secret for iSCSI target and initiator authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesIscsiSecretRef>,
    /// targetPortal is iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port
    /// is other than default (typically TCP ports 860 and 3260).
    #[serde(rename = "targetPortal")]
    pub target_portal: String,
}

/// secretRef is the CHAP Secret for iSCSI target and initiator authentication
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesIscsiSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// nfs represents an NFS mount on the host that shares a pod's lifetime
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesNfs {
    /// path that is exported by the NFS server.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    pub path: String,
    /// readOnly here will force the NFS export to be mounted with read-only permissions.
    /// Defaults to false.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// server is the hostname or IP address of the NFS server.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    pub server: String,
}

/// persistentVolumeClaimVolumeSource represents a reference to a
/// PersistentVolumeClaim in the same namespace.
/// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesPersistentVolumeClaim {
    /// claimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
    #[serde(rename = "claimName")]
    pub claim_name: String,
    /// readOnly Will force the ReadOnly setting in VolumeMounts.
    /// Default false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesPhotonPersistentDisk {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// pdID is the ID that identifies Photon Controller persistent disk
    #[serde(rename = "pdID")]
    pub pd_id: String,
}

/// portworxVolume represents a portworx volume attached and mounted on kubelets host machine
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesPortworxVolume {
    /// fSType represents the filesystem type to mount
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeID uniquely identifies a Portworx volume
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// projected items for all in one resources secrets, configmaps, and downward API
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjected {
    /// defaultMode are the mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// sources is the list of volume projections
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sources: Option<Vec<SparkApplicationVolumesProjectedSources>>,
}

/// Projection that may be projected along with other supported volume types
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSources {
    /// ClusterTrustBundle allows a pod to access the `.spec.trustBundle` field
    /// of ClusterTrustBundle objects in an auto-updating file.
    /// 
    /// 
    /// Alpha, gated by the ClusterTrustBundleProjection feature gate.
    /// 
    /// 
    /// ClusterTrustBundle objects can either be selected by name, or by the
    /// combination of signer name and a label selector.
    /// 
    /// 
    /// Kubelet performs aggressive normalization of the PEM contents written
    /// into the pod filesystem.  Esoteric PEM features such as inter-block
    /// comments and block headers are stripped.  Certificates are deduplicated.
    /// The ordering of certificates within the file is arbitrary, and Kubelet
    /// may change the order over time.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterTrustBundle")]
    pub cluster_trust_bundle: Option<SparkApplicationVolumesProjectedSourcesClusterTrustBundle>,
    /// configMap information about the configMap data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<SparkApplicationVolumesProjectedSourcesConfigMap>,
    /// downwardAPI information about the downwardAPI data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "downwardAPI")]
    pub downward_api: Option<SparkApplicationVolumesProjectedSourcesDownwardApi>,
    /// secret information about the secret data to project
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secret: Option<SparkApplicationVolumesProjectedSourcesSecret>,
    /// serviceAccountToken is information about the serviceAccountToken data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountToken")]
    pub service_account_token: Option<SparkApplicationVolumesProjectedSourcesServiceAccountToken>,
}

/// ClusterTrustBundle allows a pod to access the `.spec.trustBundle` field
/// of ClusterTrustBundle objects in an auto-updating file.
/// 
/// 
/// Alpha, gated by the ClusterTrustBundleProjection feature gate.
/// 
/// 
/// ClusterTrustBundle objects can either be selected by name, or by the
/// combination of signer name and a label selector.
/// 
/// 
/// Kubelet performs aggressive normalization of the PEM contents written
/// into the pod filesystem.  Esoteric PEM features such as inter-block
/// comments and block headers are stripped.  Certificates are deduplicated.
/// The ordering of certificates within the file is arbitrary, and Kubelet
/// may change the order over time.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesClusterTrustBundle {
    /// Select all ClusterTrustBundles that match this label selector.  Only has
    /// effect if signerName is set.  Mutually-exclusive with name.  If unset,
    /// interpreted as "match nothing".  If set but empty, interpreted as "match
    /// everything".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelector>,
    /// Select a single ClusterTrustBundle by object name.  Mutually-exclusive
    /// with signerName and labelSelector.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// If true, don't block pod startup if the referenced ClusterTrustBundle(s)
    /// aren't available.  If using name, then the named ClusterTrustBundle is
    /// allowed not to exist.  If using signerName, then the combination of
    /// signerName and labelSelector is allowed to match zero
    /// ClusterTrustBundles.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
    /// Relative path from the volume root to write the bundle.
    pub path: String,
    /// Select all ClusterTrustBundles that match this signer name.
    /// Mutually-exclusive with name.  The contents of all selected
    /// ClusterTrustBundles will be unified and deduplicated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "signerName")]
    pub signer_name: Option<String>,
}

/// Select all ClusterTrustBundles that match this label selector.  Only has
/// effect if signerName is set.  Mutually-exclusive with name.  If unset,
/// interpreted as "match nothing".  If set but empty, interpreted as "match
/// everything".
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// configMap information about the configMap data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesConfigMap {
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// ConfigMap will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the ConfigMap,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesProjectedSourcesConfigMapItems>>,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional specify whether the ConfigMap or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesConfigMapItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// downwardAPI information about the downwardAPI data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApi {
    /// Items is a list of DownwardAPIVolume file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesProjectedSourcesDownwardApiItems>>,
}

/// DownwardAPIVolumeFile represents information to create the file containing the pod field
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApiItems {
    /// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationVolumesProjectedSourcesDownwardApiItemsFieldRef>,
    /// Optional: mode bits used to set permissions on this file, must be an octal value
    /// between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'
    pub path: String,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationVolumesProjectedSourcesDownwardApiItemsResourceFieldRef>,
}

/// Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApiItemsFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApiItemsResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// secret information about the secret data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesSecret {
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// Secret will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the Secret,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesProjectedSourcesSecretItems>>,
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional field specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesSecretItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// serviceAccountToken is information about the serviceAccountToken data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesServiceAccountToken {
    /// audience is the intended audience of the token. A recipient of a token
    /// must identify itself with an identifier specified in the audience of the
    /// token, and otherwise should reject the token. The audience defaults to the
    /// identifier of the apiserver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub audience: Option<String>,
    /// expirationSeconds is the requested duration of validity of the service
    /// account token. As the token approaches expiration, the kubelet volume
    /// plugin will proactively rotate the service account token. The kubelet will
    /// start trying to rotate the token if the token is older than 80 percent of
    /// its time to live or if the token is older than 24 hours.Defaults to 1 hour
    /// and must be at least 10 minutes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "expirationSeconds")]
    pub expiration_seconds: Option<i64>,
    /// path is the path relative to the mount point of the file to project the
    /// token into.
    pub path: String,
}

/// quobyte represents a Quobyte mount on the host that shares a pod's lifetime
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesQuobyte {
    /// group to map volume access to
    /// Default is no group
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub group: Option<String>,
    /// readOnly here will force the Quobyte volume to be mounted with read-only permissions.
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// registry represents a single or multiple Quobyte Registry services
    /// specified as a string as host:port pair (multiple entries are separated with commas)
    /// which acts as the central registry for volumes
    pub registry: String,
    /// tenant owning the given Quobyte volume in the Backend
    /// Used with dynamically provisioned Quobyte volumes, value is set by the plugin
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tenant: Option<String>,
    /// user to map volume access to
    /// Defaults to serivceaccount user
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
    /// volume is a string that references an already created Quobyte volume by name.
    pub volume: String,
}

/// rbd represents a Rados Block Device mount on the host that shares a pod's lifetime.
/// More info: https://examples.k8s.io/volumes/rbd/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesRbd {
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd
    /// TODO: how do we prevent errors in the filesystem from compromising the machine
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// image is the rados image name.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    pub image: String,
    /// keyring is the path to key ring for RBDUser.
    /// Default is /etc/ceph/keyring.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub keyring: Option<String>,
    /// monitors is a collection of Ceph monitors.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    pub monitors: Vec<String>,
    /// pool is the rados pool name.
    /// Default is rbd.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pool: Option<String>,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is name of the authentication secret for RBDUser. If provided
    /// overrides keyring.
    /// Default is nil.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesRbdSecretRef>,
    /// user is the rados user name.
    /// Default is admin.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// secretRef is name of the authentication secret for RBDUser. If provided
/// overrides keyring.
/// Default is nil.
/// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesRbdSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesScaleIo {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs".
    /// Default is "xfs".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// gateway is the host address of the ScaleIO API Gateway.
    pub gateway: String,
    /// protectionDomain is the name of the ScaleIO Protection Domain for the configured storage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "protectionDomain")]
    pub protection_domain: Option<String>,
    /// readOnly Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef references to the secret for ScaleIO user and other
    /// sensitive information. If this is not provided, Login operation will fail.
    #[serde(rename = "secretRef")]
    pub secret_ref: SparkApplicationVolumesScaleIoSecretRef,
    /// sslEnabled Flag enable/disable SSL communication with Gateway, default false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sslEnabled")]
    pub ssl_enabled: Option<bool>,
    /// storageMode indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned.
    /// Default is ThinProvisioned.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storageMode")]
    pub storage_mode: Option<String>,
    /// storagePool is the ScaleIO Storage Pool associated with the protection domain.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePool")]
    pub storage_pool: Option<String>,
    /// system is the name of the storage system as configured in ScaleIO.
    pub system: String,
    /// volumeName is the name of a volume already created in the ScaleIO system
    /// that is associated with this volume source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
}

/// secretRef references to the secret for ScaleIO user and other
/// sensitive information. If this is not provided, Login operation will fail.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesScaleIoSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// secret represents a secret that should populate this volume.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesSecret {
    /// defaultMode is Optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values
    /// for mode bits. Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// items If unspecified, each key-value pair in the Data field of the referenced
    /// Secret will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the Secret,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesSecretItems>>,
    /// optional field specify whether the Secret or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
    /// secretName is the name of the secret in the pod's namespace to use.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesSecretItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesStorageos {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef specifies the secret to use for obtaining the StorageOS API
    /// credentials.  If not specified, default values will be attempted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesStorageosSecretRef>,
    /// volumeName is the human-readable name of the StorageOS volume.  Volume
    /// names are only unique within a namespace.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
    /// volumeNamespace specifies the scope of the volume within StorageOS.  If no
    /// namespace is specified then the Pod's namespace will be used.  This allows the
    /// Kubernetes name scoping to be mirrored within StorageOS for tighter integration.
    /// Set VolumeName to any name to override the default behaviour.
    /// Set to "default" if you are not using namespaces within StorageOS.
    /// Namespaces that do not pre-exist within StorageOS will be created.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeNamespace")]
    pub volume_namespace: Option<String>,
}

/// secretRef specifies the secret to use for obtaining the StorageOS API
/// credentials.  If not specified, default values will be attempted.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesStorageosSecretRef {
    /// Name of the referent.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    /// TODO: Add other useful fields. apiVersion, kind, uid?
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesVsphereVolume {
    /// fsType is filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// storagePolicyID is the storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePolicyID")]
    pub storage_policy_id: Option<String>,
    /// storagePolicyName is the storage Policy Based Management (SPBM) profile name.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePolicyName")]
    pub storage_policy_name: Option<String>,
    /// volumePath is the path that identifies vSphere volume vmdk
    #[serde(rename = "volumePath")]
    pub volume_path: String,
}

/// SparkApplicationStatus defines the observed state of SparkApplication
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationStatus {
    /// AppState tells the overall application state.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "applicationState")]
    pub application_state: Option<SparkApplicationStatusApplicationState>,
    /// DriverInfo has information about the driver.
    #[serde(rename = "driverInfo")]
    pub driver_info: SparkApplicationStatusDriverInfo,
    /// ExecutionAttempts is the total number of attempts to run a submitted application to completion.
    /// Incremented upon each attempted run of the application and reset upon invalidation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionAttempts")]
    pub execution_attempts: Option<i32>,
    /// ExecutorState records the state of executors by executor Pod names.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executorState")]
    pub executor_state: Option<BTreeMap<String, String>>,
    /// LastSubmissionAttemptTime is the time for the last application submission attempt.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lastSubmissionAttemptTime")]
    pub last_submission_attempt_time: Option<String>,
    /// SparkApplicationID is set by the spark-distribution(via spark.app.id config) on the driver and executor pods
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkApplicationId")]
    pub spark_application_id: Option<String>,
    /// SubmissionAttempts is the total number of attempts to submit an application to run.
    /// Incremented upon each attempted submission of the application and reset upon invalidation and rerun.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "submissionAttempts")]
    pub submission_attempts: Option<i32>,
    /// SubmissionID is a unique ID of the current submission of the application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "submissionID")]
    pub submission_id: Option<String>,
    /// CompletionTime is the time when the application runs to completion if it does.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationTime")]
    pub termination_time: Option<String>,
}

/// AppState tells the overall application state.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationStatusApplicationState {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorMessage")]
    pub error_message: Option<String>,
    /// ApplicationStateType represents the type of the current state of an application.
    pub state: String,
}

/// DriverInfo has information about the driver.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationStatusDriverInfo {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podName")]
    pub pod_name: Option<String>,
    /// UI Details for the UI created via ClusterIP service accessible from within the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIAddress")]
    pub web_ui_address: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIIngressAddress")]
    pub web_ui_ingress_address: Option<String>,
    /// Ingress Details if an ingress for the UI was created.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIIngressName")]
    pub web_ui_ingress_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIPort")]
    pub web_ui_port: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIServiceName")]
    pub web_ui_service_name: Option<String>,
}

