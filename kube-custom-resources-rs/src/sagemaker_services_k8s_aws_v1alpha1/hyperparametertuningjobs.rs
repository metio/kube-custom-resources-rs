// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --auto --filename ./crd-catalog/aws-controllers-k8s/sagemaker-controller/sagemaker.services.k8s.aws/v1alpha1/hyperparametertuningjobs.yaml
// kopium version: 0.16.1

use kube::CustomResource;
use schemars::JsonSchema;
use serde::{Serialize, Deserialize};
use std::collections::BTreeMap;

/// HyperParameterTuningJobSpec defines the desired state of HyperParameterTuningJob.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)]
#[kube(group = "sagemaker.services.k8s.aws", version = "v1alpha1", kind = "HyperParameterTuningJob", plural = "hyperparametertuningjobs")]
#[kube(namespaced)]
#[kube(status = "HyperParameterTuningJobStatus")]
pub struct HyperParameterTuningJobSpec {
    /// The HyperParameterTuningJobConfig object that describes the tuning job, including the search strategy, the objective metric used to evaluate training jobs, ranges of parameters to search, and resource limits for the tuning job. For more information, see How Hyperparameter Tuning Works (https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html).
    #[serde(rename = "hyperParameterTuningJobConfig")]
    pub hyper_parameter_tuning_job_config: HyperParameterTuningJobHyperParameterTuningJobConfig,
    /// The name of the tuning job. This name is the prefix for the names of all training jobs that this tuning job launches. The name must be unique within the same Amazon Web Services account and Amazon Web Services Region. The name must have 1 to 32 characters. Valid characters are a-z, A-Z, 0-9, and : + = @ _ % - (hyphen). The name is not case sensitive.
    #[serde(rename = "hyperParameterTuningJobName")]
    pub hyper_parameter_tuning_job_name: String,
    /// An array of key-value pairs. You can use tags to categorize your Amazon Web Services resources in different ways, for example, by purpose, owner, or environment. For more information, see Tagging Amazon Web Services Resources (https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html). 
    ///  Tags that you specify for the tuning job are also added to all training jobs that the tuning job launches.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<HyperParameterTuningJobTags>>,
    /// The HyperParameterTrainingJobDefinition object that describes the training jobs that this tuning job launches, including static hyperparameters, input data configuration, output data configuration, resource configuration, and stopping condition.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobDefinition")]
    pub training_job_definition: Option<HyperParameterTuningJobTrainingJobDefinition>,
    /// A list of the HyperParameterTrainingJobDefinition objects launched for this tuning job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobDefinitions")]
    pub training_job_definitions: Option<Vec<HyperParameterTuningJobTrainingJobDefinitions>>,
    /// Specifies the configuration for starting the hyperparameter tuning job using one or more previous tuning jobs as a starting point. The results of previous tuning jobs are used to inform which combinations of hyperparameters to search over in the new tuning job. 
    ///  All training jobs launched by the new hyperparameter tuning job are evaluated by using the objective metric. If you specify IDENTICAL_DATA_AND_ALGORITHM as the WarmStartType value for the warm start configuration, the training job that performs the best in the new tuning job is compared to the best training jobs from the parent tuning jobs. From these, the training job that performs the best as measured by the objective metric is returned as the overall best training job. 
    ///  All training jobs launched by parent hyperparameter tuning jobs and the new hyperparameter tuning jobs count against the limit of training jobs for the tuning job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "warmStartConfig")]
    pub warm_start_config: Option<HyperParameterTuningJobWarmStartConfig>,
}

/// The HyperParameterTuningJobConfig object that describes the tuning job, including the search strategy, the objective metric used to evaluate training jobs, ranges of parameters to search, and resource limits for the tuning job. For more information, see How Hyperparameter Tuning Works (https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html).
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfig {
    /// Defines the objective metric for a hyperparameter tuning job. Hyperparameter tuning uses the value of this metric to evaluate the training jobs it launches, and returns the training job that results in either the highest or lowest value for this metric, depending on the value you specify for the Type parameter.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hyperParameterTuningJobObjective")]
    pub hyper_parameter_tuning_job_objective: Option<HyperParameterTuningJobHyperParameterTuningJobConfigHyperParameterTuningJobObjective>,
    /// Specifies ranges of integer, continuous, and categorical hyperparameters that a hyperparameter tuning job searches. The hyperparameter tuning job launches training jobs with hyperparameter values within these ranges to find the combination of values that result in the training job with the best performance as measured by the objective metric of the hyperparameter tuning job. 
    ///  The maximum number of items specified for Array Members refers to the maximum number of hyperparameters for each range and also the maximum for the hyperparameter tuning job itself. That is, the sum of the number of hyperparameters for all the ranges can't exceed the maximum number specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parameterRanges")]
    pub parameter_ranges: Option<HyperParameterTuningJobHyperParameterTuningJobConfigParameterRanges>,
    /// Specifies the maximum number of training jobs and parallel training jobs that a hyperparameter tuning job can launch.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceLimits")]
    pub resource_limits: Option<HyperParameterTuningJobHyperParameterTuningJobConfigResourceLimits>,
    /// The strategy hyperparameter tuning uses to find the best combination of hyperparameters for your model.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub strategy: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobEarlyStoppingType")]
    pub training_job_early_stopping_type: Option<String>,
    /// The job completion criteria.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tuningJobCompletionCriteria")]
    pub tuning_job_completion_criteria: Option<HyperParameterTuningJobHyperParameterTuningJobConfigTuningJobCompletionCriteria>,
}

/// Defines the objective metric for a hyperparameter tuning job. Hyperparameter tuning uses the value of this metric to evaluate the training jobs it launches, and returns the training job that results in either the highest or lowest value for this metric, depending on the value you specify for the Type parameter.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfigHyperParameterTuningJobObjective {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricName")]
    pub metric_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
}

/// Specifies ranges of integer, continuous, and categorical hyperparameters that a hyperparameter tuning job searches. The hyperparameter tuning job launches training jobs with hyperparameter values within these ranges to find the combination of values that result in the training job with the best performance as measured by the objective metric of the hyperparameter tuning job. 
///  The maximum number of items specified for Array Members refers to the maximum number of hyperparameters for each range and also the maximum for the hyperparameter tuning job itself. That is, the sum of the number of hyperparameters for all the ranges can't exceed the maximum number specified.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfigParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "categoricalParameterRanges")]
    pub categorical_parameter_ranges: Option<Vec<HyperParameterTuningJobHyperParameterTuningJobConfigParameterRangesCategoricalParameterRanges>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continuousParameterRanges")]
    pub continuous_parameter_ranges: Option<Vec<HyperParameterTuningJobHyperParameterTuningJobConfigParameterRangesContinuousParameterRanges>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "integerParameterRanges")]
    pub integer_parameter_ranges: Option<Vec<HyperParameterTuningJobHyperParameterTuningJobConfigParameterRangesIntegerParameterRanges>>,
}

/// A list of categorical hyperparameters to tune.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfigParameterRangesCategoricalParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A list of continuous hyperparameters to tune.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfigParameterRangesContinuousParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxValue")]
    pub max_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minValue")]
    pub min_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scalingType")]
    pub scaling_type: Option<String>,
}

/// For a hyperparameter of the integer type, specifies the range that a hyperparameter tuning job searches.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfigParameterRangesIntegerParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxValue")]
    pub max_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minValue")]
    pub min_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scalingType")]
    pub scaling_type: Option<String>,
}

/// Specifies the maximum number of training jobs and parallel training jobs that a hyperparameter tuning job can launch.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfigResourceLimits {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxNumberOfTrainingJobs")]
    pub max_number_of_training_jobs: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxParallelTrainingJobs")]
    pub max_parallel_training_jobs: Option<i64>,
}

/// The job completion criteria.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobHyperParameterTuningJobConfigTuningJobCompletionCriteria {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetObjectiveMetricValue")]
    pub target_objective_metric_value: Option<f64>,
}

/// A tag object that consists of a key and an optional value, used to manage metadata for SageMaker Amazon Web Services resources. 
///  You can add tags to notebook instances, training jobs, hyperparameter tuning jobs, batch transform jobs, models, labeling jobs, work teams, endpoint configurations, and endpoints. For more information on adding tags to SageMaker resources, see AddTags. 
///  For more information on adding metadata to your Amazon Web Services resources with tagging, see Tagging Amazon Web Services resources (https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html). For advice on best practices for managing Amazon Web Services resources with tagging, see Tagging Best Practices: Implement an Effective Amazon Web Services Resource Tagging Strategy (https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf).
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTags {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The HyperParameterTrainingJobDefinition object that describes the training jobs that this tuning job launches, including static hyperparameters, input data configuration, output data configuration, resource configuration, and stopping condition.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinition {
    /// Specifies which training algorithm to use for training jobs that a hyperparameter tuning job launches and the metrics to monitor.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "algorithmSpecification")]
    pub algorithm_specification: Option<HyperParameterTuningJobTrainingJobDefinitionAlgorithmSpecification>,
    /// Contains information about the output location for managed spot training checkpoint data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "checkpointConfig")]
    pub checkpoint_config: Option<HyperParameterTuningJobTrainingJobDefinitionCheckpointConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "definitionName")]
    pub definition_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableInterContainerTrafficEncryption")]
    pub enable_inter_container_traffic_encryption: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableManagedSpotTraining")]
    pub enable_managed_spot_training: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableNetworkIsolation")]
    pub enable_network_isolation: Option<bool>,
    /// Specifies ranges of integer, continuous, and categorical hyperparameters that a hyperparameter tuning job searches. The hyperparameter tuning job launches training jobs with hyperparameter values within these ranges to find the combination of values that result in the training job with the best performance as measured by the objective metric of the hyperparameter tuning job. 
    ///  The maximum number of items specified for Array Members refers to the maximum number of hyperparameters for each range and also the maximum for the hyperparameter tuning job itself. That is, the sum of the number of hyperparameters for all the ranges can't exceed the maximum number specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hyperParameterRanges")]
    pub hyper_parameter_ranges: Option<HyperParameterTuningJobTrainingJobDefinitionHyperParameterRanges>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputDataConfig")]
    pub input_data_config: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionInputDataConfig>>,
    /// Provides information about how to store model training results (model artifacts).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputDataConfig")]
    pub output_data_config: Option<HyperParameterTuningJobTrainingJobDefinitionOutputDataConfig>,
    /// Describes the resources, including machine learning (ML) compute instances and ML storage volumes, to use for model training.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceConfig")]
    pub resource_config: Option<HyperParameterTuningJobTrainingJobDefinitionResourceConfig>,
    /// The retry strategy to use when a training job fails due to an InternalServerError. RetryStrategy is specified as part of the CreateTrainingJob and CreateHyperParameterTuningJob requests. You can add the StoppingCondition parameter to the request to limit the training time for the complete job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryStrategy")]
    pub retry_strategy: Option<HyperParameterTuningJobTrainingJobDefinitionRetryStrategy>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleARN")]
    pub role_arn: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "staticHyperParameters")]
    pub static_hyper_parameters: Option<BTreeMap<String, String>>,
    /// Specifies a limit to how long a model training job or model compilation job can run. It also specifies how long a managed spot training job has to complete. When the job reaches the time limit, SageMaker ends the training or compilation job. Use this API to cap model training costs. 
    ///  To stop a training job, SageMaker sends the algorithm the SIGTERM signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost. 
    ///  The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with CreateModel. 
    ///  The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stoppingCondition")]
    pub stopping_condition: Option<HyperParameterTuningJobTrainingJobDefinitionStoppingCondition>,
    /// Defines the objective metric for a hyperparameter tuning job. Hyperparameter tuning uses the value of this metric to evaluate the training jobs it launches, and returns the training job that results in either the highest or lowest value for this metric, depending on the value you specify for the Type parameter.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tuningObjective")]
    pub tuning_objective: Option<HyperParameterTuningJobTrainingJobDefinitionTuningObjective>,
    /// Specifies a VPC that your training jobs and hosted models have access to. Control access to and from your training and model containers by configuring the VPC. For more information, see Protect Endpoints by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html) and Protect Training Jobs by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vpcConfig")]
    pub vpc_config: Option<HyperParameterTuningJobTrainingJobDefinitionVpcConfig>,
}

/// Specifies which training algorithm to use for training jobs that a hyperparameter tuning job launches and the metrics to monitor.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionAlgorithmSpecification {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "algorithmName")]
    pub algorithm_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricDefinitions")]
    pub metric_definitions: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionAlgorithmSpecificationMetricDefinitions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingImage")]
    pub training_image: Option<String>,
    /// The training input mode that the algorithm supports. For more information about input modes, see Algorithms (https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). 
    ///  Pipe mode 
    ///  If an algorithm supports Pipe mode, Amazon SageMaker streams data directly from Amazon S3 to the container. 
    ///  File mode 
    ///  If an algorithm supports File mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container. 
    ///  You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any. 
    ///  For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training. 
    ///  FastFile mode 
    ///  If an algorithm supports FastFile mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk. 
    ///  FastFile mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingInputMode")]
    pub training_input_mode: Option<String>,
}

/// Specifies a metric that the training algorithm writes to stderr or stdout. SageMakerhyperparameter tuning captures all defined metrics. You specify one metric that a hyperparameter tuning job uses as its objective metric to choose the best training job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionAlgorithmSpecificationMetricDefinitions {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regex: Option<String>,
}

/// Contains information about the output location for managed spot training checkpoint data.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionCheckpointConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localPath")]
    pub local_path: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3URI")]
    pub s3_uri: Option<String>,
}

/// Specifies ranges of integer, continuous, and categorical hyperparameters that a hyperparameter tuning job searches. The hyperparameter tuning job launches training jobs with hyperparameter values within these ranges to find the combination of values that result in the training job with the best performance as measured by the objective metric of the hyperparameter tuning job. 
///  The maximum number of items specified for Array Members refers to the maximum number of hyperparameters for each range and also the maximum for the hyperparameter tuning job itself. That is, the sum of the number of hyperparameters for all the ranges can't exceed the maximum number specified.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionHyperParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "categoricalParameterRanges")]
    pub categorical_parameter_ranges: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionHyperParameterRangesCategoricalParameterRanges>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continuousParameterRanges")]
    pub continuous_parameter_ranges: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionHyperParameterRangesContinuousParameterRanges>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "integerParameterRanges")]
    pub integer_parameter_ranges: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionHyperParameterRangesIntegerParameterRanges>>,
}

/// A list of categorical hyperparameters to tune.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionHyperParameterRangesCategoricalParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A list of continuous hyperparameters to tune.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionHyperParameterRangesContinuousParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxValue")]
    pub max_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minValue")]
    pub min_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scalingType")]
    pub scaling_type: Option<String>,
}

/// For a hyperparameter of the integer type, specifies the range that a hyperparameter tuning job searches.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionHyperParameterRangesIntegerParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxValue")]
    pub max_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minValue")]
    pub min_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scalingType")]
    pub scaling_type: Option<String>,
}

/// A channel is a named input source that training algorithms can consume.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionInputDataConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "channelName")]
    pub channel_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "compressionType")]
    pub compression_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "contentType")]
    pub content_type: Option<String>,
    /// Describes the location of the channel data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSource")]
    pub data_source: Option<HyperParameterTuningJobTrainingJobDefinitionInputDataConfigDataSource>,
    /// The training input mode that the algorithm supports. For more information about input modes, see Algorithms (https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). 
    ///  Pipe mode 
    ///  If an algorithm supports Pipe mode, Amazon SageMaker streams data directly from Amazon S3 to the container. 
    ///  File mode 
    ///  If an algorithm supports File mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container. 
    ///  You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any. 
    ///  For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training. 
    ///  FastFile mode 
    ///  If an algorithm supports FastFile mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk. 
    ///  FastFile mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputMode")]
    pub input_mode: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recordWrapperType")]
    pub record_wrapper_type: Option<String>,
    /// A configuration for a shuffle option for input data in a channel. If you use S3Prefix for S3DataType, the results of the S3 key prefix matches are shuffled. If you use ManifestFile, the order of the S3 object references in the ManifestFile is shuffled. If you use AugmentedManifestFile, the order of the JSON lines in the AugmentedManifestFile is shuffled. The shuffling order is determined using the Seed value. 
    ///  For Pipe input mode, when ShuffleConfig is specified shuffling is done at the start of every epoch. With large datasets, this ensures that the order of the training data is different for each epoch, and it helps reduce bias and possible overfitting. In a multi-node training job when ShuffleConfig is combined with S3DataDistributionType of ShardedByS3Key, the data is shuffled across nodes so that the content sent to a particular node on the first epoch might be sent to a different node on the second epoch.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shuffleConfig")]
    pub shuffle_config: Option<HyperParameterTuningJobTrainingJobDefinitionInputDataConfigShuffleConfig>,
}

/// Describes the location of the channel data.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionInputDataConfigDataSource {
    /// Specifies a file system data source for a channel.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemDataSource")]
    pub file_system_data_source: Option<HyperParameterTuningJobTrainingJobDefinitionInputDataConfigDataSourceFileSystemDataSource>,
    /// Describes the S3 data source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3DataSource")]
    pub s3_data_source: Option<HyperParameterTuningJobTrainingJobDefinitionInputDataConfigDataSourceS3DataSource>,
}

/// Specifies a file system data source for a channel.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionInputDataConfigDataSourceFileSystemDataSource {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "directoryPath")]
    pub directory_path: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemAccessMode")]
    pub file_system_access_mode: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemID")]
    pub file_system_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemType")]
    pub file_system_type: Option<String>,
}

/// Describes the S3 data source.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionInputDataConfigDataSourceS3DataSource {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "attributeNames")]
    pub attribute_names: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroupNames")]
    pub instance_group_names: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3DataDistributionType")]
    pub s3_data_distribution_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3DataType")]
    pub s3_data_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3URI")]
    pub s3_uri: Option<String>,
}

/// A configuration for a shuffle option for input data in a channel. If you use S3Prefix for S3DataType, the results of the S3 key prefix matches are shuffled. If you use ManifestFile, the order of the S3 object references in the ManifestFile is shuffled. If you use AugmentedManifestFile, the order of the JSON lines in the AugmentedManifestFile is shuffled. The shuffling order is determined using the Seed value. 
///  For Pipe input mode, when ShuffleConfig is specified shuffling is done at the start of every epoch. With large datasets, this ensures that the order of the training data is different for each epoch, and it helps reduce bias and possible overfitting. In a multi-node training job when ShuffleConfig is combined with S3DataDistributionType of ShardedByS3Key, the data is shuffled across nodes so that the content sent to a particular node on the first epoch might be sent to a different node on the second epoch.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionInputDataConfigShuffleConfig {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub seed: Option<i64>,
}

/// Provides information about how to store model training results (model artifacts).
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionOutputDataConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyID")]
    pub kms_key_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputPath")]
    pub s3_output_path: Option<String>,
}

/// Describes the resources, including machine learning (ML) compute instances and ML storage volumes, to use for model training.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionResourceConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceCount")]
    pub instance_count: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroups")]
    pub instance_groups: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionResourceConfigInstanceGroups>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keepAlivePeriodInSeconds")]
    pub keep_alive_period_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeKMSKeyID")]
    pub volume_kms_key_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeSizeInGB")]
    pub volume_size_in_gb: Option<i64>,
}

/// Defines an instance group for heterogeneous cluster training. When requesting a training job using the CreateTrainingJob (https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html) API, you can configure multiple instance groups .
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionResourceConfigInstanceGroups {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceCount")]
    pub instance_count: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroupName")]
    pub instance_group_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
}

/// The retry strategy to use when a training job fails due to an InternalServerError. RetryStrategy is specified as part of the CreateTrainingJob and CreateHyperParameterTuningJob requests. You can add the StoppingCondition parameter to the request to limit the training time for the complete job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionRetryStrategy {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<i64>,
}

/// Specifies a limit to how long a model training job or model compilation job can run. It also specifies how long a managed spot training job has to complete. When the job reaches the time limit, SageMaker ends the training or compilation job. Use this API to cap model training costs. 
///  To stop a training job, SageMaker sends the algorithm the SIGTERM signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost. 
///  The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with CreateModel. 
///  The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionStoppingCondition {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxRuntimeInSeconds")]
    pub max_runtime_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxWaitTimeInSeconds")]
    pub max_wait_time_in_seconds: Option<i64>,
}

/// Defines the objective metric for a hyperparameter tuning job. Hyperparameter tuning uses the value of this metric to evaluate the training jobs it launches, and returns the training job that results in either the highest or lowest value for this metric, depending on the value you specify for the Type parameter.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionTuningObjective {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricName")]
    pub metric_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
}

/// Specifies a VPC that your training jobs and hosted models have access to. Control access to and from your training and model containers by configuring the VPC. For more information, see Protect Endpoints by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html) and Protect Training Jobs by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html).
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionVpcConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroupIDs")]
    pub security_group_i_ds: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// Defines the training jobs launched by a hyperparameter tuning job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitions {
    /// Specifies which training algorithm to use for training jobs that a hyperparameter tuning job launches and the metrics to monitor.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "algorithmSpecification")]
    pub algorithm_specification: Option<HyperParameterTuningJobTrainingJobDefinitionsAlgorithmSpecification>,
    /// Contains information about the output location for managed spot training checkpoint data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "checkpointConfig")]
    pub checkpoint_config: Option<HyperParameterTuningJobTrainingJobDefinitionsCheckpointConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "definitionName")]
    pub definition_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableInterContainerTrafficEncryption")]
    pub enable_inter_container_traffic_encryption: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableManagedSpotTraining")]
    pub enable_managed_spot_training: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableNetworkIsolation")]
    pub enable_network_isolation: Option<bool>,
    /// Specifies ranges of integer, continuous, and categorical hyperparameters that a hyperparameter tuning job searches. The hyperparameter tuning job launches training jobs with hyperparameter values within these ranges to find the combination of values that result in the training job with the best performance as measured by the objective metric of the hyperparameter tuning job. 
    ///  The maximum number of items specified for Array Members refers to the maximum number of hyperparameters for each range and also the maximum for the hyperparameter tuning job itself. That is, the sum of the number of hyperparameters for all the ranges can't exceed the maximum number specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hyperParameterRanges")]
    pub hyper_parameter_ranges: Option<HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRanges>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputDataConfig")]
    pub input_data_config: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionsInputDataConfig>>,
    /// Provides information about how to store model training results (model artifacts).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputDataConfig")]
    pub output_data_config: Option<HyperParameterTuningJobTrainingJobDefinitionsOutputDataConfig>,
    /// Describes the resources, including machine learning (ML) compute instances and ML storage volumes, to use for model training.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceConfig")]
    pub resource_config: Option<HyperParameterTuningJobTrainingJobDefinitionsResourceConfig>,
    /// The retry strategy to use when a training job fails due to an InternalServerError. RetryStrategy is specified as part of the CreateTrainingJob and CreateHyperParameterTuningJob requests. You can add the StoppingCondition parameter to the request to limit the training time for the complete job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryStrategy")]
    pub retry_strategy: Option<HyperParameterTuningJobTrainingJobDefinitionsRetryStrategy>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleARN")]
    pub role_arn: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "staticHyperParameters")]
    pub static_hyper_parameters: Option<BTreeMap<String, String>>,
    /// Specifies a limit to how long a model training job or model compilation job can run. It also specifies how long a managed spot training job has to complete. When the job reaches the time limit, SageMaker ends the training or compilation job. Use this API to cap model training costs. 
    ///  To stop a training job, SageMaker sends the algorithm the SIGTERM signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost. 
    ///  The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with CreateModel. 
    ///  The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stoppingCondition")]
    pub stopping_condition: Option<HyperParameterTuningJobTrainingJobDefinitionsStoppingCondition>,
    /// Defines the objective metric for a hyperparameter tuning job. Hyperparameter tuning uses the value of this metric to evaluate the training jobs it launches, and returns the training job that results in either the highest or lowest value for this metric, depending on the value you specify for the Type parameter.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tuningObjective")]
    pub tuning_objective: Option<HyperParameterTuningJobTrainingJobDefinitionsTuningObjective>,
    /// Specifies a VPC that your training jobs and hosted models have access to. Control access to and from your training and model containers by configuring the VPC. For more information, see Protect Endpoints by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html) and Protect Training Jobs by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vpcConfig")]
    pub vpc_config: Option<HyperParameterTuningJobTrainingJobDefinitionsVpcConfig>,
}

/// Specifies which training algorithm to use for training jobs that a hyperparameter tuning job launches and the metrics to monitor.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsAlgorithmSpecification {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "algorithmName")]
    pub algorithm_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricDefinitions")]
    pub metric_definitions: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionsAlgorithmSpecificationMetricDefinitions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingImage")]
    pub training_image: Option<String>,
    /// The training input mode that the algorithm supports. For more information about input modes, see Algorithms (https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). 
    ///  Pipe mode 
    ///  If an algorithm supports Pipe mode, Amazon SageMaker streams data directly from Amazon S3 to the container. 
    ///  File mode 
    ///  If an algorithm supports File mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container. 
    ///  You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any. 
    ///  For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training. 
    ///  FastFile mode 
    ///  If an algorithm supports FastFile mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk. 
    ///  FastFile mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingInputMode")]
    pub training_input_mode: Option<String>,
}

/// Specifies a metric that the training algorithm writes to stderr or stdout. SageMakerhyperparameter tuning captures all defined metrics. You specify one metric that a hyperparameter tuning job uses as its objective metric to choose the best training job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsAlgorithmSpecificationMetricDefinitions {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regex: Option<String>,
}

/// Contains information about the output location for managed spot training checkpoint data.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsCheckpointConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localPath")]
    pub local_path: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3URI")]
    pub s3_uri: Option<String>,
}

/// Specifies ranges of integer, continuous, and categorical hyperparameters that a hyperparameter tuning job searches. The hyperparameter tuning job launches training jobs with hyperparameter values within these ranges to find the combination of values that result in the training job with the best performance as measured by the objective metric of the hyperparameter tuning job. 
///  The maximum number of items specified for Array Members refers to the maximum number of hyperparameters for each range and also the maximum for the hyperparameter tuning job itself. That is, the sum of the number of hyperparameters for all the ranges can't exceed the maximum number specified.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "categoricalParameterRanges")]
    pub categorical_parameter_ranges: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRangesCategoricalParameterRanges>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continuousParameterRanges")]
    pub continuous_parameter_ranges: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRangesContinuousParameterRanges>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "integerParameterRanges")]
    pub integer_parameter_ranges: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRangesIntegerParameterRanges>>,
}

/// A list of categorical hyperparameters to tune.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRangesCategoricalParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A list of continuous hyperparameters to tune.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRangesContinuousParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxValue")]
    pub max_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minValue")]
    pub min_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scalingType")]
    pub scaling_type: Option<String>,
}

/// For a hyperparameter of the integer type, specifies the range that a hyperparameter tuning job searches.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsHyperParameterRangesIntegerParameterRanges {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxValue")]
    pub max_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minValue")]
    pub min_value: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scalingType")]
    pub scaling_type: Option<String>,
}

/// A channel is a named input source that training algorithms can consume.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsInputDataConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "channelName")]
    pub channel_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "compressionType")]
    pub compression_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "contentType")]
    pub content_type: Option<String>,
    /// Describes the location of the channel data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSource")]
    pub data_source: Option<HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigDataSource>,
    /// The training input mode that the algorithm supports. For more information about input modes, see Algorithms (https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). 
    ///  Pipe mode 
    ///  If an algorithm supports Pipe mode, Amazon SageMaker streams data directly from Amazon S3 to the container. 
    ///  File mode 
    ///  If an algorithm supports File mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container. 
    ///  You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any. 
    ///  For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training. 
    ///  FastFile mode 
    ///  If an algorithm supports FastFile mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk. 
    ///  FastFile mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputMode")]
    pub input_mode: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recordWrapperType")]
    pub record_wrapper_type: Option<String>,
    /// A configuration for a shuffle option for input data in a channel. If you use S3Prefix for S3DataType, the results of the S3 key prefix matches are shuffled. If you use ManifestFile, the order of the S3 object references in the ManifestFile is shuffled. If you use AugmentedManifestFile, the order of the JSON lines in the AugmentedManifestFile is shuffled. The shuffling order is determined using the Seed value. 
    ///  For Pipe input mode, when ShuffleConfig is specified shuffling is done at the start of every epoch. With large datasets, this ensures that the order of the training data is different for each epoch, and it helps reduce bias and possible overfitting. In a multi-node training job when ShuffleConfig is combined with S3DataDistributionType of ShardedByS3Key, the data is shuffled across nodes so that the content sent to a particular node on the first epoch might be sent to a different node on the second epoch.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shuffleConfig")]
    pub shuffle_config: Option<HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigShuffleConfig>,
}

/// Describes the location of the channel data.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigDataSource {
    /// Specifies a file system data source for a channel.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemDataSource")]
    pub file_system_data_source: Option<HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigDataSourceFileSystemDataSource>,
    /// Describes the S3 data source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3DataSource")]
    pub s3_data_source: Option<HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigDataSourceS3DataSource>,
}

/// Specifies a file system data source for a channel.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigDataSourceFileSystemDataSource {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "directoryPath")]
    pub directory_path: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemAccessMode")]
    pub file_system_access_mode: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemID")]
    pub file_system_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileSystemType")]
    pub file_system_type: Option<String>,
}

/// Describes the S3 data source.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigDataSourceS3DataSource {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "attributeNames")]
    pub attribute_names: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroupNames")]
    pub instance_group_names: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3DataDistributionType")]
    pub s3_data_distribution_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3DataType")]
    pub s3_data_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3URI")]
    pub s3_uri: Option<String>,
}

/// A configuration for a shuffle option for input data in a channel. If you use S3Prefix for S3DataType, the results of the S3 key prefix matches are shuffled. If you use ManifestFile, the order of the S3 object references in the ManifestFile is shuffled. If you use AugmentedManifestFile, the order of the JSON lines in the AugmentedManifestFile is shuffled. The shuffling order is determined using the Seed value. 
///  For Pipe input mode, when ShuffleConfig is specified shuffling is done at the start of every epoch. With large datasets, this ensures that the order of the training data is different for each epoch, and it helps reduce bias and possible overfitting. In a multi-node training job when ShuffleConfig is combined with S3DataDistributionType of ShardedByS3Key, the data is shuffled across nodes so that the content sent to a particular node on the first epoch might be sent to a different node on the second epoch.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsInputDataConfigShuffleConfig {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub seed: Option<i64>,
}

/// Provides information about how to store model training results (model artifacts).
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsOutputDataConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyID")]
    pub kms_key_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputPath")]
    pub s3_output_path: Option<String>,
}

/// Describes the resources, including machine learning (ML) compute instances and ML storage volumes, to use for model training.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsResourceConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceCount")]
    pub instance_count: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroups")]
    pub instance_groups: Option<Vec<HyperParameterTuningJobTrainingJobDefinitionsResourceConfigInstanceGroups>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keepAlivePeriodInSeconds")]
    pub keep_alive_period_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeKMSKeyID")]
    pub volume_kms_key_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeSizeInGB")]
    pub volume_size_in_gb: Option<i64>,
}

/// Defines an instance group for heterogeneous cluster training. When requesting a training job using the CreateTrainingJob (https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html) API, you can configure multiple instance groups .
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsResourceConfigInstanceGroups {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceCount")]
    pub instance_count: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroupName")]
    pub instance_group_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
}

/// The retry strategy to use when a training job fails due to an InternalServerError. RetryStrategy is specified as part of the CreateTrainingJob and CreateHyperParameterTuningJob requests. You can add the StoppingCondition parameter to the request to limit the training time for the complete job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsRetryStrategy {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<i64>,
}

/// Specifies a limit to how long a model training job or model compilation job can run. It also specifies how long a managed spot training job has to complete. When the job reaches the time limit, SageMaker ends the training or compilation job. Use this API to cap model training costs. 
///  To stop a training job, SageMaker sends the algorithm the SIGTERM signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost. 
///  The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with CreateModel. 
///  The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsStoppingCondition {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxRuntimeInSeconds")]
    pub max_runtime_in_seconds: Option<i64>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxWaitTimeInSeconds")]
    pub max_wait_time_in_seconds: Option<i64>,
}

/// Defines the objective metric for a hyperparameter tuning job. Hyperparameter tuning uses the value of this metric to evaluate the training jobs it launches, and returns the training job that results in either the highest or lowest value for this metric, depending on the value you specify for the Type parameter.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsTuningObjective {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricName")]
    pub metric_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
}

/// Specifies a VPC that your training jobs and hosted models have access to. Control access to and from your training and model containers by configuring the VPC. For more information, see Protect Endpoints by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html) and Protect Training Jobs by Using an Amazon Virtual Private Cloud (https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html).
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobTrainingJobDefinitionsVpcConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroupIDs")]
    pub security_group_i_ds: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// Specifies the configuration for starting the hyperparameter tuning job using one or more previous tuning jobs as a starting point. The results of previous tuning jobs are used to inform which combinations of hyperparameters to search over in the new tuning job. 
///  All training jobs launched by the new hyperparameter tuning job are evaluated by using the objective metric. If you specify IDENTICAL_DATA_AND_ALGORITHM as the WarmStartType value for the warm start configuration, the training job that performs the best in the new tuning job is compared to the best training jobs from the parent tuning jobs. From these, the training job that performs the best as measured by the objective metric is returned as the overall best training job. 
///  All training jobs launched by parent hyperparameter tuning jobs and the new hyperparameter tuning jobs count against the limit of training jobs for the tuning job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobWarmStartConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parentHyperParameterTuningJobs")]
    pub parent_hyper_parameter_tuning_jobs: Option<Vec<HyperParameterTuningJobWarmStartConfigParentHyperParameterTuningJobs>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "warmStartType")]
    pub warm_start_type: Option<String>,
}

/// A previously completed or stopped hyperparameter tuning job to be used as a starting point for a new hyperparameter tuning job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobWarmStartConfigParentHyperParameterTuningJobs {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hyperParameterTuningJobName")]
    pub hyper_parameter_tuning_job_name: Option<String>,
}

/// HyperParameterTuningJobStatus defines the observed state of HyperParameterTuningJob
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobStatus {
    /// All CRs managed by ACK have a common `Status.ACKResourceMetadata` member that is used to contain resource sync state, account ownership, constructed ARN for the resource
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ackResourceMetadata")]
    pub ack_resource_metadata: Option<HyperParameterTuningJobStatusAckResourceMetadata>,
    /// A TrainingJobSummary object that describes the training job that completed with the best current HyperParameterTuningJobObjective.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bestTrainingJob")]
    pub best_training_job: Option<HyperParameterTuningJobStatusBestTrainingJob>,
    /// All CRS managed by ACK have a common `Status.Conditions` member that contains a collection of `ackv1alpha1.Condition` objects that describe the various terminal states of the CR and its backend AWS service API resource
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<HyperParameterTuningJobStatusConditions>>,
    /// If the tuning job failed, the reason it failed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureReason")]
    pub failure_reason: Option<String>,
    /// The status of the tuning job: InProgress, Completed, Failed, Stopping, or Stopped.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hyperParameterTuningJobStatus")]
    pub hyper_parameter_tuning_job_status: Option<String>,
    /// If the hyperparameter tuning job is an warm start tuning job with a WarmStartType of IDENTICAL_DATA_AND_ALGORITHM, this is the TrainingJobSummary for the training job with the best objective metric value of all training jobs launched by this tuning job and all parent jobs specified for the warm start tuning job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "overallBestTrainingJob")]
    pub overall_best_training_job: Option<HyperParameterTuningJobStatusOverallBestTrainingJob>,
}

/// All CRs managed by ACK have a common `Status.ACKResourceMetadata` member that is used to contain resource sync state, account ownership, constructed ARN for the resource
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobStatusAckResourceMetadata {
    /// ARN is the Amazon Resource Name for the resource. This is a globally-unique identifier and is set only by the ACK service controller once the controller has orchestrated the creation of the resource OR when it has verified that an "adopted" resource (a resource where the ARN annotation was set by the Kubernetes user on the CR) exists and matches the supplied CR's Spec field values. TODO(vijat@): Find a better strategy for resources that do not have ARN in CreateOutputResponse https://github.com/aws/aws-controllers-k8s/issues/270
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
    /// OwnerAccountID is the AWS Account ID of the account that owns the backend AWS service API resource.
    #[serde(rename = "ownerAccountID")]
    pub owner_account_id: String,
    /// Region is the AWS region in which the resource exists or will exist.
    pub region: String,
}

/// A TrainingJobSummary object that describes the training job that completed with the best current HyperParameterTuningJobObjective.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobStatusBestTrainingJob {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "creationTime")]
    pub creation_time: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureReason")]
    pub failure_reason: Option<String>,
    /// Shows the latest objective metric emitted by a training job that was launched by a hyperparameter tuning job. You define the objective metric in the HyperParameterTuningJobObjective parameter of HyperParameterTuningJobConfig.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "finalHyperParameterTuningJobObjectiveMetric")]
    pub final_hyper_parameter_tuning_job_objective_metric: Option<HyperParameterTuningJobStatusBestTrainingJobFinalHyperParameterTuningJobObjectiveMetric>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectiveStatus")]
    pub objective_status: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingEndTime")]
    pub training_end_time: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobARN")]
    pub training_job_arn: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobDefinitionName")]
    pub training_job_definition_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobName")]
    pub training_job_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobStatus")]
    pub training_job_status: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingStartTime")]
    pub training_start_time: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tunedHyperParameters")]
    pub tuned_hyper_parameters: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tuningJobName")]
    pub tuning_job_name: Option<String>,
}

/// Shows the latest objective metric emitted by a training job that was launched by a hyperparameter tuning job. You define the objective metric in the HyperParameterTuningJobObjective parameter of HyperParameterTuningJobConfig.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobStatusBestTrainingJobFinalHyperParameterTuningJobObjectiveMetric {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricName")]
    pub metric_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<f64>,
}

/// Condition is the common struct used by all CRDs managed by ACK service controllers to indicate terminal states  of the CR and its backend AWS service API resource
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobStatusConditions {
    /// Last time the condition transitioned from one status to another.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lastTransitionTime")]
    pub last_transition_time: Option<String>,
    /// A human readable message indicating details about the transition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub message: Option<String>,
    /// The reason for the condition's last transition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reason: Option<String>,
    /// Status of the condition, one of True, False, Unknown.
    pub status: String,
    /// Type is the type of the Condition
    #[serde(rename = "type")]
    pub r#type: String,
}

/// If the hyperparameter tuning job is an warm start tuning job with a WarmStartType of IDENTICAL_DATA_AND_ALGORITHM, this is the TrainingJobSummary for the training job with the best objective metric value of all training jobs launched by this tuning job and all parent jobs specified for the warm start tuning job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobStatusOverallBestTrainingJob {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "creationTime")]
    pub creation_time: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureReason")]
    pub failure_reason: Option<String>,
    /// Shows the latest objective metric emitted by a training job that was launched by a hyperparameter tuning job. You define the objective metric in the HyperParameterTuningJobObjective parameter of HyperParameterTuningJobConfig.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "finalHyperParameterTuningJobObjectiveMetric")]
    pub final_hyper_parameter_tuning_job_objective_metric: Option<HyperParameterTuningJobStatusOverallBestTrainingJobFinalHyperParameterTuningJobObjectiveMetric>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectiveStatus")]
    pub objective_status: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingEndTime")]
    pub training_end_time: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobARN")]
    pub training_job_arn: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobDefinitionName")]
    pub training_job_definition_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobName")]
    pub training_job_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingJobStatus")]
    pub training_job_status: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "trainingStartTime")]
    pub training_start_time: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tunedHyperParameters")]
    pub tuned_hyper_parameters: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tuningJobName")]
    pub tuning_job_name: Option<String>,
}

/// Shows the latest objective metric emitted by a training job that was launched by a hyperparameter tuning job. You define the objective metric in the HyperParameterTuningJobObjective parameter of HyperParameterTuningJobConfig.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct HyperParameterTuningJobStatusOverallBestTrainingJobFinalHyperParameterTuningJobObjectiveMetric {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricName")]
    pub metric_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type_")]
    pub r#type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<f64>,
}

