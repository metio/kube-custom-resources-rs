// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --derive=Default --derive=PartialEq --smart-derive-elision --filename crd-catalog/volcano-sh/volcano/scheduling.volcano.sh/v1beta1/podgroups.yaml
// kopium version: 0.22.5

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
    pub use k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// Specification of the desired behavior of the pod group.
/// More info: <https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status>
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
#[kube(group = "scheduling.volcano.sh", version = "v1beta1", kind = "PodGroup", plural = "podgroups")]
#[kube(namespaced)]
#[kube(status = "PodGroupStatus")]
#[kube(schema = "disabled")]
#[kube(derive="Default")]
#[kube(derive="PartialEq")]
pub struct PodGroupSpec {
    /// MinMember defines the minimal number of members/tasks to run the pod group;
    /// if there's not enough resources to start all tasks, the scheduler
    /// will not start anyone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minMember")]
    pub min_member: Option<i32>,
    /// MinResources defines the minimal resource of members/tasks to run the pod group;
    /// if there's not enough resources to start all tasks, the scheduler
    /// will not start anyone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minResources")]
    pub min_resources: Option<BTreeMap<String, IntOrString>>,
    /// MinTaskMember defines the minimal number of pods to run for each task in the pod group;
    /// if there's not enough resources to start each task, the scheduler
    /// will not start anyone.
    /// SubGroupPolicy covers all capabilities of minTaskMember, while providing richer network topology and Gang scheduling management capabilities.
    /// Recommend using SubGroupPolicy to uniformly manage Gang scheduling for each Task group.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minTaskMember")]
    pub min_task_member: Option<BTreeMap<String, i32>>,
    /// NetworkTopology defines the NetworkTopology config, this field works in conjunction with network topology feature and hyperNode CRD.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "networkTopology")]
    pub network_topology: Option<PodGroupNetworkTopology>,
    /// If specified, indicates the PodGroup's priority. "system-node-critical" and
    /// "system-cluster-critical" are two special keywords which indicate the
    /// highest priorities with the former being the highest priority. Any other
    /// name must be defined by creating a PriorityClass object with that name.
    /// If not specified, the PodGroup priority will be default or zero if there is no
    /// default.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// Queue defines the queue to allocate resource for PodGroup; if queue does not exist,
    /// the PodGroup will not be scheduled. Defaults to `default` Queue with the lowest weight.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queue: Option<String>,
    /// Compared with minTaskMember, it offers more comprehensive topology scheduling and Gang scheduling management capabilities.
    /// Concurrent use with minTaskMember is not recommended, and SubGroupPolicy is the long-term evolution direction.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subGroupPolicy")]
    pub sub_group_policy: Option<Vec<PodGroupSubGroupPolicy>>,
}

/// NetworkTopology defines the NetworkTopology config, this field works in conjunction with network topology feature and hyperNode CRD.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PodGroupNetworkTopology {
    /// HighestTierAllowed specifies the highest tier that a job allowed to cross when scheduling.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "highestTierAllowed")]
    pub highest_tier_allowed: Option<i64>,
    /// HighestTierName specifies the highest tier name that a job allowed to cross when scheduling.
    /// HighestTierName and HighestTierAllowed cannot be set simultaneously.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "highestTierName")]
    pub highest_tier_name: Option<String>,
    /// Mode specifies the mode of the network topology constrain.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<PodGroupNetworkTopologyMode>,
}

/// NetworkTopology defines the NetworkTopology config, this field works in conjunction with network topology feature and hyperNode CRD.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum PodGroupNetworkTopologyMode {
    #[serde(rename = "hard")]
    Hard,
    #[serde(rename = "soft")]
    Soft,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PodGroupSubGroupPolicy {
    /// LabelSelector is used to find matching pods.
    /// Pods that match this label selector are counted to determine the number of pods
    /// in their corresponding topology domain.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<PodGroupSubGroupPolicyLabelSelector>,
    /// MatchLabelKeys: A label-based grouping configuration field for Pods, defining filtering rules for grouping label keys
    /// Core function: Refine grouping of Pods that meet LabelSelector criteria by label attributes, with the following rules and constraints:
    /// 1. Scope: Only applies to Pods matching the predefined LabelSelector
    /// 2. Grouping rule: Specify one or more label keys; Pods containing the target label keys with exactly the same corresponding label values are grouped together
    /// 3. Policy constraint: Pods in the same group follow a unified NetworkTopology policy to achieve group-level network behavior governance
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MinSubGroups: Minimum number of subgroups required to trigger scheduling. Scheduling is initiated only if cluster resources meet the requirements of at least this number of subgroups.
    /// Subgroup-level Gang Scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minSubGroups")]
    pub min_sub_groups: Option<i32>,
    /// Name specifies the name of SubGroupPolicy
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// NetworkTopology defines the NetworkTopology config, this field works in conjunction with network topology feature and hyperNode CRD.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "networkTopology")]
    pub network_topology: Option<PodGroupSubGroupPolicyNetworkTopology>,
    /// SubGroupSize defines the number of pods in each sub-affinity group.
    /// Only when a subGroup of pods, with a size of "subGroupSize", can satisfy the network topology constraint then will the subGroup be scheduled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subGroupSize")]
    pub sub_group_size: Option<i32>,
}

/// LabelSelector is used to find matching pods.
/// Pods that match this label selector are counted to determine the number of pods
/// in their corresponding topology domain.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PodGroupSubGroupPolicyLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<PodGroupSubGroupPolicyLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PodGroupSubGroupPolicyLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// NetworkTopology defines the NetworkTopology config, this field works in conjunction with network topology feature and hyperNode CRD.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PodGroupSubGroupPolicyNetworkTopology {
    /// HighestTierAllowed specifies the highest tier that a job allowed to cross when scheduling.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "highestTierAllowed")]
    pub highest_tier_allowed: Option<i64>,
    /// HighestTierName specifies the highest tier name that a job allowed to cross when scheduling.
    /// HighestTierName and HighestTierAllowed cannot be set simultaneously.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "highestTierName")]
    pub highest_tier_name: Option<String>,
    /// Mode specifies the mode of the network topology constrain.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<PodGroupSubGroupPolicyNetworkTopologyMode>,
}

/// NetworkTopology defines the NetworkTopology config, this field works in conjunction with network topology feature and hyperNode CRD.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum PodGroupSubGroupPolicyNetworkTopologyMode {
    #[serde(rename = "hard")]
    Hard,
    #[serde(rename = "soft")]
    Soft,
}

/// Status represents the current information about a pod group.
/// This data may not be up to date.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct PodGroupStatus {
    /// The conditions of PodGroup.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Condition>>,
    /// The number of pods which reached phase Failed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub failed: Option<i32>,
    /// Current phase of PodGroup.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub phase: Option<PodGroupStatusPhase>,
    /// The number of actively running pods.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub running: Option<i32>,
    /// The number of pods which reached phase Succeeded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub succeeded: Option<i32>,
}

/// Status represents the current information about a pod group.
/// This data may not be up to date.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum PodGroupStatusPhase {
    Pending,
    Running,
    Unknown,
    Inqueue,
    Completed,
}

