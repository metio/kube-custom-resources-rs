// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --derive=Default --derive=PartialEq --smart-derive-elision --filename crd-catalog/stackabletech/spark-k8s-operator/spark.stackable.tech/v1alpha1/sparkapplications.yaml
// kopium version: 0.22.5

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
}
use self::prelude::*;

/// A Spark cluster stacklet. This resource is managed by the Stackable operator for Apache Spark.
/// Find more information on how to use it and the resources that the operator generates in the
/// [operator documentation](<https://docs.stackable.tech/home/nightly/spark-k8s/).>
/// 
/// The SparkApplication CRD looks a little different than the CRDs of the other products on the
/// Stackable Data Platform.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, PartialEq)]
#[kube(group = "spark.stackable.tech", version = "v1alpha1", kind = "SparkApplication", plural = "sparkapplications")]
#[kube(namespaced)]
#[kube(status = "SparkApplicationStatus")]
#[kube(schema = "disabled")]
#[kube(derive="PartialEq")]
pub struct SparkApplicationSpec {
    /// Arguments passed directly to the job artifact.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Job dependencies: a list of python packages that will be installed via pip, a list of packages
    /// or repositories that is passed directly to spark-submit, or a list of excluded packages
    /// (also passed directly to spark-submit).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub deps: Option<SparkApplicationDeps>,
    /// The driver role specifies the configuration that, together with the driver pod template, is used by
    /// Spark to create driver pods.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub driver: Option<SparkApplicationDriver>,
    /// A list of environment variables that will be set in the job pod and the driver and executor
    /// pod templates.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationEnv>>,
    /// The executor role specifies the configuration that, together with the driver pod template, is used by
    /// Spark to create the executor pods.
    /// This is RoleGroup instead of plain CommonConfiguration because it needs to allow for the number of replicas.
    /// to be specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub executor: Option<SparkApplicationExecutor>,
    /// User-supplied image containing spark-job dependencies that will be copied to the specified volume mount.
    /// See the [examples](<https://docs.stackable.tech/home/nightly/spark-k8s/usage-guide/examples).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// The job builds a spark-submit command, complete with arguments and referenced dependencies
    /// such as templates, and passes it on to Spark.
    /// The reason this property uses its own type (SubmitConfigFragment) is because logging is not
    /// supported for spark-submit processes.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub job: Option<SparkApplicationJob>,
    /// The log file directory definition used by the Spark history server.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logFileDirectory")]
    pub log_file_directory: Option<SparkApplicationLogFileDirectory>,
    /// The actual application file that will be called by `spark-submit`.
    #[serde(rename = "mainApplicationFile")]
    pub main_application_file: String,
    /// The main class - i.e. entry point - for JVM artifacts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// Mode: cluster or client. Currently only cluster is supported.
    pub mode: SparkApplicationMode,
    /// Configure an S3 connection that the SparkApplication has access to.
    /// Read more in the [Spark S3 usage guide](<https://docs.stackable.tech/home/nightly/spark-k8s/usage-guide/s3).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub s3connection: Option<SparkApplicationS3connection>,
    /// A map of key/value strings that will be passed directly to spark-submit.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConf")]
    pub spark_conf: Option<BTreeMap<String, String>>,
    /// Specify which image to use, the easiest way is to only configure the `productVersion`.
    /// You can also configure a custom image registry to pull from, as well as completely custom
    /// images.
    /// 
    /// Consult the [Product image selection documentation](<https://docs.stackable.tech/home/nightly/concepts/product_image_selection)>
    /// for details.
    #[serde(rename = "sparkImage")]
    pub spark_image: SparkApplicationSparkImage,
    /// Name of the Vector aggregator [discovery ConfigMap](<https://docs.stackable.tech/home/nightly/concepts/service_discovery).>
    /// It must contain the key `ADDRESS` with the address of the Vector aggregator.
    /// Follow the [logging tutorial](<https://docs.stackable.tech/home/nightly/tutorials/logging-vector-aggregator)>
    /// to learn how to configure log aggregation with Vector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vectorAggregatorConfigMapName")]
    pub vector_aggregator_config_map_name: Option<String>,
    /// A list of volumes that can be made available to the job, driver or executors via their volume mounts.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub volumes: Option<Vec<BTreeMap<String, serde_json::Value>>>,
}

/// Job dependencies: a list of python packages that will be installed via pip, a list of packages
/// or repositories that is passed directly to spark-submit, or a list of excluded packages
/// (also passed directly to spark-submit).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDeps {
    /// A list of excluded packages that is passed directly to `spark-submit`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "excludePackages")]
    pub exclude_packages: Option<Vec<String>>,
    /// A list of packages that is passed directly to `spark-submit`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub packages: Option<Vec<String>>,
    /// A list of repositories that is passed directly to `spark-submit`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub repositories: Option<Vec<String>>,
    /// Under the `requirements` you can specify Python dependencies that will be installed with `pip`.
    /// Example: `tabulate==0.8.9`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requirements: Option<Vec<String>>,
}

/// The driver role specifies the configuration that, together with the driver pod template, is used by
/// Spark to create driver pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriver {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cliOverrides")]
    pub cli_overrides: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<SparkApplicationDriverConfig>,
    /// The `configOverrides` can be used to configure properties in product config files
    /// that are not exposed in the CRD. Read the
    /// [config overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#config-overrides)>
    /// and consult the operator specific usage guide documentation for details on the
    /// available config files and settings for the specific product.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configOverrides")]
    pub config_overrides: Option<BTreeMap<String, BTreeMap<String, String>>>,
    /// `envOverrides` configure environment variables to be set in the Pods.
    /// It is a map from strings to strings - environment variables and the value to set.
    /// Read the
    /// [environment variable overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#env-overrides)>
    /// for more information and consult the operator specific usage guide to find out about
    /// the product specific environment variables that are available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envOverrides")]
    pub env_overrides: Option<BTreeMap<String, String>>,
    /// Allows overriding JVM arguments.
    /// Please read on the [JVM argument overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides)>
    /// for details on the usage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jvmArgumentOverrides")]
    pub jvm_argument_overrides: Option<SparkApplicationDriverJvmArgumentOverrides>,
    /// In the `podOverrides` property you can define a
    /// [PodTemplateSpec](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#podtemplatespec-v1-core)>
    /// to override any property that can be set on a Kubernetes Pod.
    /// Read the
    /// [Pod overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#pod-overrides)>
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podOverrides")]
    pub pod_overrides: Option<BTreeMap<String, serde_json::Value>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfig {
    /// These configuration settings control
    /// [Pod placement](<https://docs.stackable.tech/home/nightly/concepts/operations/pod_placement).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<SparkApplicationDriverConfigAffinity>,
    /// Logging configuration, learn more in the [logging concept documentation](<https://docs.stackable.tech/home/nightly/concepts/logging).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub logging: Option<SparkApplicationDriverConfigLogging>,
    /// Request secret (currently only autoTls certificates) lifetime from the secret operator, e.g. `7d`, or `30d`.
    /// This can be shortened by the `maxCertificateLifetime` setting on the SecretClass issuing the TLS certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requestedSecretLifetime")]
    pub requested_secret_lifetime: Option<String>,
    /// Resource usage is configured here, this includes CPU usage, memory usage and disk storage
    /// usage, if this role needs any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationDriverConfigResources>,
    /// Volume mounts for the spark-submit, driver and executor pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<BTreeMap<String, serde_json::Value>>>,
}

/// These configuration settings control
/// [Pod placement](<https://docs.stackable.tech/home/nightly/concepts/operations/pod_placement).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigAffinity {
    /// Same as the `spec.affinity.nodeAffinity` field on the Pod, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<BTreeMap<String, serde_json::Value>>,
    /// Simple key-value pairs forming a nodeSelector, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// Same as the `spec.affinity.podAffinity` field on the Pod, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<BTreeMap<String, serde_json::Value>>,
    /// Same as the `spec.affinity.podAntiAffinity` field on the Pod, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<BTreeMap<String, serde_json::Value>>,
}

/// Logging configuration, learn more in the [logging concept documentation](<https://docs.stackable.tech/home/nightly/concepts/logging).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigLogging {
    /// Log configuration per container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub containers: Option<BTreeMap<String, SparkApplicationDriverConfigLoggingContainers>>,
    /// Wether or not to deploy a container with the Vector log agent.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVectorAgent")]
    pub enable_vector_agent: Option<bool>,
}

/// Log configuration per container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigLoggingContainers {
    /// Configuration for the console appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub console: Option<SparkApplicationDriverConfigLoggingContainersConsole>,
    /// Log configuration provided in a ConfigMap
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub custom: Option<SparkApplicationDriverConfigLoggingContainersCustom>,
    /// Configuration for the file appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<SparkApplicationDriverConfigLoggingContainersFile>,
    /// Configuration per logger
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub loggers: Option<BTreeMap<String, SparkApplicationDriverConfigLoggingContainersLoggers>>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigLoggingContainersConsole {
    /// The log level threshold.
    /// Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkApplicationDriverConfigLoggingContainersConsoleLevel>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationDriverConfigLoggingContainersConsoleLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Log configuration provided in a ConfigMap
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigLoggingContainersCustom {
    /// ConfigMap containing the log configuration files
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<String>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigLoggingContainersFile {
    /// The log level threshold.
    /// Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkApplicationDriverConfigLoggingContainersFileLevel>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationDriverConfigLoggingContainersFileLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigLoggingContainersLoggers {
    /// The log level threshold.
    /// Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkApplicationDriverConfigLoggingContainersLoggersLevel>,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationDriverConfigLoggingContainersLoggersLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Resource usage is configured here, this includes CPU usage, memory usage and disk storage
/// usage, if this role needs any.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigResources {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<SparkApplicationDriverConfigResourcesCpu>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<SparkApplicationDriverConfigResourcesMemory>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storage: Option<SparkApplicationDriverConfigResourcesStorage>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigResourcesCpu {
    /// The maximum amount of CPU cores that can be requested by Pods.
    /// Equivalent to the `limit` for Pod resource configuration.
    /// Cores are specified either as a decimal point number or as milli units.
    /// For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub max: Option<String>,
    /// The minimal amount of CPU cores that Pods need to run.
    /// Equivalent to the `request` for Pod resource configuration.
    /// Cores are specified either as a decimal point number or as milli units.
    /// For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub min: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigResourcesMemory {
    /// The maximum amount of memory that should be available to the Pod.
    /// Specified as a byte [Quantity](<https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/),>
    /// which means these suffixes are supported: E, P, T, G, M, k.
    /// You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.
    /// For example, the following represent roughly the same value:
    /// `128974848, 129e6, 129M,  128974848000m, 123Mi`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limit: Option<String>,
    /// Additional options that can be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runtimeLimits")]
    pub runtime_limits: Option<SparkApplicationDriverConfigResourcesMemoryRuntimeLimits>,
}

/// Additional options that can be specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigResourcesMemoryRuntimeLimits {
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigResourcesStorage {
}

/// Allows overriding JVM arguments.
/// Please read on the [JVM argument overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides)>
/// for details on the usage.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverJvmArgumentOverrides {
    /// JVM arguments to be added
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// JVM arguments to be removed by exact match
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub remove: Option<Vec<String>>,
    /// JVM arguments matching any of this regexes will be removed
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "removeRegex")]
    pub remove_regex: Option<Vec<String>>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationEnv {
    /// Name of the environment variable. May consist of any printable ASCII characters except '='.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationEnvValueFromFieldRef>,
    /// FileKeyRef selects a key of the env file. Requires the EnvFiles feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileKeyRef")]
    pub file_key_ref: Option<SparkApplicationEnvValueFromFileKeyRef>,
    /// Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: <https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names>
    pub name: String,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// FileKeyRef selects a key of the env file. Requires the EnvFiles feature gate to be enabled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationEnvValueFromFileKeyRef {
    /// The key within the env file. An invalid key will prevent the pod from starting. The keys defined within a source may consist of any printable ASCII characters except '='. During Alpha stage of the EnvFiles feature gate, the key size is limited to 128 characters.
    pub key: String,
    /// Specify whether the file or its key must be defined. If the file or key does not exist, then the env var is not published. If optional is set to true and the specified key does not exist, the environment variable will not be set in the Pod's containers.
    /// 
    /// If optional is set to false and the specified key does not exist, an error will be returned during Pod creation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
    /// The path within the volume from which to select the file. Must be relative and may not contain the '..' path or start with '..'.
    pub path: String,
    /// The name of the volume mount containing the env file.
    #[serde(rename = "volumeName")]
    pub volume_name: String,
}

/// Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<String>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: <https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names>
    pub name: String,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The executor role specifies the configuration that, together with the driver pod template, is used by
/// Spark to create the executor pods.
/// This is RoleGroup instead of plain CommonConfiguration because it needs to allow for the number of replicas.
/// to be specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutor {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cliOverrides")]
    pub cli_overrides: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<SparkApplicationExecutorConfig>,
    /// The `configOverrides` can be used to configure properties in product config files
    /// that are not exposed in the CRD. Read the
    /// [config overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#config-overrides)>
    /// and consult the operator specific usage guide documentation for details on the
    /// available config files and settings for the specific product.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configOverrides")]
    pub config_overrides: Option<BTreeMap<String, BTreeMap<String, String>>>,
    /// `envOverrides` configure environment variables to be set in the Pods.
    /// It is a map from strings to strings - environment variables and the value to set.
    /// Read the
    /// [environment variable overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#env-overrides)>
    /// for more information and consult the operator specific usage guide to find out about
    /// the product specific environment variables that are available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envOverrides")]
    pub env_overrides: Option<BTreeMap<String, String>>,
    /// Allows overriding JVM arguments.
    /// Please read on the [JVM argument overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides)>
    /// for details on the usage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jvmArgumentOverrides")]
    pub jvm_argument_overrides: Option<SparkApplicationExecutorJvmArgumentOverrides>,
    /// In the `podOverrides` property you can define a
    /// [PodTemplateSpec](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#podtemplatespec-v1-core)>
    /// to override any property that can be set on a Kubernetes Pod.
    /// Read the
    /// [Pod overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#pod-overrides)>
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podOverrides")]
    pub pod_overrides: Option<BTreeMap<String, serde_json::Value>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub replicas: Option<u16>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfig {
    /// These configuration settings control
    /// [Pod placement](<https://docs.stackable.tech/home/nightly/concepts/operations/pod_placement).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<SparkApplicationExecutorConfigAffinity>,
    /// Logging configuration, learn more in the [logging concept documentation](<https://docs.stackable.tech/home/nightly/concepts/logging).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub logging: Option<SparkApplicationExecutorConfigLogging>,
    /// Request secret (currently only autoTls certificates) lifetime from the secret operator, e.g. `7d`, or `30d`.
    /// This can be shortened by the `maxCertificateLifetime` setting on the SecretClass issuing the TLS certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requestedSecretLifetime")]
    pub requested_secret_lifetime: Option<String>,
    /// Resource usage is configured here, this includes CPU usage, memory usage and disk storage
    /// usage, if this role needs any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationExecutorConfigResources>,
    /// Volume mounts for the spark-submit, driver and executor pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<BTreeMap<String, serde_json::Value>>>,
}

/// These configuration settings control
/// [Pod placement](<https://docs.stackable.tech/home/nightly/concepts/operations/pod_placement).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigAffinity {
    /// Same as the `spec.affinity.nodeAffinity` field on the Pod, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<BTreeMap<String, serde_json::Value>>,
    /// Simple key-value pairs forming a nodeSelector, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// Same as the `spec.affinity.podAffinity` field on the Pod, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<BTreeMap<String, serde_json::Value>>,
    /// Same as the `spec.affinity.podAntiAffinity` field on the Pod, see the [Kubernetes docs](<https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<BTreeMap<String, serde_json::Value>>,
}

/// Logging configuration, learn more in the [logging concept documentation](<https://docs.stackable.tech/home/nightly/concepts/logging).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigLogging {
    /// Log configuration per container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub containers: Option<BTreeMap<String, SparkApplicationExecutorConfigLoggingContainers>>,
    /// Wether or not to deploy a container with the Vector log agent.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVectorAgent")]
    pub enable_vector_agent: Option<bool>,
}

/// Log configuration per container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigLoggingContainers {
    /// Configuration for the console appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub console: Option<SparkApplicationExecutorConfigLoggingContainersConsole>,
    /// Log configuration provided in a ConfigMap
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub custom: Option<SparkApplicationExecutorConfigLoggingContainersCustom>,
    /// Configuration for the file appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<SparkApplicationExecutorConfigLoggingContainersFile>,
    /// Configuration per logger
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub loggers: Option<BTreeMap<String, SparkApplicationExecutorConfigLoggingContainersLoggers>>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigLoggingContainersConsole {
    /// The log level threshold.
    /// Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkApplicationExecutorConfigLoggingContainersConsoleLevel>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationExecutorConfigLoggingContainersConsoleLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Log configuration provided in a ConfigMap
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigLoggingContainersCustom {
    /// ConfigMap containing the log configuration files
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<String>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigLoggingContainersFile {
    /// The log level threshold.
    /// Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkApplicationExecutorConfigLoggingContainersFileLevel>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationExecutorConfigLoggingContainersFileLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigLoggingContainersLoggers {
    /// The log level threshold.
    /// Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkApplicationExecutorConfigLoggingContainersLoggersLevel>,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationExecutorConfigLoggingContainersLoggersLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Resource usage is configured here, this includes CPU usage, memory usage and disk storage
/// usage, if this role needs any.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigResources {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<SparkApplicationExecutorConfigResourcesCpu>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<SparkApplicationExecutorConfigResourcesMemory>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storage: Option<SparkApplicationExecutorConfigResourcesStorage>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigResourcesCpu {
    /// The maximum amount of CPU cores that can be requested by Pods.
    /// Equivalent to the `limit` for Pod resource configuration.
    /// Cores are specified either as a decimal point number or as milli units.
    /// For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub max: Option<String>,
    /// The minimal amount of CPU cores that Pods need to run.
    /// Equivalent to the `request` for Pod resource configuration.
    /// Cores are specified either as a decimal point number or as milli units.
    /// For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub min: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigResourcesMemory {
    /// The maximum amount of memory that should be available to the Pod.
    /// Specified as a byte [Quantity](<https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/),>
    /// which means these suffixes are supported: E, P, T, G, M, k.
    /// You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.
    /// For example, the following represent roughly the same value:
    /// `128974848, 129e6, 129M,  128974848000m, 123Mi`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limit: Option<String>,
    /// Additional options that can be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runtimeLimits")]
    pub runtime_limits: Option<SparkApplicationExecutorConfigResourcesMemoryRuntimeLimits>,
}

/// Additional options that can be specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigResourcesMemoryRuntimeLimits {
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigResourcesStorage {
}

/// Allows overriding JVM arguments.
/// Please read on the [JVM argument overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides)>
/// for details on the usage.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorJvmArgumentOverrides {
    /// JVM arguments to be added
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// JVM arguments to be removed by exact match
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub remove: Option<Vec<String>>,
    /// JVM arguments matching any of this regexes will be removed
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "removeRegex")]
    pub remove_regex: Option<Vec<String>>,
}

/// The job builds a spark-submit command, complete with arguments and referenced dependencies
/// such as templates, and passes it on to Spark.
/// The reason this property uses its own type (SubmitConfigFragment) is because logging is not
/// supported for spark-submit processes.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJob {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cliOverrides")]
    pub cli_overrides: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<SparkApplicationJobConfig>,
    /// The `configOverrides` can be used to configure properties in product config files
    /// that are not exposed in the CRD. Read the
    /// [config overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#config-overrides)>
    /// and consult the operator specific usage guide documentation for details on the
    /// available config files and settings for the specific product.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configOverrides")]
    pub config_overrides: Option<BTreeMap<String, BTreeMap<String, String>>>,
    /// `envOverrides` configure environment variables to be set in the Pods.
    /// It is a map from strings to strings - environment variables and the value to set.
    /// Read the
    /// [environment variable overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#env-overrides)>
    /// for more information and consult the operator specific usage guide to find out about
    /// the product specific environment variables that are available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envOverrides")]
    pub env_overrides: Option<BTreeMap<String, String>>,
    /// Allows overriding JVM arguments.
    /// Please read on the [JVM argument overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides)>
    /// for details on the usage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jvmArgumentOverrides")]
    pub jvm_argument_overrides: Option<SparkApplicationJobJvmArgumentOverrides>,
    /// In the `podOverrides` property you can define a
    /// [PodTemplateSpec](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#podtemplatespec-v1-core)>
    /// to override any property that can be set on a Kubernetes Pod.
    /// Read the
    /// [Pod overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#pod-overrides)>
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podOverrides")]
    pub pod_overrides: Option<BTreeMap<String, serde_json::Value>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJobConfig {
    /// Request secret (currently only autoTls certificates) lifetime from the secret operator, e.g. `7d`, or `30d`.
    /// This can be shortened by the `maxCertificateLifetime` setting on the SecretClass issuing the TLS certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requestedSecretLifetime")]
    pub requested_secret_lifetime: Option<String>,
    /// Resource usage is configured here, this includes CPU usage, memory usage and disk storage
    /// usage, if this role needs any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationJobConfigResources>,
    /// Volume mounts for the spark-submit, driver and executor pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<BTreeMap<String, serde_json::Value>>>,
}

/// Resource usage is configured here, this includes CPU usage, memory usage and disk storage
/// usage, if this role needs any.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJobConfigResources {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<SparkApplicationJobConfigResourcesCpu>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<SparkApplicationJobConfigResourcesMemory>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storage: Option<SparkApplicationJobConfigResourcesStorage>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJobConfigResourcesCpu {
    /// The maximum amount of CPU cores that can be requested by Pods.
    /// Equivalent to the `limit` for Pod resource configuration.
    /// Cores are specified either as a decimal point number or as milli units.
    /// For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub max: Option<String>,
    /// The minimal amount of CPU cores that Pods need to run.
    /// Equivalent to the `request` for Pod resource configuration.
    /// Cores are specified either as a decimal point number or as milli units.
    /// For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub min: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJobConfigResourcesMemory {
    /// The maximum amount of memory that should be available to the Pod.
    /// Specified as a byte [Quantity](<https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/),>
    /// which means these suffixes are supported: E, P, T, G, M, k.
    /// You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.
    /// For example, the following represent roughly the same value:
    /// `128974848, 129e6, 129M,  128974848000m, 123Mi`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limit: Option<String>,
    /// Additional options that can be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runtimeLimits")]
    pub runtime_limits: Option<SparkApplicationJobConfigResourcesMemoryRuntimeLimits>,
}

/// Additional options that can be specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJobConfigResourcesMemoryRuntimeLimits {
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJobConfigResourcesStorage {
}

/// Allows overriding JVM arguments.
/// Please read on the [JVM argument overrides documentation](<https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides)>
/// for details on the usage.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationJobJvmArgumentOverrides {
    /// JVM arguments to be added
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// JVM arguments to be removed by exact match
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub remove: Option<Vec<String>>,
    /// JVM arguments matching any of this regexes will be removed
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "removeRegex")]
    pub remove_regex: Option<Vec<String>>,
}

/// The log file directory definition used by the Spark history server.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectory {
    /// A custom log directory
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customLogDirectory")]
    pub custom_log_directory: Option<String>,
    /// An S3 bucket storing the log events
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub s3: Option<SparkApplicationLogFileDirectoryS3>,
}

/// An S3 bucket storing the log events
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3 {
    pub bucket: SparkApplicationLogFileDirectoryS3Bucket,
    pub prefix: String,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3Bucket {
    /// S3 bucket specification containing the bucket name and an inlined or referenced connection specification.
    /// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub inline: Option<SparkApplicationLogFileDirectoryS3BucketInline>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reference: Option<String>,
}

/// S3 bucket specification containing the bucket name and an inlined or referenced connection specification.
/// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInline {
    /// The name of the S3 bucket.
    #[serde(rename = "bucketName")]
    pub bucket_name: String,
    /// The definition of an S3 connection, either inline or as a reference.
    pub connection: SparkApplicationLogFileDirectoryS3BucketInlineConnection,
}

/// The definition of an S3 connection, either inline or as a reference.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnection {
    /// S3 connection definition as a resource.
    /// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub inline: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInline>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reference: Option<String>,
}

/// S3 connection definition as a resource.
/// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInline {
    /// Which access style to use.
    /// Defaults to virtual hosted-style as most of the data products out there.
    /// Have a look at the [AWS documentation](<https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html).>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "accessStyle")]
    pub access_style: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineAccessStyle>,
    /// If the S3 uses authentication you have to specify you S3 credentials.
    /// In the most cases a [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)>
    /// providing `accessKey` and `secretKey` is sufficient.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineCredentials>,
    /// Host of the S3 server without any protocol or port. For example: `west1.my-cloud.com`.
    pub host: String,
    /// Port the S3 server listens on.
    /// If not specified the product will determine the port to use.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<u16>,
    /// Bucket region used for signing headers (sigv4).
    /// 
    /// This defaults to `us-east-1` which is compatible with other implementations such as Minio.
    /// 
    /// WARNING: Some products use the Hadoop S3 implementation which falls back to us-east-2.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub region: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineRegion>,
    /// Use a TLS connection. If not specified no TLS will be used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTls>,
}

/// S3 connection definition as a resource.
/// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineAccessStyle {
    Path,
    VirtualHosted,
}

/// If the S3 uses authentication you have to specify you S3 credentials.
/// In the most cases a [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)>
/// providing `accessKey` and `secretKey` is sufficient.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineCredentials {
    /// [Scope](<https://docs.stackable.tech/home/nightly/secret-operator/scope)> of the
    /// [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scope: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineCredentialsScope>,
    /// [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)> containing the LDAP bind credentials.
    #[serde(rename = "secretClass")]
    pub secret_class: String,
}

/// [Scope](<https://docs.stackable.tech/home/nightly/secret-operator/scope)> of the
/// [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineCredentialsScope {
    /// The listener volume scope allows Node and Service scopes to be inferred from the applicable listeners.
    /// This must correspond to Volume names in the Pod that mount Listeners.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "listenerVolumes")]
    pub listener_volumes: Option<Vec<String>>,
    /// The node scope is resolved to the name of the Kubernetes Node object that the Pod is running on.
    /// This will typically be the DNS name of the node.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub node: Option<bool>,
    /// The pod scope is resolved to the name of the Kubernetes Pod.
    /// This allows the secret to differentiate between StatefulSet replicas.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pod: Option<bool>,
    /// The service scope allows Pod objects to specify custom scopes.
    /// This should typically correspond to Service objects that the Pod participates in.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub services: Option<Vec<String>>,
}

/// Bucket region used for signing headers (sigv4).
/// 
/// This defaults to `us-east-1` which is compatible with other implementations such as Minio.
/// 
/// WARNING: Some products use the Hadoop S3 implementation which falls back to us-east-2.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineRegion {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// Use a TLS connection. If not specified no TLS will be used.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTls {
    /// The verification method used to verify the certificates of the server and/or the client.
    pub verification: SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerification,
}

/// The verification method used to verify the certificates of the server and/or the client.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerification {
    /// Use TLS but don't verify certificates.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub none: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationNone>,
    /// Use TLS and a CA certificate to verify the server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationServer>,
}

/// Use TLS but don't verify certificates.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationNone {
}

/// Use TLS and a CA certificate to verify the server.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationServer {
    /// CA cert to verify the server.
    #[serde(rename = "caCert")]
    pub ca_cert: SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationServerCaCert,
}

/// CA cert to verify the server.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationServerCaCert {
    /// Name of the [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)> which will provide the CA certificate.
    /// Note that a SecretClass does not need to have a key but can also work with just a CA certificate,
    /// so if you got provided with a CA cert but don't have access to the key you can still use this method.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretClass")]
    pub secret_class: Option<String>,
    /// Use TLS and the CA certificates trusted by the common web browsers to verify the server.
    /// This can be useful when you e.g. use public AWS S3 or other public available services.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webPki")]
    pub web_pki: Option<SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationServerCaCertWebPki>,
}

/// Use TLS and the CA certificates trusted by the common web browsers to verify the server.
/// This can be useful when you e.g. use public AWS S3 or other public available services.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationLogFileDirectoryS3BucketInlineConnectionInlineTlsVerificationServerCaCertWebPki {
}

/// A Spark cluster stacklet. This resource is managed by the Stackable operator for Apache Spark.
/// Find more information on how to use it and the resources that the operator generates in the
/// [operator documentation](<https://docs.stackable.tech/home/nightly/spark-k8s/).>
/// 
/// The SparkApplication CRD looks a little different than the CRDs of the other products on the
/// Stackable Data Platform.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationMode {
    #[serde(rename = "cluster")]
    Cluster,
    #[serde(rename = "client")]
    Client,
}

/// Configure an S3 connection that the SparkApplication has access to.
/// Read more in the [Spark S3 usage guide](<https://docs.stackable.tech/home/nightly/spark-k8s/usage-guide/s3).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connection {
    /// S3 connection definition as a resource.
    /// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub inline: Option<SparkApplicationS3connectionInline>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reference: Option<String>,
}

/// S3 connection definition as a resource.
/// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInline {
    /// Which access style to use.
    /// Defaults to virtual hosted-style as most of the data products out there.
    /// Have a look at the [AWS documentation](<https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html).>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "accessStyle")]
    pub access_style: Option<SparkApplicationS3connectionInlineAccessStyle>,
    /// If the S3 uses authentication you have to specify you S3 credentials.
    /// In the most cases a [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)>
    /// providing `accessKey` and `secretKey` is sufficient.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<SparkApplicationS3connectionInlineCredentials>,
    /// Host of the S3 server without any protocol or port. For example: `west1.my-cloud.com`.
    pub host: String,
    /// Port the S3 server listens on.
    /// If not specified the product will determine the port to use.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<u16>,
    /// Bucket region used for signing headers (sigv4).
    /// 
    /// This defaults to `us-east-1` which is compatible with other implementations such as Minio.
    /// 
    /// WARNING: Some products use the Hadoop S3 implementation which falls back to us-east-2.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub region: Option<SparkApplicationS3connectionInlineRegion>,
    /// Use a TLS connection. If not specified no TLS will be used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<SparkApplicationS3connectionInlineTls>,
}

/// S3 connection definition as a resource.
/// Learn more on the [S3 concept documentation](<https://docs.stackable.tech/home/nightly/concepts/s3).>
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationS3connectionInlineAccessStyle {
    Path,
    VirtualHosted,
}

/// If the S3 uses authentication you have to specify you S3 credentials.
/// In the most cases a [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)>
/// providing `accessKey` and `secretKey` is sufficient.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineCredentials {
    /// [Scope](<https://docs.stackable.tech/home/nightly/secret-operator/scope)> of the
    /// [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass).>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scope: Option<SparkApplicationS3connectionInlineCredentialsScope>,
    /// [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)> containing the LDAP bind credentials.
    #[serde(rename = "secretClass")]
    pub secret_class: String,
}

/// [Scope](<https://docs.stackable.tech/home/nightly/secret-operator/scope)> of the
/// [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass).>
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineCredentialsScope {
    /// The listener volume scope allows Node and Service scopes to be inferred from the applicable listeners.
    /// This must correspond to Volume names in the Pod that mount Listeners.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "listenerVolumes")]
    pub listener_volumes: Option<Vec<String>>,
    /// The node scope is resolved to the name of the Kubernetes Node object that the Pod is running on.
    /// This will typically be the DNS name of the node.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub node: Option<bool>,
    /// The pod scope is resolved to the name of the Kubernetes Pod.
    /// This allows the secret to differentiate between StatefulSet replicas.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pod: Option<bool>,
    /// The service scope allows Pod objects to specify custom scopes.
    /// This should typically correspond to Service objects that the Pod participates in.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub services: Option<Vec<String>>,
}

/// Bucket region used for signing headers (sigv4).
/// 
/// This defaults to `us-east-1` which is compatible with other implementations such as Minio.
/// 
/// WARNING: Some products use the Hadoop S3 implementation which falls back to us-east-2.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineRegion {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// Use a TLS connection. If not specified no TLS will be used.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineTls {
    /// The verification method used to verify the certificates of the server and/or the client.
    pub verification: SparkApplicationS3connectionInlineTlsVerification,
}

/// The verification method used to verify the certificates of the server and/or the client.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineTlsVerification {
    /// Use TLS but don't verify certificates.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub none: Option<SparkApplicationS3connectionInlineTlsVerificationNone>,
    /// Use TLS and a CA certificate to verify the server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<SparkApplicationS3connectionInlineTlsVerificationServer>,
}

/// Use TLS but don't verify certificates.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineTlsVerificationNone {
}

/// Use TLS and a CA certificate to verify the server.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineTlsVerificationServer {
    /// CA cert to verify the server.
    #[serde(rename = "caCert")]
    pub ca_cert: SparkApplicationS3connectionInlineTlsVerificationServerCaCert,
}

/// CA cert to verify the server.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineTlsVerificationServerCaCert {
    /// Name of the [SecretClass](<https://docs.stackable.tech/home/nightly/secret-operator/secretclass)> which will provide the CA certificate.
    /// Note that a SecretClass does not need to have a key but can also work with just a CA certificate,
    /// so if you got provided with a CA cert but don't have access to the key you can still use this method.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretClass")]
    pub secret_class: Option<String>,
    /// Use TLS and the CA certificates trusted by the common web browsers to verify the server.
    /// This can be useful when you e.g. use public AWS S3 or other public available services.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webPki")]
    pub web_pki: Option<SparkApplicationS3connectionInlineTlsVerificationServerCaCertWebPki>,
}

/// Use TLS and the CA certificates trusted by the common web browsers to verify the server.
/// This can be useful when you e.g. use public AWS S3 or other public available services.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationS3connectionInlineTlsVerificationServerCaCertWebPki {
}

/// Specify which image to use, the easiest way is to only configure the `productVersion`.
/// You can also configure a custom image registry to pull from, as well as completely custom
/// images.
/// 
/// Consult the [Product image selection documentation](<https://docs.stackable.tech/home/nightly/concepts/product_image_selection)>
/// for details.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationSparkImage {
    /// Overwrite the docker image.
    /// Specify the full docker image name, e.g. `oci.stackable.tech/sdp/superset:1.4.1-stackable2.1.0`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub custom: Option<String>,
    /// Version of the product, e.g. `1.4.1`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "productVersion")]
    pub product_version: Option<String>,
    /// [Pull policy](<https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy)> used when pulling the image.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pullPolicy")]
    pub pull_policy: Option<SparkApplicationSparkImagePullPolicy>,
    /// [Image pull secrets](<https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod)> to pull images from a private registry.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pullSecrets")]
    pub pull_secrets: Option<Vec<SparkApplicationSparkImagePullSecrets>>,
    /// Name of the docker repo, e.g. `oci.stackable.tech/sdp`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub repo: Option<String>,
    /// Stackable version of the product, e.g. `23.4`, `23.4.1` or `0.0.0-dev`.
    /// If not specified, the operator will use its own version, e.g. `23.4.1`.
    /// When using a nightly operator or a pr version, it will use the nightly `0.0.0-dev` image.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stackableVersion")]
    pub stackable_version: Option<String>,
}

/// Specify which image to use, the easiest way is to only configure the `productVersion`.
/// You can also configure a custom image registry to pull from, as well as completely custom
/// images.
/// 
/// Consult the [Product image selection documentation](<https://docs.stackable.tech/home/nightly/concepts/product_image_selection)>
/// for details.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationSparkImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationSparkImagePullSecrets {
    /// Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: <https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names>
    pub name: String,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationStatus {
    pub phase: String,
}

