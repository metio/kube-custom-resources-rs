// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --derive=Default --derive=PartialEq --smart-derive-elision --filename crd-catalog/stackabletech/spark-k8s-operator/spark.stackable.tech/v1alpha1/sparkconnectservers.yaml
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// An Apache Spark Connect server component. This resource is managed by the Stackable operator for Apache Spark. Find more information on how to use it in the [operator documentation](https://docs.stackable.tech/home/nightly/spark-k8s/usage-guide/connect-server).
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
#[kube(group = "spark.stackable.tech", version = "v1alpha1", kind = "SparkConnectServer", plural = "sparkconnectservers")]
#[kube(namespaced)]
#[kube(status = "SparkConnectServerStatus")]
#[kube(schema = "disabled")]
#[kube(derive="Default")]
#[kube(derive="PartialEq")]
pub struct SparkConnectServerSpec {
    /// User provided command line arguments appended to the server entry point.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Global Spark Connect server configuration that applies to all roles.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterConfig")]
    pub cluster_config: Option<SparkConnectServerClusterConfig>,
    /// [Cluster operations](https://docs.stackable.tech/home/nightly/concepts/operations/cluster_operations) properties, allow stopping the product instance as well as pausing reconciliation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterOperation")]
    pub cluster_operation: Option<SparkConnectServerClusterOperation>,
    /// Spark Connect executor properties.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub executor: Option<SparkConnectServerExecutor>,
    /// Specify which image to use, the easiest way is to only configure the `productVersion`. You can also configure a custom image registry to pull from, as well as completely custom images.
    /// 
    /// Consult the [Product image selection documentation](https://docs.stackable.tech/home/nightly/concepts/product_image_selection) for details.
    pub image: SparkConnectServerImage,
    /// A Spark Connect server definition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<SparkConnectServerServer>,
    /// Name of the Vector aggregator discovery ConfigMap. It must contain the key `ADDRESS` with the address of the Vector aggregator.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vectorAggregatorConfigMapName")]
    pub vector_aggregator_config_map_name: Option<String>,
}

/// Global Spark Connect server configuration that applies to all roles.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerClusterConfig {
    /// This field controls which type of Service the Operator creates for this ConnectServer:
    /// 
    /// * cluster-internal: Use a ClusterIP service
    /// 
    /// * external-unstable: Use a NodePort service
    /// 
    /// * external-stable: Use a LoadBalancer service
    /// 
    /// This is a temporary solution with the goal to keep yaml manifests forward compatible. In the future, this setting will control which ListenerClass <https://docs.stackable.tech/home/stable/listener-operator/listenerclass.html> will be used to expose the service, and ListenerClass names will stay the same, allowing for a non-breaking change.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "listenerClass")]
    pub listener_class: Option<SparkConnectServerClusterConfigListenerClass>,
}

/// Global Spark Connect server configuration that applies to all roles.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerClusterConfigListenerClass {
    #[serde(rename = "cluster-internal")]
    ClusterInternal,
    #[serde(rename = "external-unstable")]
    ExternalUnstable,
    #[serde(rename = "external-stable")]
    ExternalStable,
}

/// [Cluster operations](https://docs.stackable.tech/home/nightly/concepts/operations/cluster_operations) properties, allow stopping the product instance as well as pausing reconciliation.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerClusterOperation {
    /// Flag to stop cluster reconciliation by the operator. This means that all changes in the custom resource spec are ignored until this flag is set to false or removed. The operator will however still watch the deployed resources at the time and update the custom resource status field. If applied at the same time with `stopped`, `reconciliationPaused` will take precedence over `stopped` and stop the reconciliation immediately.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "reconciliationPaused")]
    pub reconciliation_paused: Option<bool>,
    /// Flag to stop the cluster. This means all deployed resources (e.g. Services, StatefulSets, ConfigMaps) are kept but all deployed Pods (e.g. replicas from a StatefulSet) are scaled to 0 and therefore stopped and removed. If applied at the same time with `reconciliationPaused`, the latter will pause reconciliation and `stopped` will take no effect until `reconciliationPaused` is set to false or removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stopped: Option<bool>,
}

/// Spark Connect executor properties.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutor {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cliOverrides")]
    pub cli_overrides: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<SparkConnectServerExecutorConfig>,
    /// The `configOverrides` can be used to configure properties in product config files that are not exposed in the CRD. Read the [config overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#config-overrides) and consult the operator specific usage guide documentation for details on the available config files and settings for the specific product.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configOverrides")]
    pub config_overrides: Option<BTreeMap<String, SparkConnectServerExecutorConfigOverrides>>,
    /// `envOverrides` configure environment variables to be set in the Pods. It is a map from strings to strings - environment variables and the value to set. Read the [environment variable overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#env-overrides) for more information and consult the operator specific usage guide to find out about the product specific environment variables that are available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envOverrides")]
    pub env_overrides: Option<BTreeMap<String, String>>,
    /// Allows overriding JVM arguments. Please read on the [JVM argument overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides) for details on the usage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jvmArgumentOverrides")]
    pub jvm_argument_overrides: Option<SparkConnectServerExecutorJvmArgumentOverrides>,
    /// In the `podOverrides` property you can define a [PodTemplateSpec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#podtemplatespec-v1-core) to override any property that can be set on a Kubernetes Pod. Read the [Pod overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#pod-overrides) for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podOverrides")]
    pub pod_overrides: Option<BTreeMap<String, serde_json::Value>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfig {
    /// These configuration settings control [Pod placement](https://docs.stackable.tech/home/nightly/concepts/operations/pod_placement).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<SparkConnectServerExecutorConfigAffinity>,
    /// Logging configuration, learn more in the [logging concept documentation](https://docs.stackable.tech/home/nightly/concepts/logging).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub logging: Option<SparkConnectServerExecutorConfigLogging>,
    /// Request secret (currently only autoTls certificates) lifetime from the secret operator, e.g. `7d`, or `30d`. This can be shortened by the `maxCertificateLifetime` setting on the SecretClass issuing the TLS certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requestedSecretLifetime")]
    pub requested_secret_lifetime: Option<String>,
    /// Resource usage is configured here, this includes CPU usage, memory usage and disk storage usage, if this role needs any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkConnectServerExecutorConfigResources>,
}

/// These configuration settings control [Pod placement](https://docs.stackable.tech/home/nightly/concepts/operations/pod_placement).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigAffinity {
    /// Same as the `spec.affinity.nodeAffinity` field on the Pod, see the [Kubernetes docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<BTreeMap<String, serde_json::Value>>,
    /// Simple key-value pairs forming a nodeSelector, see the [Kubernetes docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// Same as the `spec.affinity.podAffinity` field on the Pod, see the [Kubernetes docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<BTreeMap<String, serde_json::Value>>,
    /// Same as the `spec.affinity.podAntiAffinity` field on the Pod, see the [Kubernetes docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<BTreeMap<String, serde_json::Value>>,
}

/// Logging configuration, learn more in the [logging concept documentation](https://docs.stackable.tech/home/nightly/concepts/logging).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigLogging {
    /// Log configuration per container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub containers: Option<BTreeMap<String, SparkConnectServerExecutorConfigLoggingContainers>>,
    /// Wether or not to deploy a container with the Vector log agent.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVectorAgent")]
    pub enable_vector_agent: Option<bool>,
}

/// Log configuration per container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigLoggingContainers {
    /// Configuration for the console appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub console: Option<SparkConnectServerExecutorConfigLoggingContainersConsole>,
    /// Custom log configuration provided in a ConfigMap
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub custom: Option<SparkConnectServerExecutorConfigLoggingContainersCustom>,
    /// Configuration for the file appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<SparkConnectServerExecutorConfigLoggingContainersFile>,
    /// Configuration per logger
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub loggers: Option<BTreeMap<String, SparkConnectServerExecutorConfigLoggingContainersLoggers>>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigLoggingContainersConsole {
    /// The log level threshold. Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkConnectServerExecutorConfigLoggingContainersConsoleLevel>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerExecutorConfigLoggingContainersConsoleLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Custom log configuration provided in a ConfigMap
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigLoggingContainersCustom {
    /// ConfigMap containing the log configuration files
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<String>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigLoggingContainersFile {
    /// The log level threshold. Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkConnectServerExecutorConfigLoggingContainersFileLevel>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerExecutorConfigLoggingContainersFileLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigLoggingContainersLoggers {
    /// The log level threshold. Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkConnectServerExecutorConfigLoggingContainersLoggersLevel>,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerExecutorConfigLoggingContainersLoggersLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Resource usage is configured here, this includes CPU usage, memory usage and disk storage usage, if this role needs any.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigResources {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<SparkConnectServerExecutorConfigResourcesCpu>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<SparkConnectServerExecutorConfigResourcesMemory>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storage: Option<SparkConnectServerExecutorConfigResourcesStorage>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigResourcesCpu {
    /// The maximum amount of CPU cores that can be requested by Pods. Equivalent to the `limit` for Pod resource configuration. Cores are specified either as a decimal point number or as milli units. For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub max: Option<String>,
    /// The minimal amount of CPU cores that Pods need to run. Equivalent to the `request` for Pod resource configuration. Cores are specified either as a decimal point number or as milli units. For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub min: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigResourcesMemory {
    /// The maximum amount of memory that should be available to the Pod. Specified as a byte [Quantity](https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/), which means these suffixes are supported: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value: `128974848, 129e6, 129M,  128974848000m, 123Mi`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limit: Option<String>,
    /// Additional options that can be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runtimeLimits")]
    pub runtime_limits: Option<SparkConnectServerExecutorConfigResourcesMemoryRuntimeLimits>,
}

/// Additional options that can be specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigResourcesMemoryRuntimeLimits {
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorConfigResourcesStorage {
}

/// Allows overriding JVM arguments. Please read on the [JVM argument overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides) for details on the usage.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerExecutorJvmArgumentOverrides {
    /// JVM arguments to be added
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// JVM arguments to be removed by exact match
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub remove: Option<Vec<String>>,
    /// JVM arguments matching any of this regexes will be removed
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "removeRegex")]
    pub remove_regex: Option<Vec<String>>,
}

/// Specify which image to use, the easiest way is to only configure the `productVersion`. You can also configure a custom image registry to pull from, as well as completely custom images.
/// 
/// Consult the [Product image selection documentation](https://docs.stackable.tech/home/nightly/concepts/product_image_selection) for details.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerImage {
    /// Overwrite the docker image. Specify the full docker image name, e.g. `oci.stackable.tech/sdp/superset:1.4.1-stackable2.1.0`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub custom: Option<String>,
    /// Version of the product, e.g. `1.4.1`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "productVersion")]
    pub product_version: Option<String>,
    /// [Pull policy](https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy) used when pulling the image.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pullPolicy")]
    pub pull_policy: Option<SparkConnectServerImagePullPolicy>,
    /// [Image pull secrets](https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod) to pull images from a private registry.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pullSecrets")]
    pub pull_secrets: Option<Vec<SparkConnectServerImagePullSecrets>>,
    /// Name of the docker repo, e.g. `oci.stackable.tech/sdp`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub repo: Option<String>,
    /// Stackable version of the product, e.g. `23.4`, `23.4.1` or `0.0.0-dev`. If not specified, the operator will use its own version, e.g. `23.4.1`. When using a nightly operator or a pr version, it will use the nightly `0.0.0-dev` image.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stackableVersion")]
    pub stackable_version: Option<String>,
}

/// Specify which image to use, the easiest way is to only configure the `productVersion`. You can also configure a custom image registry to pull from, as well as completely custom images.
/// 
/// Consult the [Product image selection documentation](https://docs.stackable.tech/home/nightly/concepts/product_image_selection) for details.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerImagePullSecrets {
    /// Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
}

/// A Spark Connect server definition.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServer {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cliOverrides")]
    pub cli_overrides: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<SparkConnectServerServerConfig>,
    /// The `configOverrides` can be used to configure properties in product config files that are not exposed in the CRD. Read the [config overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#config-overrides) and consult the operator specific usage guide documentation for details on the available config files and settings for the specific product.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configOverrides")]
    pub config_overrides: Option<BTreeMap<String, SparkConnectServerServerConfigOverrides>>,
    /// `envOverrides` configure environment variables to be set in the Pods. It is a map from strings to strings - environment variables and the value to set. Read the [environment variable overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#env-overrides) for more information and consult the operator specific usage guide to find out about the product specific environment variables that are available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envOverrides")]
    pub env_overrides: Option<BTreeMap<String, String>>,
    /// Allows overriding JVM arguments. Please read on the [JVM argument overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides) for details on the usage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jvmArgumentOverrides")]
    pub jvm_argument_overrides: Option<SparkConnectServerServerJvmArgumentOverrides>,
    /// In the `podOverrides` property you can define a [PodTemplateSpec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#podtemplatespec-v1-core) to override any property that can be set on a Kubernetes Pod. Read the [Pod overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#pod-overrides) for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podOverrides")]
    pub pod_overrides: Option<BTreeMap<String, serde_json::Value>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfig {
    /// Logging configuration, learn more in the [logging concept documentation](https://docs.stackable.tech/home/nightly/concepts/logging).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub logging: Option<SparkConnectServerServerConfigLogging>,
    /// Request secret (currently only autoTls certificates) lifetime from the secret operator, e.g. `7d`, or `30d`. This can be shortened by the `maxCertificateLifetime` setting on the SecretClass issuing the TLS certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requestedSecretLifetime")]
    pub requested_secret_lifetime: Option<String>,
    /// Resource usage is configured here, this includes CPU usage, memory usage and disk storage usage, if this role needs any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkConnectServerServerConfigResources>,
}

/// Logging configuration, learn more in the [logging concept documentation](https://docs.stackable.tech/home/nightly/concepts/logging).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigLogging {
    /// Log configuration per container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub containers: Option<BTreeMap<String, SparkConnectServerServerConfigLoggingContainers>>,
    /// Wether or not to deploy a container with the Vector log agent.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVectorAgent")]
    pub enable_vector_agent: Option<bool>,
}

/// Log configuration per container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigLoggingContainers {
    /// Configuration for the console appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub console: Option<SparkConnectServerServerConfigLoggingContainersConsole>,
    /// Custom log configuration provided in a ConfigMap
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub custom: Option<SparkConnectServerServerConfigLoggingContainersCustom>,
    /// Configuration for the file appender
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<SparkConnectServerServerConfigLoggingContainersFile>,
    /// Configuration per logger
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub loggers: Option<BTreeMap<String, SparkConnectServerServerConfigLoggingContainersLoggers>>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigLoggingContainersConsole {
    /// The log level threshold. Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkConnectServerServerConfigLoggingContainersConsoleLevel>,
}

/// Configuration for the console appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerServerConfigLoggingContainersConsoleLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Custom log configuration provided in a ConfigMap
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigLoggingContainersCustom {
    /// ConfigMap containing the log configuration files
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<String>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigLoggingContainersFile {
    /// The log level threshold. Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkConnectServerServerConfigLoggingContainersFileLevel>,
}

/// Configuration for the file appender
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerServerConfigLoggingContainersFileLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigLoggingContainersLoggers {
    /// The log level threshold. Log events with a lower log level are discarded.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<SparkConnectServerServerConfigLoggingContainersLoggersLevel>,
}

/// Configuration per logger
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkConnectServerServerConfigLoggingContainersLoggersLevel {
    #[serde(rename = "TRACE")]
    Trace,
    #[serde(rename = "DEBUG")]
    Debug,
    #[serde(rename = "INFO")]
    Info,
    #[serde(rename = "WARN")]
    Warn,
    #[serde(rename = "ERROR")]
    Error,
    #[serde(rename = "FATAL")]
    Fatal,
    #[serde(rename = "NONE")]
    None,
}

/// Resource usage is configured here, this includes CPU usage, memory usage and disk storage usage, if this role needs any.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigResources {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<SparkConnectServerServerConfigResourcesCpu>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<SparkConnectServerServerConfigResourcesMemory>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storage: Option<SparkConnectServerServerConfigResourcesStorage>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigResourcesCpu {
    /// The maximum amount of CPU cores that can be requested by Pods. Equivalent to the `limit` for Pod resource configuration. Cores are specified either as a decimal point number or as milli units. For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub max: Option<String>,
    /// The minimal amount of CPU cores that Pods need to run. Equivalent to the `request` for Pod resource configuration. Cores are specified either as a decimal point number or as milli units. For example:`1.5` will be 1.5 cores, also written as `1500m`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub min: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigResourcesMemory {
    /// The maximum amount of memory that should be available to the Pod. Specified as a byte [Quantity](https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/), which means these suffixes are supported: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value: `128974848, 129e6, 129M,  128974848000m, 123Mi`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limit: Option<String>,
    /// Additional options that can be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runtimeLimits")]
    pub runtime_limits: Option<SparkConnectServerServerConfigResourcesMemoryRuntimeLimits>,
}

/// Additional options that can be specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigResourcesMemoryRuntimeLimits {
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerConfigResourcesStorage {
}

/// Allows overriding JVM arguments. Please read on the [JVM argument overrides documentation](https://docs.stackable.tech/home/nightly/concepts/overrides#jvm-argument-overrides) for details on the usage.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerServerJvmArgumentOverrides {
    /// JVM arguments to be added
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// JVM arguments to be removed by exact match
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub remove: Option<Vec<String>>,
    /// JVM arguments matching any of this regexes will be removed
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "removeRegex")]
    pub remove_regex: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkConnectServerStatus {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Condition>>,
}

