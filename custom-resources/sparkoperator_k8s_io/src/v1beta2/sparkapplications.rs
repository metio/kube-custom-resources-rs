// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --derive=Default --derive=PartialEq --smart-derive-elision --filename crd-catalog/GoogleCloudPlatform/spark-on-k8s-operator/sparkoperator.k8s.io/v1beta2/sparkapplications.yaml
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
    pub use k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;
}
use self::prelude::*;

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, PartialEq)]
#[kube(group = "sparkoperator.k8s.io", version = "v1beta2", kind = "SparkApplication", plural = "sparkapplications")]
#[kube(namespaced)]
#[kube(status = "SparkApplicationStatus")]
#[kube(schema = "disabled")]
#[kube(derive="PartialEq")]
pub struct SparkApplicationSpec {
    /// Arguments is a list of arguments to be passed to the application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arguments: Option<Vec<String>>,
    /// BatchScheduler configures which batch scheduler will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchScheduler")]
    pub batch_scheduler: Option<String>,
    /// BatchSchedulerOptions provides fine-grained control on how to batch scheduling.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSchedulerOptions")]
    pub batch_scheduler_options: Option<SparkApplicationBatchSchedulerOptions>,
    /// Deps captures all possible types of dependencies of a Spark application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub deps: Option<SparkApplicationDeps>,
    /// Driver is the driver specification.
    pub driver: SparkApplicationDriver,
    /// DriverIngressOptions allows configuring the Service and the Ingress to expose ports inside Spark Driver
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverIngressOptions")]
    pub driver_ingress_options: Option<Vec<SparkApplicationDriverIngressOptions>>,
    /// DynamicAllocation configures dynamic allocation that becomes available for the Kubernetes
    /// scheduler backend since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dynamicAllocation")]
    pub dynamic_allocation: Option<SparkApplicationDynamicAllocation>,
    /// Executor is the executor specification.
    pub executor: SparkApplicationExecutor,
    /// FailureRetries is the number of times to retry a failed application before giving up.
    /// This is best effort and actual retry attempts can be >= the value specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureRetries")]
    pub failure_retries: Option<i32>,
    /// HadoopConf carries user-specified Hadoop configuration properties as they would use the "--conf" option
    /// in spark-submit. The SparkApplication controller automatically adds prefix "spark.hadoop." to Hadoop
    /// configuration properties.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConf")]
    pub hadoop_conf: Option<BTreeMap<String, String>>,
    /// HadoopConfigMap carries the name of the ConfigMap containing Hadoop configuration files such as core-site.xml.
    /// The controller will add environment variable HADOOP_CONF_DIR to the path where the ConfigMap is mounted to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConfigMap")]
    pub hadoop_config_map: Option<String>,
    /// Image is the container image for the driver, executor, and init-container. Any custom container images for the
    /// driver, executor, or init-container takes precedence over this.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// ImagePullPolicy is the image pull policy for the driver, executor, and init-container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// ImagePullSecrets is the list of image-pull secrets.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullSecrets")]
    pub image_pull_secrets: Option<Vec<String>>,
    /// MainFile is the path to a bundled JAR, Python, or R file of the application.
    #[serde(rename = "mainApplicationFile")]
    pub main_application_file: String,
    /// MainClass is the fully-qualified main class of the Spark application.
    /// This only applies to Java/Scala Spark applications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// This sets the Memory Overhead Factor that will allocate memory to non-JVM memory.
    /// For JVM-based jobs this value will default to 0.10, for non-JVM jobs 0.40. Value of this field will
    /// be overridden by `Spec.Driver.MemoryOverhead` and `Spec.Executor.MemoryOverhead` if they are set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverheadFactor")]
    pub memory_overhead_factor: Option<String>,
    /// Mode is the deployment mode of the Spark application.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<SparkApplicationMode>,
    /// Monitoring configures how monitoring is handled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub monitoring: Option<SparkApplicationMonitoring>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at podSpec level (driver or executor).
    /// This field will be deprecated in future versions (at SparkApplicationSpec level).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// ProxyUser specifies the user to impersonate when submitting the application.
    /// It maps to the command-line flag "--proxy-user" in spark-submit.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "proxyUser")]
    pub proxy_user: Option<String>,
    /// This sets the major Python version of the docker
    /// image used to run the driver and executor containers. Can either be 2 or 3, default 2.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonVersion")]
    pub python_version: Option<SparkApplicationPythonVersion>,
    /// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<SparkApplicationRestartPolicy>,
    /// RetryInterval is the unit of intervals in seconds between submission retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryInterval")]
    pub retry_interval: Option<i64>,
    /// SparkConf carries user-specified Spark configuration properties as they would use the  "--conf" option in
    /// spark-submit.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConf")]
    pub spark_conf: Option<BTreeMap<String, String>>,
    /// SparkConfigMap carries the name of the ConfigMap containing Spark configuration files such as log4j.properties.
    /// The controller will add environment variable SPARK_CONF_DIR to the path where the ConfigMap is mounted to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConfigMap")]
    pub spark_config_map: Option<String>,
    /// SparkUIOptions allows configuring the Service and the Ingress to expose the sparkUI
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkUIOptions")]
    pub spark_ui_options: Option<SparkApplicationSparkUiOptions>,
    /// SparkVersion is the version of Spark the application uses.
    #[serde(rename = "sparkVersion")]
    pub spark_version: String,
    /// TimeToLiveSeconds defines the Time-To-Live (TTL) duration in seconds for this SparkApplication
    /// after its termination.
    /// The SparkApplication object will be garbage collected if the current time is more than the
    /// TimeToLiveSeconds since its termination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeToLiveSeconds")]
    pub time_to_live_seconds: Option<i64>,
    /// Type tells the type of the Spark application.
    #[serde(rename = "type")]
    pub r#type: SparkApplicationType,
    /// Volumes is the list of Kubernetes volumes that can be mounted by the driver and/or executors.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub volumes: Option<Vec<SparkApplicationVolumes>>,
}

/// BatchSchedulerOptions provides fine-grained control on how to batch scheduling.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationBatchSchedulerOptions {
    /// PriorityClassName stands for the name of k8s PriorityClass resource, it's being used in Volcano batch scheduler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// Queue stands for the resource queue which the application belongs to, it's being used in Volcano batch scheduler.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queue: Option<String>,
    /// Resources stands for the resource list custom request for. Usually it is used to define the lower-bound limit.
    /// If specified, volcano scheduler will consider it as the resources requested.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<BTreeMap<String, IntOrString>>,
}

/// Deps captures all possible types of dependencies of a Spark application.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDeps {
    /// Archives is a list of archives to be extracted into the working directory of each executor.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub archives: Option<Vec<String>>,
    /// ExcludePackages is a list of "groupId:artifactId", to exclude while resolving the
    /// dependencies provided in Packages to avoid dependency conflicts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "excludePackages")]
    pub exclude_packages: Option<Vec<String>>,
    /// Files is a list of files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub files: Option<Vec<String>>,
    /// Jars is a list of JAR files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub jars: Option<Vec<String>>,
    /// Packages is a list of maven coordinates of jars to include on the driver and executor
    /// classpaths. This will search the local maven repo, then maven central and any additional
    /// remote repositories given by the "repositories" option.
    /// Each package should be of the form "groupId:artifactId:version".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub packages: Option<Vec<String>>,
    /// PyFiles is a list of Python files the Spark application depends on.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pyFiles")]
    pub py_files: Option<Vec<String>>,
    /// Repositories is a list of additional remote repositories to search for the maven coordinate
    /// given with the "packages" option.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub repositories: Option<Vec<String>>,
}

/// Driver is the driver specification.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriver {
    /// Affinity specifies the affinity/anti-affinity settings for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<SparkApplicationDriverAffinity>,
    /// Annotations are the Kubernetes annotations to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<BTreeMap<String, String>>,
    /// ConfigMaps carries information of other ConfigMaps to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMaps")]
    pub config_maps: Option<Vec<SparkApplicationDriverConfigMaps>>,
    /// CoreLimit specifies a hard limit on CPU cores for the pod.
    /// Optional
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreLimit")]
    pub core_limit: Option<String>,
    /// CoreRequest is the physical CPU core request for the driver.
    /// Maps to `spark.kubernetes.driver.request.cores` that is available since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreRequest")]
    pub core_request: Option<String>,
    /// Cores maps to `spark.driver.cores` or `spark.executor.cores` for the driver and executors, respectively.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cores: Option<i32>,
    /// DnsConfig dns settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dnsConfig")]
    pub dns_config: Option<SparkApplicationDriverDnsConfig>,
    /// Env carries the environment variables to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationDriverEnv>>,
    /// EnvFrom is a list of sources to populate environment variables in the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationDriverEnvFrom>>,
    /// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envSecretKeyRefs")]
    pub env_secret_key_refs: Option<BTreeMap<String, SparkApplicationDriverEnvSecretKeyRefs>>,
    /// EnvVars carries the environment variables to add to the pod.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envVars")]
    pub env_vars: Option<BTreeMap<String, String>>,
    /// GPU specifies GPU requirement for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu: Option<SparkApplicationDriverGpu>,
    /// HostAliases settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostAliases")]
    pub host_aliases: Option<Vec<SparkApplicationDriverHostAliases>>,
    /// HostNetwork indicates whether to request host networking for the pod or not.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostNetwork")]
    pub host_network: Option<bool>,
    /// Image is the container image to use. Overrides Spec.Image if set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// InitContainers is a list of init-containers that run to completion before the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initContainers")]
    pub init_containers: Option<Vec<SparkApplicationDriverInitContainers>>,
    /// JavaOptions is a string of extra JVM options to pass to the driver. For instance,
    /// GC settings or other logging.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "javaOptions")]
    pub java_options: Option<String>,
    /// KubernetesMaster is the URL of the Kubernetes master used by the driver to manage executor pods and
    /// other Kubernetes resources. Default to https://kubernetes.default.svc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesMaster")]
    pub kubernetes_master: Option<String>,
    /// Labels are the Kubernetes labels to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<BTreeMap<String, String>>,
    /// Lifecycle for running preStop or postStart commands
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationDriverLifecycle>,
    /// Memory is the amount of memory to request for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// MemoryOverhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverhead")]
    pub memory_overhead: Option<String>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at SparkApplication level (which will be deprecated).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// PodName is the name of the driver pod that the user creates. This is used for the
    /// in-cluster client mode in which the user creates a client pod where the driver of
    /// the user application runs. It's an error to set this field if Mode is not
    /// in-cluster-client.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podName")]
    pub pod_name: Option<String>,
    /// PodSecurityContext specifies the PodSecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podSecurityContext")]
    pub pod_security_context: Option<SparkApplicationDriverPodSecurityContext>,
    /// Ports settings for the pods, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationDriverPorts>>,
    /// PriorityClassName is the name of the PriorityClass for the driver pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// SchedulerName specifies the scheduler that will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "schedulerName")]
    pub scheduler_name: Option<String>,
    /// Secrets carries information of secrets to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secrets: Option<Vec<SparkApplicationDriverSecrets>>,
    /// SecurityContext specifies the container's SecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationDriverSecurityContext>,
    /// ServiceAccount is the name of the custom Kubernetes service account used by the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// ServiceAnnotations defines the annotations to be added to the Kubernetes headless service used by
    /// executors to connect to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels defines the labels to be added to the Kubernetes headless service used by
    /// executors to connect to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ShareProcessNamespace settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shareProcessNamespace")]
    pub share_process_namespace: Option<bool>,
    /// Sidecars is a list of sidecar containers that run along side the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sidecars: Option<Vec<SparkApplicationDriverSidecars>>,
    /// Template is a pod template that can be used to define the driver or executor pod configurations that Spark configurations do not support.
    /// Spark version >= 3.0.0 is required.
    /// Ref: https://spark.apache.org/docs/latest/running-on-kubernetes.html#pod-template.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub template: Option<BTreeMap<String, serde_json::Value>>,
    /// Termination grace period seconds for the pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Tolerations specifies the tolerations listed in ".spec.tolerations" to be applied to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tolerations: Option<Vec<SparkApplicationDriverTolerations>>,
    /// VolumeMounts specifies the volumes listed in ".spec.volumes" to mount into the main container's filesystem.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationDriverVolumeMounts>>,
}

/// Affinity specifies the affinity/anti-affinity settings for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinity {
    /// Describes node affinity scheduling rules for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<SparkApplicationDriverAffinityNodeAffinity>,
    /// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<SparkApplicationDriverAffinityPodAffinity>,
    /// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<SparkApplicationDriverAffinityPodAntiAffinity>,
}

/// Describes node affinity scheduling rules for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node matches the corresponding matchExpressions; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to an update), the system
    /// may or may not try to eventually evict the pod from its node.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution>,
}

/// An empty preferred scheduling term matches all objects with implicit weight 0
/// (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// A node selector term, associated with the corresponding weight.
    pub preference: SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference,
    /// Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.
    pub weight: i32,
}

/// A node selector term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// If the affinity requirements specified by this field are not met at
/// scheduling time, the pod will not be scheduled onto the node.
/// If the affinity requirements specified by this field cease to be met
/// at some point during pod execution (e.g. due to an update), the system
/// may or may not try to eventually evict the pod from its node.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// Required. A list of node selector terms. The terms are ORed.
    #[serde(rename = "nodeSelectorTerms")]
    pub node_selector_terms: Vec<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms>,
}

/// A null or empty node selector term matches no objects. The requirements of
/// them are ANDed.
/// The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the anti-affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling anti-affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the anti-affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the anti-affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// NamePath is a pair of a name and a path to which the named objects should be mounted to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverConfigMaps {
    pub name: String,
    pub path: String,
}

/// DnsConfig dns settings for the pod, following the Kubernetes specifications.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverDnsConfig {
    /// A list of DNS name server IP addresses.
    /// This will be appended to the base nameservers generated from DNSPolicy.
    /// Duplicated nameservers will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nameservers: Option<Vec<String>>,
    /// A list of DNS resolver options.
    /// This will be merged with the base options generated from DNSPolicy.
    /// Duplicated entries will be removed. Resolution options given in Options
    /// will override those that appear in the base DNSPolicy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<Vec<SparkApplicationDriverDnsConfigOptions>>,
    /// A list of DNS search domains for host-name lookup.
    /// This will be appended to the base search paths generated from DNSPolicy.
    /// Duplicated search paths will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub searches: Option<Vec<String>>,
}

/// PodDNSConfigOption defines DNS resolver options of a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverDnsConfigOptions {
    /// Name is this DNS resolver option's name.
    /// Required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Value is this DNS resolver option's value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationDriverEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationDriverEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationDriverEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationDriverEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationDriverEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationDriverEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationDriverEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvFromConfigMapRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvFromSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
/// Deprecated. Consider using `env` instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverEnvSecretKeyRefs {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// GPU specifies GPU requirement for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverGpu {
    /// Name is GPU resource name, such as: nvidia.com/gpu or amd.com/gpu
    pub name: String,
    /// Quantity is the number of GPUs to request for driver or executor.
    pub quantity: i64,
}

/// HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the
/// pod's hosts file.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverHostAliases {
    /// Hostnames for the above IP address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hostnames: Option<Vec<String>>,
    /// IP address of the host file entry.
    pub ip: String,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainers {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationDriverInitContainersEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationDriverInitContainersEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationDriverInitContainersLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationDriverInitContainersLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationDriverInitContainersPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationDriverInitContainersReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationDriverInitContainersResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationDriverInitContainersResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationDriverInitContainersSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationDriverInitContainersStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationDriverInitContainersVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationDriverInitContainersVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationDriverInitContainersEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationDriverInitContainersEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationDriverInitContainersEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationDriverInitContainersEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationDriverInitContainersEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationDriverInitContainersEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationDriverInitContainersEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvFromConfigMapRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersEnvFromSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationDriverInitContainersLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationDriverInitContainersLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStart {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersLifecyclePostStartExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersLifecyclePostStartHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverInitContainersLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersLifecyclePostStartTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStop {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersLifecyclePreStopExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersLifecyclePreStopHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverInitContainersLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersLifecyclePreStopTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverInitContainersLivenessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverInitContainersReadinessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationDriverInitContainersResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
    /// Request is the name chosen for a request in the referenced claim.
    /// If empty, everything from the claim is made available, otherwise
    /// only the result of this request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub request: Option<String>,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// appArmorProfile is the AppArmor options to use by this container. If set, this profile
    /// overrides the pod's appArmorProfile.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationDriverInitContainersSecurityContextAppArmorProfile>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationDriverInitContainersSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default value is Default which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverInitContainersSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverInitContainersSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverInitContainersSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by this container. If set, this profile
/// overrides the pod's appArmorProfile.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverInitContainersStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverInitContainersStartupProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverInitContainersStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverInitContainersStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverInitContainersStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverInitContainersVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    /// When RecursiveReadOnly is set to IfPossible or to Enabled, MountPropagation must be None or unspecified
    /// (which defaults to None).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// RecursiveReadOnly specifies whether read-only mounts should be handled
    /// recursively.
    /// 
    /// If ReadOnly is false, this field has no meaning and must be unspecified.
    /// 
    /// If ReadOnly is true, and this field is set to Disabled, the mount is not made
    /// recursively read-only.  If this field is set to IfPossible, the mount is made
    /// recursively read-only, if it is supported by the container runtime.  If this
    /// field is set to Enabled, the mount is made recursively read-only if it is
    /// supported by the container runtime, otherwise the pod will not be started and
    /// an error will be generated to indicate the reason.
    /// 
    /// If this field is set to IfPossible or Enabled, MountPropagation must be set to
    /// None (or be unspecified, which defaults to None).
    /// 
    /// If this field is not specified, it is treated as an equivalent of Disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recursiveReadOnly")]
    pub recursive_read_only: Option<String>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// Lifecycle for running preStop or postStart commands
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationDriverLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationDriverLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStart {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverLifecyclePostStartExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverLifecyclePostStartHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverLifecyclePostStartTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStop {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverLifecyclePreStopExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverLifecyclePreStopHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverLifecyclePreStopTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PodSecurityContext specifies the PodSecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContext {
    /// appArmorProfile is the AppArmor options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationDriverPodSecurityContextAppArmorProfile>,
    /// A special supplemental group that applies to all containers in a pod.
    /// Some volume types allow the Kubelet to change the ownership of that volume
    /// to be owned by the pod:
    /// 
    /// 1. The owning GID will be the FSGroup
    /// 2. The setgid bit is set (new files created in the volume will be owned by FSGroup)
    /// 3. The permission bits are OR'd with rw-rw----
    /// 
    /// If unset, the Kubelet will not modify the ownership and permissions of any volume.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroup")]
    pub fs_group: Option<i64>,
    /// fsGroupChangePolicy defines behavior of changing ownership and permission of the volume
    /// before being exposed inside Pod. This field will only apply to
    /// volume types which support fsGroup based ownership(and permissions).
    /// It will have no effect on ephemeral volume types such as: secret, configmaps
    /// and emptydir.
    /// Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroupChangePolicy")]
    pub fs_group_change_policy: Option<String>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// seLinuxChangePolicy defines how the container's SELinux label is applied to all volumes used by the Pod.
    /// It has no effect on nodes that do not support SELinux or to volumes does not support SELinux.
    /// Valid values are "MountOption" and "Recursive".
    /// 
    /// "Recursive" means relabeling of all files on all Pod volumes by the container runtime.
    /// This may be slow for large volumes, but allows mixing privileged and unprivileged Pods sharing the same volume on the same node.
    /// 
    /// "MountOption" mounts all eligible Pod volumes with `-o context` mount option.
    /// This requires all Pods that share the same volume to use the same SELinux label.
    /// It is not possible to share the same volume among privileged and unprivileged Pods.
    /// Eligible volumes are in-tree FibreChannel and iSCSI volumes, and all CSI volumes
    /// whose CSI driver announces SELinux support by setting spec.seLinuxMount: true in their
    /// CSIDriver instance. Other volumes are always re-labelled recursively.
    /// "MountOption" value is allowed only when SELinuxMount feature gate is enabled.
    /// 
    /// If not specified and SELinuxMount feature gate is enabled, "MountOption" is used.
    /// If not specified and SELinuxMount feature gate is disabled, "MountOption" is used for ReadWriteOncePod volumes
    /// and "Recursive" for all other volumes.
    /// 
    /// This field affects only Pods that have SELinux label set, either in PodSecurityContext or in SecurityContext of all containers.
    /// 
    /// All Pods that use the same volume should use the same seLinuxChangePolicy, otherwise some pods can get stuck in ContainerCreating state.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxChangePolicy")]
    pub se_linux_change_policy: Option<String>,
    /// The SELinux context to be applied to all containers.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in SecurityContext.  If set in
    /// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
    /// takes precedence for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverPodSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverPodSecurityContextSeccompProfile>,
    /// A list of groups applied to the first process run in each container, in
    /// addition to the container's primary GID and fsGroup (if specified).  If
    /// the SupplementalGroupsPolicy feature is enabled, the
    /// supplementalGroupsPolicy field determines whether these are in addition
    /// to or instead of any group memberships defined in the container image.
    /// If unspecified, no additional groups are added, though group memberships
    /// defined in the container image may still be used, depending on the
    /// supplementalGroupsPolicy field.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroups")]
    pub supplemental_groups: Option<Vec<i64>>,
    /// Defines how supplemental groups of the first container processes are calculated.
    /// Valid values are "Merge" and "Strict". If not specified, "Merge" is used.
    /// (Alpha) Using the field requires the SupplementalGroupsPolicy feature gate to be enabled
    /// and the container runtime must implement support for this feature.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroupsPolicy")]
    pub supplemental_groups_policy: Option<String>,
    /// Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported
    /// sysctls (by the container runtime) might fail to launch.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sysctls: Option<Vec<SparkApplicationDriverPodSecurityContextSysctls>>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options within a container's SecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverPodSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The SELinux context to be applied to all containers.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in SecurityContext.  If set in
/// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
/// takes precedence for that container.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Sysctl defines a kernel parameter to be set
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextSysctls {
    /// Name of a property to set
    pub name: String,
    /// Value of a property to set
    pub value: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options within a container's SecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPodSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// Port represents the port definition in the pods objects.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverPorts {
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    pub name: String,
    pub protocol: String,
}

/// SecretInfo captures information of a secret.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecrets {
    pub name: String,
    pub path: String,
    /// SecretType tells the type of a secret.
    #[serde(rename = "secretType")]
    pub secret_type: String,
}

/// SecurityContext specifies the container's SecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// appArmorProfile is the AppArmor options to use by this container. If set, this profile
    /// overrides the pod's appArmorProfile.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationDriverSecurityContextAppArmorProfile>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationDriverSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default value is Default which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by this container. If set, this profile
/// overrides the pod's appArmorProfile.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecars {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationDriverSidecarsEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationDriverSidecarsEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationDriverSidecarsLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationDriverSidecarsLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationDriverSidecarsPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationDriverSidecarsReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationDriverSidecarsResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationDriverSidecarsResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationDriverSidecarsSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationDriverSidecarsStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationDriverSidecarsVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationDriverSidecarsVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationDriverSidecarsEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationDriverSidecarsEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationDriverSidecarsEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationDriverSidecarsEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationDriverSidecarsEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationDriverSidecarsEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationDriverSidecarsEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvFromConfigMapRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsEnvFromSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationDriverSidecarsLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationDriverSidecarsLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStart {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsLifecyclePostStartExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsLifecyclePostStartHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverSidecarsLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsLifecyclePostStartTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStop {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsLifecyclePreStopExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsLifecyclePreStopHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationDriverSidecarsLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsLifecyclePreStopTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverSidecarsLivenessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverSidecarsReadinessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationDriverSidecarsResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
    /// Request is the name chosen for a request in the referenced claim.
    /// If empty, everything from the claim is made available, otherwise
    /// only the result of this request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub request: Option<String>,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// appArmorProfile is the AppArmor options to use by this container. If set, this profile
    /// overrides the pod's appArmorProfile.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationDriverSidecarsSecurityContextAppArmorProfile>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationDriverSidecarsSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default value is Default which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationDriverSidecarsSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationDriverSidecarsSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationDriverSidecarsSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by this container. If set, this profile
/// overrides the pod's appArmorProfile.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationDriverSidecarsStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationDriverSidecarsStartupProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationDriverSidecarsStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationDriverSidecarsStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationDriverSidecarsStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverSidecarsVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    /// When RecursiveReadOnly is set to IfPossible or to Enabled, MountPropagation must be None or unspecified
    /// (which defaults to None).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// RecursiveReadOnly specifies whether read-only mounts should be handled
    /// recursively.
    /// 
    /// If ReadOnly is false, this field has no meaning and must be unspecified.
    /// 
    /// If ReadOnly is true, and this field is set to Disabled, the mount is not made
    /// recursively read-only.  If this field is set to IfPossible, the mount is made
    /// recursively read-only, if it is supported by the container runtime.  If this
    /// field is set to Enabled, the mount is made recursively read-only if it is
    /// supported by the container runtime, otherwise the pod will not be started and
    /// an error will be generated to indicate the reason.
    /// 
    /// If this field is set to IfPossible or Enabled, MountPropagation must be set to
    /// None (or be unspecified, which defaults to None).
    /// 
    /// If this field is not specified, it is treated as an equivalent of Disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recursiveReadOnly")]
    pub recursive_read_only: Option<String>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// The pod this Toleration is attached to tolerates any taint that matches
/// the triple <key,value,effect> using the matching operator <operator>.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverTolerations {
    /// Effect indicates the taint effect to match. Empty means match all taint effects.
    /// When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub effect: Option<String>,
    /// Key is the taint key that the toleration applies to. Empty means match all taint keys.
    /// If the key is empty, operator must be Exists; this combination means to match all values and all keys.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Operator represents a key's relationship to the value.
    /// Valid operators are Exists and Equal. Defaults to Equal.
    /// Exists is equivalent to wildcard for value, so that a pod can
    /// tolerate all taints of a particular category.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub operator: Option<String>,
    /// TolerationSeconds represents the period of time the toleration (which must be
    /// of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default,
    /// it is not set, which means tolerate the taint forever (do not evict). Zero and
    /// negative values will be treated as 0 (evict immediately) by the system.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tolerationSeconds")]
    pub toleration_seconds: Option<i64>,
    /// Value is the taint value the toleration matches to.
    /// If the operator is Exists, the value should be empty, otherwise just a regular string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    /// When RecursiveReadOnly is set to IfPossible or to Enabled, MountPropagation must be None or unspecified
    /// (which defaults to None).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// RecursiveReadOnly specifies whether read-only mounts should be handled
    /// recursively.
    /// 
    /// If ReadOnly is false, this field has no meaning and must be unspecified.
    /// 
    /// If ReadOnly is true, and this field is set to Disabled, the mount is not made
    /// recursively read-only.  If this field is set to IfPossible, the mount is made
    /// recursively read-only, if it is supported by the container runtime.  If this
    /// field is set to Enabled, the mount is made recursively read-only if it is
    /// supported by the container runtime, otherwise the pod will not be started and
    /// an error will be generated to indicate the reason.
    /// 
    /// If this field is set to IfPossible or Enabled, MountPropagation must be set to
    /// None (or be unspecified, which defaults to None).
    /// 
    /// If this field is not specified, it is treated as an equivalent of Disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recursiveReadOnly")]
    pub recursive_read_only: Option<String>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// DriverIngressConfiguration is for driver ingress specific configuration parameters.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverIngressOptions {
    /// IngressAnnotations is a map of key,value pairs of annotations that might be added to the ingress object. i.e. specify nginx as ingress.class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressAnnotations")]
    pub ingress_annotations: Option<BTreeMap<String, String>>,
    /// TlsHosts is useful If we need to declare SSL certificates to the ingress object
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressTLS")]
    pub ingress_tls: Option<Vec<SparkApplicationDriverIngressOptionsIngressTls>>,
    /// IngressURLFormat is the URL for the ingress.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressURLFormat")]
    pub ingress_url_format: Option<String>,
    /// ServiceAnnotations is a map of key,value pairs of annotations that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels is a map of key,value pairs of labels that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ServicePort allows configuring the port at service level that might be different from the targetPort.
    #[serde(rename = "servicePort")]
    pub service_port: i32,
    /// ServicePortName allows configuring the name of the service port.
    /// This may be useful for sidecar proxies like Envoy injected by Istio which require specific ports names to treat traffic as proper HTTP.
    #[serde(rename = "servicePortName")]
    pub service_port_name: String,
    /// ServiceType allows configuring the type of the service. Defaults to ClusterIP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceType")]
    pub service_type: Option<String>,
}

/// IngressTLS describes the transport layer security associated with an ingress.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDriverIngressOptionsIngressTls {
    /// hosts is a list of hosts included in the TLS certificate. The values in
    /// this list must match the name/s used in the tlsSecret. Defaults to the
    /// wildcard host setting for the loadbalancer controller fulfilling this
    /// Ingress, if left unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hosts: Option<Vec<String>>,
    /// secretName is the name of the secret used to terminate TLS traffic on
    /// port 443. Field is left optional to allow TLS routing based on SNI
    /// hostname alone. If the SNI host in a listener conflicts with the "Host"
    /// header field used by an IngressRule, the SNI host is used for termination
    /// and value of the "Host" header is used for routing.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// DynamicAllocation configures dynamic allocation that becomes available for the Kubernetes
/// scheduler backend since Spark 3.0.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationDynamicAllocation {
    /// Enabled controls whether dynamic allocation is enabled or not.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enabled: Option<bool>,
    /// InitialExecutors is the initial number of executors to request. If .spec.executor.instances
    /// is also set, the initial number of executors is set to the bigger of that and this option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialExecutors")]
    pub initial_executors: Option<i32>,
    /// MaxExecutors is the upper bound for the number of executors if dynamic allocation is enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxExecutors")]
    pub max_executors: Option<i32>,
    /// MinExecutors is the lower bound for the number of executors if dynamic allocation is enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minExecutors")]
    pub min_executors: Option<i32>,
    /// ShuffleTrackingTimeout controls the timeout in milliseconds for executors that are holding
    /// shuffle data if shuffle tracking is enabled (true by default if dynamic allocation is enabled).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shuffleTrackingTimeout")]
    pub shuffle_tracking_timeout: Option<i64>,
}

/// Executor is the executor specification.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutor {
    /// Affinity specifies the affinity/anti-affinity settings for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub affinity: Option<SparkApplicationExecutorAffinity>,
    /// Annotations are the Kubernetes annotations to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<BTreeMap<String, String>>,
    /// ConfigMaps carries information of other ConfigMaps to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMaps")]
    pub config_maps: Option<Vec<SparkApplicationExecutorConfigMaps>>,
    /// CoreLimit specifies a hard limit on CPU cores for the pod.
    /// Optional
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreLimit")]
    pub core_limit: Option<String>,
    /// CoreRequest is the physical CPU core request for the executors.
    /// Maps to `spark.kubernetes.executor.request.cores` that is available since Spark 2.4.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "coreRequest")]
    pub core_request: Option<String>,
    /// Cores maps to `spark.driver.cores` or `spark.executor.cores` for the driver and executors, respectively.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cores: Option<i32>,
    /// DeleteOnTermination specify whether executor pods should be deleted in case of failure or normal termination.
    /// Maps to `spark.kubernetes.executor.deleteOnTermination` that is available since Spark 3.0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deleteOnTermination")]
    pub delete_on_termination: Option<bool>,
    /// DnsConfig dns settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dnsConfig")]
    pub dns_config: Option<SparkApplicationExecutorDnsConfig>,
    /// Env carries the environment variables to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationExecutorEnv>>,
    /// EnvFrom is a list of sources to populate environment variables in the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationExecutorEnvFrom>>,
    /// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envSecretKeyRefs")]
    pub env_secret_key_refs: Option<BTreeMap<String, SparkApplicationExecutorEnvSecretKeyRefs>>,
    /// EnvVars carries the environment variables to add to the pod.
    /// Deprecated. Consider using `env` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envVars")]
    pub env_vars: Option<BTreeMap<String, String>>,
    /// GPU specifies GPU requirement for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu: Option<SparkApplicationExecutorGpu>,
    /// HostAliases settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostAliases")]
    pub host_aliases: Option<Vec<SparkApplicationExecutorHostAliases>>,
    /// HostNetwork indicates whether to request host networking for the pod or not.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostNetwork")]
    pub host_network: Option<bool>,
    /// Image is the container image to use. Overrides Spec.Image if set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// InitContainers is a list of init-containers that run to completion before the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initContainers")]
    pub init_containers: Option<Vec<SparkApplicationExecutorInitContainers>>,
    /// Instances is the number of executor instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub instances: Option<i32>,
    /// JavaOptions is a string of extra JVM options to pass to the executors. For instance,
    /// GC settings or other logging.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "javaOptions")]
    pub java_options: Option<String>,
    /// Labels are the Kubernetes labels to be added to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<BTreeMap<String, String>>,
    /// Lifecycle for running preStop or postStart commands
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationExecutorLifecycle>,
    /// Memory is the amount of memory to request for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// MemoryOverhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryOverhead")]
    pub memory_overhead: Option<String>,
    /// NodeSelector is the Kubernetes node selector to be added to the driver and executor pods.
    /// This field is mutually exclusive with nodeSelector at SparkApplication level (which will be deprecated).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeSelector")]
    pub node_selector: Option<BTreeMap<String, String>>,
    /// PodSecurityContext specifies the PodSecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podSecurityContext")]
    pub pod_security_context: Option<SparkApplicationExecutorPodSecurityContext>,
    /// Ports settings for the pods, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationExecutorPorts>>,
    /// PriorityClassName is the name of the PriorityClass for the executor pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "priorityClassName")]
    pub priority_class_name: Option<String>,
    /// SchedulerName specifies the scheduler that will be used for scheduling
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "schedulerName")]
    pub scheduler_name: Option<String>,
    /// Secrets carries information of secrets to add to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secrets: Option<Vec<SparkApplicationExecutorSecrets>>,
    /// SecurityContext specifies the container's SecurityContext to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationExecutorSecurityContext>,
    /// ServiceAccount is the name of the custom Kubernetes service account used by the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// ShareProcessNamespace settings for the pod, following the Kubernetes specifications.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shareProcessNamespace")]
    pub share_process_namespace: Option<bool>,
    /// Sidecars is a list of sidecar containers that run along side the main Spark container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sidecars: Option<Vec<SparkApplicationExecutorSidecars>>,
    /// Template is a pod template that can be used to define the driver or executor pod configurations that Spark configurations do not support.
    /// Spark version >= 3.0.0 is required.
    /// Ref: https://spark.apache.org/docs/latest/running-on-kubernetes.html#pod-template.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub template: Option<BTreeMap<String, serde_json::Value>>,
    /// Termination grace period seconds for the pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Tolerations specifies the tolerations listed in ".spec.tolerations" to be applied to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tolerations: Option<Vec<SparkApplicationExecutorTolerations>>,
    /// VolumeMounts specifies the volumes listed in ".spec.volumes" to mount into the main container's filesystem.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationExecutorVolumeMounts>>,
}

/// Affinity specifies the affinity/anti-affinity settings for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinity {
    /// Describes node affinity scheduling rules for the pod.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeAffinity")]
    pub node_affinity: Option<SparkApplicationExecutorAffinityNodeAffinity>,
    /// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAffinity")]
    pub pod_affinity: Option<SparkApplicationExecutorAffinityPodAffinity>,
    /// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podAntiAffinity")]
    pub pod_anti_affinity: Option<SparkApplicationExecutorAffinityPodAntiAffinity>,
}

/// Describes node affinity scheduling rules for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node matches the corresponding matchExpressions; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to an update), the system
    /// may or may not try to eventually evict the pod from its node.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution>,
}

/// An empty preferred scheduling term matches all objects with implicit weight 0
/// (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// A node selector term, associated with the corresponding weight.
    pub preference: SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference,
    /// Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.
    pub weight: i32,
}

/// A node selector term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreference {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityPreferredDuringSchedulingIgnoredDuringExecutionPreferenceMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// If the affinity requirements specified by this field are not met at
/// scheduling time, the pod will not be scheduled onto the node.
/// If the affinity requirements specified by this field cease to be met
/// at some point during pod execution (e.g. due to an update), the system
/// may or may not try to eventually evict the pod from its node.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// Required. A list of node selector terms. The terms are ORed.
    #[serde(rename = "nodeSelectorTerms")]
    pub node_selector_terms: Vec<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms>,
}

/// A null or empty node selector term matches no objects. The requirements of
/// them are ANDed.
/// The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTerms {
    /// A list of node selector requirements by node's labels.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions>>,
    /// A list of node selector requirements by node's fields.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchFields")]
    pub match_fields: Option<Vec<SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchExpressions {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A node selector requirement is a selector that contains values, a key, and an operator
/// that relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityNodeAffinityRequiredDuringSchedulingIgnoredDuringExecutionNodeSelectorTermsMatchFields {
    /// The label key that the selector applies to.
    pub key: String,
    /// Represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.
    pub operator: String,
    /// An array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. If the operator is Gt or Lt, the values
    /// array must have a single element, which will be interpreted as an integer.
    /// This array is replaced during a strategic merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinity {
    /// The scheduler will prefer to schedule pods to nodes that satisfy
    /// the anti-affinity expressions specified by this field, but it may choose
    /// a node that violates one or more of the expressions. The node that is
    /// most preferred is the one with the greatest sum of weights, i.e.
    /// for each node that meets all of the scheduling requirements (resource
    /// request, requiredDuringScheduling anti-affinity expressions, etc.),
    /// compute a sum by iterating through the elements of this field and adding
    /// "weight" to the sum if the node has pods which matches the corresponding podAffinityTerm; the
    /// node(s) with the highest sum are the most preferred.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preferredDuringSchedulingIgnoredDuringExecution")]
    pub preferred_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution>>,
    /// If the anti-affinity requirements specified by this field are not met at
    /// scheduling time, the pod will not be scheduled onto the node.
    /// If the anti-affinity requirements specified by this field cease to be met
    /// at some point during pod execution (e.g. due to a pod label update), the
    /// system may or may not try to eventually evict the pod from its node.
    /// When there are multiple elements, the lists of nodes corresponding to each
    /// podAffinityTerm are intersected, i.e. all terms must be satisfied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "requiredDuringSchedulingIgnoredDuringExecution")]
    pub required_during_scheduling_ignored_during_execution: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution>>,
}

/// The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecution {
    /// Required. A pod affinity term, associated with the corresponding weight.
    #[serde(rename = "podAffinityTerm")]
    pub pod_affinity_term: SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm,
    /// weight associated with matching the corresponding podAffinityTerm,
    /// in the range 1-100.
    pub weight: i32,
}

/// Required. A pod affinity term, associated with the corresponding weight.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTerm {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityPreferredDuringSchedulingIgnoredDuringExecutionPodAffinityTermNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Defines a set of pods (namely those matching the labelSelector
/// relative to the given namespace(s)) that this pod should be
/// co-located (affinity) or not co-located (anti-affinity) with,
/// where co-located is defined as running on a node whose value of
/// the label with key <topologyKey> matches that of any node on which
/// a pod of the set of pods is running
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecution {
    /// A label query over a set of resources, in this case pods.
    /// If it's null, this PodAffinityTerm matches with no Pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector>,
    /// MatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both matchLabelKeys and labelSelector.
    /// Also, matchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabelKeys")]
    pub match_label_keys: Option<Vec<String>>,
    /// MismatchLabelKeys is a set of pod label keys to select which pods will
    /// be taken into consideration. The keys are used to lookup values from the
    /// incoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`
    /// to select the group of existing pods which pods will be taken into consideration
    /// for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming
    /// pod labels will be ignored. The default value is empty.
    /// The same key is forbidden to exist in both mismatchLabelKeys and labelSelector.
    /// Also, mismatchLabelKeys cannot be set when labelSelector isn't set.
    /// This is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mismatchLabelKeys")]
    pub mismatch_label_keys: Option<Vec<String>>,
    /// A label query over the set of namespaces that the term applies to.
    /// The term is applied to the union of the namespaces selected by this field
    /// and the ones listed in the namespaces field.
    /// null selector and null or empty namespaces list means "this pod's namespace".
    /// An empty selector ({}) matches all namespaces.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "namespaceSelector")]
    pub namespace_selector: Option<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector>,
    /// namespaces specifies a static list of namespace names that the term applies to.
    /// The term is applied to the union of the namespaces listed in this field
    /// and the ones selected by namespaceSelector.
    /// null or empty namespaces list and null namespaceSelector means "this pod's namespace".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespaces: Option<Vec<String>>,
    /// This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching
    /// the labelSelector in the specified namespaces, where co-located is defined as running on a node
    /// whose value of the label with key topologyKey matches that of any node on which any of the
    /// selected pods is running.
    /// Empty topologyKey is not allowed.
    #[serde(rename = "topologyKey")]
    pub topology_key: String,
}

/// A label query over a set of resources, in this case pods.
/// If it's null, this PodAffinityTerm matches with no Pods.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// A label query over the set of namespaces that the term applies to.
/// The term is applied to the union of the namespaces selected by this field
/// and the ones listed in the namespaces field.
/// null selector and null or empty namespaces list means "this pod's namespace".
/// An empty selector ({}) matches all namespaces.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorAffinityPodAntiAffinityRequiredDuringSchedulingIgnoredDuringExecutionNamespaceSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// NamePath is a pair of a name and a path to which the named objects should be mounted to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorConfigMaps {
    pub name: String,
    pub path: String,
}

/// DnsConfig dns settings for the pod, following the Kubernetes specifications.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorDnsConfig {
    /// A list of DNS name server IP addresses.
    /// This will be appended to the base nameservers generated from DNSPolicy.
    /// Duplicated nameservers will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nameservers: Option<Vec<String>>,
    /// A list of DNS resolver options.
    /// This will be merged with the base options generated from DNSPolicy.
    /// Duplicated entries will be removed. Resolution options given in Options
    /// will override those that appear in the base DNSPolicy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<Vec<SparkApplicationExecutorDnsConfigOptions>>,
    /// A list of DNS search domains for host-name lookup.
    /// This will be appended to the base search paths generated from DNSPolicy.
    /// Duplicated search paths will be removed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub searches: Option<Vec<String>>,
}

/// PodDNSConfigOption defines DNS resolver options of a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorDnsConfigOptions {
    /// Name is this DNS resolver option's name.
    /// Required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Value is this DNS resolver option's value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationExecutorEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationExecutorEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationExecutorEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationExecutorEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationExecutorEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationExecutorEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationExecutorEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvFromConfigMapRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvFromSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvSecretKeyRefs holds a mapping from environment variable names to SecretKeyRefs.
/// Deprecated. Consider using `env` instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorEnvSecretKeyRefs {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// GPU specifies GPU requirement for the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorGpu {
    /// Name is GPU resource name, such as: nvidia.com/gpu or amd.com/gpu
    pub name: String,
    /// Quantity is the number of GPUs to request for driver or executor.
    pub quantity: i64,
}

/// HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the
/// pod's hosts file.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorHostAliases {
    /// Hostnames for the above IP address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hostnames: Option<Vec<String>>,
    /// IP address of the host file entry.
    pub ip: String,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainers {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationExecutorInitContainersEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationExecutorInitContainersEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationExecutorInitContainersLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationExecutorInitContainersLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationExecutorInitContainersPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationExecutorInitContainersReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationExecutorInitContainersResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationExecutorInitContainersResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationExecutorInitContainersSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationExecutorInitContainersStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationExecutorInitContainersVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationExecutorInitContainersVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationExecutorInitContainersEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationExecutorInitContainersEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationExecutorInitContainersEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationExecutorInitContainersEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvFromConfigMapRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersEnvFromSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationExecutorInitContainersLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationExecutorInitContainersLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStart {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersLifecyclePostStartExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersLifecyclePostStartHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorInitContainersLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersLifecyclePostStartTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStop {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersLifecyclePreStopExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersLifecyclePreStopHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorInitContainersLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersLifecyclePreStopTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorInitContainersLivenessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorInitContainersReadinessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationExecutorInitContainersResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
    /// Request is the name chosen for a request in the referenced claim.
    /// If empty, everything from the claim is made available, otherwise
    /// only the result of this request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub request: Option<String>,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// appArmorProfile is the AppArmor options to use by this container. If set, this profile
    /// overrides the pod's appArmorProfile.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationExecutorInitContainersSecurityContextAppArmorProfile>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationExecutorInitContainersSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default value is Default which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorInitContainersSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorInitContainersSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorInitContainersSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by this container. If set, this profile
/// overrides the pod's appArmorProfile.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorInitContainersStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorInitContainersStartupProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorInitContainersStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorInitContainersStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorInitContainersStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorInitContainersVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    /// When RecursiveReadOnly is set to IfPossible or to Enabled, MountPropagation must be None or unspecified
    /// (which defaults to None).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// RecursiveReadOnly specifies whether read-only mounts should be handled
    /// recursively.
    /// 
    /// If ReadOnly is false, this field has no meaning and must be unspecified.
    /// 
    /// If ReadOnly is true, and this field is set to Disabled, the mount is not made
    /// recursively read-only.  If this field is set to IfPossible, the mount is made
    /// recursively read-only, if it is supported by the container runtime.  If this
    /// field is set to Enabled, the mount is made recursively read-only if it is
    /// supported by the container runtime, otherwise the pod will not be started and
    /// an error will be generated to indicate the reason.
    /// 
    /// If this field is set to IfPossible or Enabled, MountPropagation must be set to
    /// None (or be unspecified, which defaults to None).
    /// 
    /// If this field is not specified, it is treated as an equivalent of Disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recursiveReadOnly")]
    pub recursive_read_only: Option<String>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// Lifecycle for running preStop or postStart commands
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationExecutorLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationExecutorLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStart {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorLifecyclePostStartExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorLifecyclePostStartHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorLifecyclePostStartTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStop {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorLifecyclePreStopExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorLifecyclePreStopHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorLifecyclePreStopTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PodSecurityContext specifies the PodSecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContext {
    /// appArmorProfile is the AppArmor options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationExecutorPodSecurityContextAppArmorProfile>,
    /// A special supplemental group that applies to all containers in a pod.
    /// Some volume types allow the Kubelet to change the ownership of that volume
    /// to be owned by the pod:
    /// 
    /// 1. The owning GID will be the FSGroup
    /// 2. The setgid bit is set (new files created in the volume will be owned by FSGroup)
    /// 3. The permission bits are OR'd with rw-rw----
    /// 
    /// If unset, the Kubelet will not modify the ownership and permissions of any volume.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroup")]
    pub fs_group: Option<i64>,
    /// fsGroupChangePolicy defines behavior of changing ownership and permission of the volume
    /// before being exposed inside Pod. This field will only apply to
    /// volume types which support fsGroup based ownership(and permissions).
    /// It will have no effect on ephemeral volume types such as: secret, configmaps
    /// and emptydir.
    /// Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsGroupChangePolicy")]
    pub fs_group_change_policy: Option<String>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in SecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence
    /// for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// seLinuxChangePolicy defines how the container's SELinux label is applied to all volumes used by the Pod.
    /// It has no effect on nodes that do not support SELinux or to volumes does not support SELinux.
    /// Valid values are "MountOption" and "Recursive".
    /// 
    /// "Recursive" means relabeling of all files on all Pod volumes by the container runtime.
    /// This may be slow for large volumes, but allows mixing privileged and unprivileged Pods sharing the same volume on the same node.
    /// 
    /// "MountOption" mounts all eligible Pod volumes with `-o context` mount option.
    /// This requires all Pods that share the same volume to use the same SELinux label.
    /// It is not possible to share the same volume among privileged and unprivileged Pods.
    /// Eligible volumes are in-tree FibreChannel and iSCSI volumes, and all CSI volumes
    /// whose CSI driver announces SELinux support by setting spec.seLinuxMount: true in their
    /// CSIDriver instance. Other volumes are always re-labelled recursively.
    /// "MountOption" value is allowed only when SELinuxMount feature gate is enabled.
    /// 
    /// If not specified and SELinuxMount feature gate is enabled, "MountOption" is used.
    /// If not specified and SELinuxMount feature gate is disabled, "MountOption" is used for ReadWriteOncePod volumes
    /// and "Recursive" for all other volumes.
    /// 
    /// This field affects only Pods that have SELinux label set, either in PodSecurityContext or in SecurityContext of all containers.
    /// 
    /// All Pods that use the same volume should use the same seLinuxChangePolicy, otherwise some pods can get stuck in ContainerCreating state.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxChangePolicy")]
    pub se_linux_change_policy: Option<String>,
    /// The SELinux context to be applied to all containers.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in SecurityContext.  If set in
    /// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
    /// takes precedence for that container.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorPodSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by the containers in this pod.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorPodSecurityContextSeccompProfile>,
    /// A list of groups applied to the first process run in each container, in
    /// addition to the container's primary GID and fsGroup (if specified).  If
    /// the SupplementalGroupsPolicy feature is enabled, the
    /// supplementalGroupsPolicy field determines whether these are in addition
    /// to or instead of any group memberships defined in the container image.
    /// If unspecified, no additional groups are added, though group memberships
    /// defined in the container image may still be used, depending on the
    /// supplementalGroupsPolicy field.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroups")]
    pub supplemental_groups: Option<Vec<i64>>,
    /// Defines how supplemental groups of the first container processes are calculated.
    /// Valid values are "Merge" and "Strict". If not specified, "Merge" is used.
    /// (Alpha) Using the field requires the SupplementalGroupsPolicy feature gate to be enabled
    /// and the container runtime must implement support for this feature.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "supplementalGroupsPolicy")]
    pub supplemental_groups_policy: Option<String>,
    /// Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported
    /// sysctls (by the container runtime) might fail to launch.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sysctls: Option<Vec<SparkApplicationExecutorPodSecurityContextSysctls>>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options within a container's SecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorPodSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The SELinux context to be applied to all containers.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in SecurityContext.  If set in
/// both SecurityContext and PodSecurityContext, the value specified in SecurityContext
/// takes precedence for that container.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by the containers in this pod.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// Sysctl defines a kernel parameter to be set
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextSysctls {
    /// Name of a property to set
    pub name: String,
    /// Value of a property to set
    pub value: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options within a container's SecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPodSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// Port represents the port definition in the pods objects.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorPorts {
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    pub name: String,
    pub protocol: String,
}

/// SecretInfo captures information of a secret.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecrets {
    pub name: String,
    pub path: String,
    /// SecretType tells the type of a secret.
    #[serde(rename = "secretType")]
    pub secret_type: String,
}

/// SecurityContext specifies the container's SecurityContext to apply.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// appArmorProfile is the AppArmor options to use by this container. If set, this profile
    /// overrides the pod's appArmorProfile.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationExecutorSecurityContextAppArmorProfile>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationExecutorSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default value is Default which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by this container. If set, this profile
/// overrides the pod's appArmorProfile.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// A single application container that you want to run within a pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecars {
    /// Arguments to the entrypoint.
    /// The container image's CMD is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// Entrypoint array. Not executed within a shell.
    /// The container image's ENTRYPOINT is used if this is not provided.
    /// Variable references $(VAR_NAME) are expanded using the container's environment. If a variable
    /// cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will
    /// produce the string literal "$(VAR_NAME)". Escaped references will never be expanded, regardless
    /// of whether the variable exists or not. Cannot be updated.
    /// More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// List of environment variables to set in the container.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<Vec<SparkApplicationExecutorSidecarsEnv>>,
    /// List of sources to populate environment variables in the container.
    /// The keys defined within a source must be a C_IDENTIFIER. All invalid keys
    /// will be reported as an event when the container is starting. When a key exists in multiple
    /// sources, the value associated with the last source will take precedence.
    /// Values defined by an Env with a duplicate key will take precedence.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "envFrom")]
    pub env_from: Option<Vec<SparkApplicationExecutorSidecarsEnvFrom>>,
    /// Container image name.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Image pull policy.
    /// One of Always, Never, IfNotPresent.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<String>,
    /// Actions that the management system should take in response to container lifecycle events.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lifecycle: Option<SparkApplicationExecutorSidecarsLifecycle>,
    /// Periodic probe of container liveness.
    /// Container will be restarted if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "livenessProbe")]
    pub liveness_probe: Option<SparkApplicationExecutorSidecarsLivenessProbe>,
    /// Name of the container specified as a DNS_LABEL.
    /// Each container in a pod must have a unique name (DNS_LABEL).
    /// Cannot be updated.
    pub name: String,
    /// List of ports to expose from the container. Not specifying a port here
    /// DOES NOT prevent that port from being exposed. Any port which is
    /// listening on the default "0.0.0.0" address inside a container will be
    /// accessible from the network.
    /// Modifying this array with strategic merge patch may corrupt the data.
    /// For more information See https://github.com/kubernetes/kubernetes/issues/108255.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<Vec<SparkApplicationExecutorSidecarsPorts>>,
    /// Periodic probe of container service readiness.
    /// Container will be removed from service endpoints if the probe fails.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readinessProbe")]
    pub readiness_probe: Option<SparkApplicationExecutorSidecarsReadinessProbe>,
    /// Resources resize policy for the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resizePolicy")]
    pub resize_policy: Option<Vec<SparkApplicationExecutorSidecarsResizePolicy>>,
    /// Compute Resources required by this container.
    /// Cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationExecutorSidecarsResources>,
    /// RestartPolicy defines the restart behavior of individual containers in a pod.
    /// This field may only be set for init containers, and the only allowed value is "Always".
    /// For non-init containers or when this field is not specified,
    /// the restart behavior is defined by the Pod's restart policy and the container type.
    /// Setting the RestartPolicy as "Always" for the init container will have the following effect:
    /// this init container will be continually restarted on
    /// exit until all regular containers have terminated. Once all regular
    /// containers have completed, all init containers with restartPolicy "Always"
    /// will be shut down. This lifecycle differs from normal init containers and
    /// is often referred to as a "sidecar" container. Although this init
    /// container still starts in the init container sequence, it does not wait
    /// for the container to complete before proceeding to the next init
    /// container. Instead, the next init container starts immediately after this
    /// init container is started, or after any startupProbe has successfully
    /// completed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "restartPolicy")]
    pub restart_policy: Option<String>,
    /// SecurityContext defines the security options the container should be run with.
    /// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
    /// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityContext")]
    pub security_context: Option<SparkApplicationExecutorSidecarsSecurityContext>,
    /// StartupProbe indicates that the Pod has successfully initialized.
    /// If specified, no other probes are executed until this completes successfully.
    /// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
    /// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
    /// when it might take a long time to load data or warm a cache, than during steady-state operation.
    /// This cannot be updated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startupProbe")]
    pub startup_probe: Option<SparkApplicationExecutorSidecarsStartupProbe>,
    /// Whether this container should allocate a buffer for stdin in the container runtime. If this
    /// is not set, reads from stdin in the container will always result in EOF.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub stdin: Option<bool>,
    /// Whether the container runtime should close the stdin channel after it has been opened by
    /// a single attach. When stdin is true the stdin stream will remain open across multiple attach
    /// sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
    /// first client attaches to stdin, and then remains open and accepts data until the client disconnects,
    /// at which time stdin is closed and remains closed until the container is restarted. If this
    /// flag is false, a container processes that reads from stdin will never receive an EOF.
    /// Default is false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stdinOnce")]
    pub stdin_once: Option<bool>,
    /// Optional: Path at which the file to which the container's termination message
    /// will be written is mounted into the container's filesystem.
    /// Message written is intended to be brief final status, such as an assertion failure message.
    /// Will be truncated by the node if greater than 4096 bytes. The total message length across
    /// all containers will be limited to 12kb.
    /// Defaults to /dev/termination-log.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePath")]
    pub termination_message_path: Option<String>,
    /// Indicate how the termination message should be populated. File will use the contents of
    /// terminationMessagePath to populate the container status message on both success and failure.
    /// FallbackToLogsOnError will use the last chunk of container log output if the termination
    /// message file is empty and the container exited with an error.
    /// The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
    /// Defaults to File.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationMessagePolicy")]
    pub termination_message_policy: Option<String>,
    /// Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.
    /// Default is false.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tty: Option<bool>,
    /// volumeDevices is the list of block devices to be used by the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeDevices")]
    pub volume_devices: Option<Vec<SparkApplicationExecutorSidecarsVolumeDevices>>,
    /// Pod volumes to mount into the container's filesystem.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMounts")]
    pub volume_mounts: Option<Vec<SparkApplicationExecutorSidecarsVolumeMounts>>,
    /// Container's working directory.
    /// If not specified, the container runtime's default will be used, which
    /// might be configured in the container image.
    /// Cannot be updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workingDir")]
    pub working_dir: Option<String>,
}

/// EnvVar represents an environment variable present in a Container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnv {
    /// Name of the environment variable. Must be a C_IDENTIFIER.
    pub name: String,
    /// Variable references $(VAR_NAME) are expanded
    /// using the previously defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. Double $$ are reduced
    /// to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.
    /// "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".
    /// Escaped references will never be expanded, regardless of whether the variable
    /// exists or not.
    /// Defaults to "".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
    /// Source for the environment variable's value. Cannot be used if value is not empty.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "valueFrom")]
    pub value_from: Option<SparkApplicationExecutorSidecarsEnvValueFrom>,
}

/// Source for the environment variable's value. Cannot be used if value is not empty.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFrom {
    /// Selects a key of a ConfigMap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapKeyRef")]
    pub config_map_key_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromConfigMapKeyRef>,
    /// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
    /// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromFieldRef>,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromResourceFieldRef>,
    /// Selects a key of a secret in the pod's namespace
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretKeyRef")]
    pub secret_key_ref: Option<SparkApplicationExecutorSidecarsEnvValueFromSecretKeyRef>,
}

/// Selects a key of a ConfigMap.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromConfigMapKeyRef {
    /// The key to select.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`, `metadata.annotations['<KEY>']`,
/// spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// Selects a key of a secret in the pod's namespace
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvValueFromSecretKeyRef {
    /// The key of the secret to select from.  Must be a valid secret key.
    pub key: String,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// EnvFromSource represents the source of a set of ConfigMaps
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvFrom {
    /// The ConfigMap to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMapRef")]
    pub config_map_ref: Option<SparkApplicationExecutorSidecarsEnvFromConfigMapRef>,
    /// An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
    /// The Secret to select from
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationExecutorSidecarsEnvFromSecretRef>,
}

/// The ConfigMap to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvFromConfigMapRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the ConfigMap must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// The Secret to select from
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsEnvFromSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Specify whether the Secret must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Actions that the management system should take in response to container lifecycle events.
/// Cannot be updated.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecycle {
    /// PostStart is called immediately after a container is created. If the handler fails,
    /// the container is terminated and restarted according to its restart policy.
    /// Other management of the container blocks until the hook completes.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "postStart")]
    pub post_start: Option<SparkApplicationExecutorSidecarsLifecyclePostStart>,
    /// PreStop is called immediately before a container is terminated due to an
    /// API request or management event such as liveness/startup probe failure,
    /// preemption, resource contention, etc. The handler is not called if the
    /// container crashes or exits. The Pod's termination grace period countdown begins before the
    /// PreStop hook is executed. Regardless of the outcome of the handler, the
    /// container will eventually terminate within the Pod's termination grace
    /// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
    /// or until the termination grace period is reached.
    /// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preStop")]
    pub pre_stop: Option<SparkApplicationExecutorSidecarsLifecyclePreStop>,
}

/// PostStart is called immediately after a container is created. If the handler fails,
/// the container is terminated and restarted according to its restart policy.
/// Other management of the container blocks until the hook completes.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStart {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsLifecyclePostStartExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsLifecyclePostStartHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorSidecarsLifecyclePostStartSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsLifecyclePostStartTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsLifecyclePostStartHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePostStartTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// PreStop is called immediately before a container is terminated due to an
/// API request or management event such as liveness/startup probe failure,
/// preemption, resource contention, etc. The handler is not called if the
/// container crashes or exits. The Pod's termination grace period countdown begins before the
/// PreStop hook is executed. Regardless of the outcome of the handler, the
/// container will eventually terminate within the Pod's termination grace
/// period (unless delayed by finalizers). Other management of the container blocks until the hook completes
/// or until the termination grace period is reached.
/// More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStop {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsLifecyclePreStopExec>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsLifecyclePreStopHttpGet>,
    /// Sleep represents a duration that the container should sleep.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sleep: Option<SparkApplicationExecutorSidecarsLifecyclePreStopSleep>,
    /// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
    /// for backward compatibility. There is no validation of this field and
    /// lifecycle hooks will fail at runtime when it is specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsLifecyclePreStopTcpSocket>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsLifecyclePreStopHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// Sleep represents a duration that the container should sleep.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopSleep {
    /// Seconds is the number of seconds to sleep.
    pub seconds: i64,
}

/// Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept
/// for backward compatibility. There is no validation of this field and
/// lifecycle hooks will fail at runtime when it is specified.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLifecyclePreStopTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// Periodic probe of container liveness.
/// Container will be restarted if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsLivenessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorSidecarsLivenessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsLivenessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsLivenessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsLivenessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsLivenessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerPort represents a network port in a single container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsPorts {
    /// Number of port to expose on the pod's IP address.
    /// This must be a valid port number, 0 < x < 65536.
    #[serde(rename = "containerPort")]
    pub container_port: i32,
    /// What host IP to bind the external port to.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostIP")]
    pub host_ip: Option<String>,
    /// Number of port to expose on the host.
    /// If specified, this must be a valid port number, 0 < x < 65536.
    /// If HostNetwork is specified, this must match ContainerPort.
    /// Most containers do not need this.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPort")]
    pub host_port: Option<i32>,
    /// If specified, this must be an IANA_SVC_NAME and unique within the pod. Each
    /// named port in a pod must have a unique name. Name for the port that can be
    /// referred to by services.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Protocol for port. Must be UDP, TCP, or SCTP.
    /// Defaults to "TCP".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<String>,
}

/// Periodic probe of container service readiness.
/// Container will be removed from service endpoints if the probe fails.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsReadinessProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorSidecarsReadinessProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsReadinessProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsReadinessProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsReadinessProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsReadinessProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// ContainerResizePolicy represents resource resize policy for the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsResizePolicy {
    /// Name of the resource to which this resource resize policy applies.
    /// Supported values: cpu, memory.
    #[serde(rename = "resourceName")]
    pub resource_name: String,
    /// Restart policy to apply when specified resource is resized.
    /// If not specified, it defaults to NotRequired.
    #[serde(rename = "restartPolicy")]
    pub restart_policy: String,
}

/// Compute Resources required by this container.
/// Cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<SparkApplicationExecutorSidecarsResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
    /// Request is the name chosen for a request in the referenced claim.
    /// If empty, everything from the claim is made available, otherwise
    /// only the result of this request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub request: Option<String>,
}

/// SecurityContext defines the security options the container should be run with.
/// If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
/// More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContext {
    /// AllowPrivilegeEscalation controls whether a process can gain more
    /// privileges than its parent process. This bool directly controls if
    /// the no_new_privs flag will be set on the container process.
    /// AllowPrivilegeEscalation is true always when the container is:
    /// 1) run as Privileged
    /// 2) has CAP_SYS_ADMIN
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "allowPrivilegeEscalation")]
    pub allow_privilege_escalation: Option<bool>,
    /// appArmorProfile is the AppArmor options to use by this container. If set, this profile
    /// overrides the pod's appArmorProfile.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "appArmorProfile")]
    pub app_armor_profile: Option<SparkApplicationExecutorSidecarsSecurityContextAppArmorProfile>,
    /// The capabilities to add/drop when running containers.
    /// Defaults to the default set of capabilities granted by the container runtime.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub capabilities: Option<SparkApplicationExecutorSidecarsSecurityContextCapabilities>,
    /// Run container in privileged mode.
    /// Processes in privileged containers are essentially equivalent to root on the host.
    /// Defaults to false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// procMount denotes the type of proc mount to use for the containers.
    /// The default value is Default which uses the container runtime defaults for
    /// readonly paths and masked paths.
    /// This requires the ProcMountType feature flag to be enabled.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "procMount")]
    pub proc_mount: Option<String>,
    /// Whether this container has a read-only root filesystem.
    /// Default is false.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnlyRootFilesystem")]
    pub read_only_root_filesystem: Option<bool>,
    /// The GID to run the entrypoint of the container process.
    /// Uses runtime default if unset.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsGroup")]
    pub run_as_group: Option<i64>,
    /// Indicates that the container must run as a non-root user.
    /// If true, the Kubelet will validate the image at runtime to ensure that it
    /// does not run as UID 0 (root) and fail to start the container if it does.
    /// If unset or false, no such validation will be performed.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsNonRoot")]
    pub run_as_non_root: Option<bool>,
    /// The UID to run the entrypoint of the container process.
    /// Defaults to user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUser")]
    pub run_as_user: Option<i64>,
    /// The SELinux context to be applied to the container.
    /// If unspecified, the container runtime will allocate a random SELinux context for each
    /// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seLinuxOptions")]
    pub se_linux_options: Option<SparkApplicationExecutorSidecarsSecurityContextSeLinuxOptions>,
    /// The seccomp options to use by this container. If seccomp options are
    /// provided at both the pod & container level, the container options
    /// override the pod options.
    /// Note that this field cannot be set when spec.os.name is windows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "seccompProfile")]
    pub seccomp_profile: Option<SparkApplicationExecutorSidecarsSecurityContextSeccompProfile>,
    /// The Windows specific settings applied to all containers.
    /// If unspecified, the options from the PodSecurityContext will be used.
    /// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
    /// Note that this field cannot be set when spec.os.name is linux.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "windowsOptions")]
    pub windows_options: Option<SparkApplicationExecutorSidecarsSecurityContextWindowsOptions>,
}

/// appArmorProfile is the AppArmor options to use by this container. If set, this profile
/// overrides the pod's appArmorProfile.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextAppArmorProfile {
    /// localhostProfile indicates a profile loaded on the node that should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must match the loaded name of the profile.
    /// Must be set if and only if type is "Localhost".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of AppArmor profile will be applied.
    /// Valid options are:
    ///   Localhost - a profile pre-loaded on the node.
    ///   RuntimeDefault - the container runtime's default profile.
    ///   Unconfined - no AppArmor enforcement.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The capabilities to add/drop when running containers.
/// Defaults to the default set of capabilities granted by the container runtime.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextCapabilities {
    /// Added capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub add: Option<Vec<String>>,
    /// Removed capabilities
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop: Option<Vec<String>>,
}

/// The SELinux context to be applied to the container.
/// If unspecified, the container runtime will allocate a random SELinux context for each
/// container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
/// PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextSeLinuxOptions {
    /// Level is SELinux level label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Role is a SELinux role label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,
    /// Type is a SELinux type label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// User is a SELinux user label that applies to the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// The seccomp options to use by this container. If seccomp options are
/// provided at both the pod & container level, the container options
/// override the pod options.
/// Note that this field cannot be set when spec.os.name is windows.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextSeccompProfile {
    /// localhostProfile indicates a profile defined in a file on the node should be used.
    /// The profile must be preconfigured on the node to work.
    /// Must be a descending path, relative to the kubelet's configured seccomp profile location.
    /// Must be set if type is "Localhost". Must NOT be set for any other type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localhostProfile")]
    pub localhost_profile: Option<String>,
    /// type indicates which kind of seccomp profile will be applied.
    /// Valid options are:
    /// 
    /// Localhost - a profile defined in a file on the node should be used.
    /// RuntimeDefault - the container runtime default profile should be used.
    /// Unconfined - no profile should be applied.
    #[serde(rename = "type")]
    pub r#type: String,
}

/// The Windows specific settings applied to all containers.
/// If unspecified, the options from the PodSecurityContext will be used.
/// If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.
/// Note that this field cannot be set when spec.os.name is linux.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsSecurityContextWindowsOptions {
    /// GMSACredentialSpec is where the GMSA admission webhook
    /// (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the
    /// GMSA credential spec named by the GMSACredentialSpecName field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpec")]
    pub gmsa_credential_spec: Option<String>,
    /// GMSACredentialSpecName is the name of the GMSA credential spec to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gmsaCredentialSpecName")]
    pub gmsa_credential_spec_name: Option<String>,
    /// HostProcess determines if a container should be run as a 'Host Process' container.
    /// All of a Pod's containers must have the same effective HostProcess value
    /// (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).
    /// In addition, if HostProcess is true then HostNetwork must also be set to true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostProcess")]
    pub host_process: Option<bool>,
    /// The UserName in Windows to run the entrypoint of the container process.
    /// Defaults to the user specified in image metadata if unspecified.
    /// May also be set in PodSecurityContext. If set in both SecurityContext and
    /// PodSecurityContext, the value specified in SecurityContext takes precedence.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "runAsUserName")]
    pub run_as_user_name: Option<String>,
}

/// StartupProbe indicates that the Pod has successfully initialized.
/// If specified, no other probes are executed until this completes successfully.
/// If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
/// This can be used to provide different probe parameters at the beginning of a Pod's lifecycle,
/// when it might take a long time to load data or warm a cache, than during steady-state operation.
/// This cannot be updated.
/// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbe {
    /// Exec specifies a command to execute in the container.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exec: Option<SparkApplicationExecutorSidecarsStartupProbeExec>,
    /// Minimum consecutive failures for the probe to be considered failed after having succeeded.
    /// Defaults to 3. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failureThreshold")]
    pub failure_threshold: Option<i32>,
    /// GRPC specifies a GRPC HealthCheckRequest.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub grpc: Option<SparkApplicationExecutorSidecarsStartupProbeGrpc>,
    /// HTTPGet specifies an HTTP GET request to perform.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpGet")]
    pub http_get: Option<SparkApplicationExecutorSidecarsStartupProbeHttpGet>,
    /// Number of seconds after the container has started before liveness probes are initiated.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initialDelaySeconds")]
    pub initial_delay_seconds: Option<i32>,
    /// How often (in seconds) to perform the probe.
    /// Default to 10 seconds. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "periodSeconds")]
    pub period_seconds: Option<i32>,
    /// Minimum consecutive successes for the probe to be considered successful after having failed.
    /// Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successThreshold")]
    pub success_threshold: Option<i32>,
    /// TCPSocket specifies a connection to a TCP port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpSocket")]
    pub tcp_socket: Option<SparkApplicationExecutorSidecarsStartupProbeTcpSocket>,
    /// Optional duration in seconds the pod needs to terminate gracefully upon probe failure.
    /// The grace period is the duration in seconds after the processes running in the pod are sent
    /// a termination signal and the time when the processes are forcibly halted with a kill signal.
    /// Set this value longer than the expected cleanup time for your process.
    /// If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this
    /// value overrides the value provided by the pod spec.
    /// Value must be non-negative integer. The value zero indicates stop immediately via
    /// the kill signal (no opportunity to shut down).
    /// This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.
    /// Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationGracePeriodSeconds")]
    pub termination_grace_period_seconds: Option<i64>,
    /// Number of seconds after which the probe times out.
    /// Defaults to 1 second. Minimum value is 1.
    /// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSeconds")]
    pub timeout_seconds: Option<i32>,
}

/// Exec specifies a command to execute in the container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeExec {
    /// Command is the command line to execute inside the container, the working directory for the
    /// command  is root ('/') in the container's filesystem. The command is simply exec'd, it is
    /// not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use
    /// a shell, you need to explicitly call out to that shell.
    /// Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
}

/// GRPC specifies a GRPC HealthCheckRequest.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeGrpc {
    /// Port number of the gRPC service. Number must be in the range 1 to 65535.
    pub port: i32,
    /// Service is the name of the service to place in the gRPC HealthCheckRequest
    /// (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
    /// 
    /// If this is not specified, the default behavior is defined by gRPC.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub service: Option<String>,
}

/// HTTPGet specifies an HTTP GET request to perform.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeHttpGet {
    /// Host name to connect to, defaults to the pod IP. You probably want to set
    /// "Host" in httpHeaders instead.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Custom headers to set in the request. HTTP allows repeated headers.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpHeaders")]
    pub http_headers: Option<Vec<SparkApplicationExecutorSidecarsStartupProbeHttpGetHttpHeaders>>,
    /// Path to access on the HTTP server.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// Name or number of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
    /// Scheme to use for connecting to the host.
    /// Defaults to HTTP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheme: Option<String>,
}

/// HTTPHeader describes a custom header to be used in HTTP probes
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeHttpGetHttpHeaders {
    /// The header field name.
    /// This will be canonicalized upon output, so case-variant names will be understood as the same header.
    pub name: String,
    /// The header field value
    pub value: String,
}

/// TCPSocket specifies a connection to a TCP port.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsStartupProbeTcpSocket {
    /// Optional: Host name to connect to, defaults to the pod IP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    /// Number or name of the port to access on the container.
    /// Number must be in the range 1 to 65535.
    /// Name must be an IANA_SVC_NAME.
    pub port: IntOrString,
}

/// volumeDevice describes a mapping of a raw block device within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsVolumeDevices {
    /// devicePath is the path inside of the container that the device will be mapped to.
    #[serde(rename = "devicePath")]
    pub device_path: String,
    /// name must match the name of a persistentVolumeClaim in the pod
    pub name: String,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorSidecarsVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    /// When RecursiveReadOnly is set to IfPossible or to Enabled, MountPropagation must be None or unspecified
    /// (which defaults to None).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// RecursiveReadOnly specifies whether read-only mounts should be handled
    /// recursively.
    /// 
    /// If ReadOnly is false, this field has no meaning and must be unspecified.
    /// 
    /// If ReadOnly is true, and this field is set to Disabled, the mount is not made
    /// recursively read-only.  If this field is set to IfPossible, the mount is made
    /// recursively read-only, if it is supported by the container runtime.  If this
    /// field is set to Enabled, the mount is made recursively read-only if it is
    /// supported by the container runtime, otherwise the pod will not be started and
    /// an error will be generated to indicate the reason.
    /// 
    /// If this field is set to IfPossible or Enabled, MountPropagation must be set to
    /// None (or be unspecified, which defaults to None).
    /// 
    /// If this field is not specified, it is treated as an equivalent of Disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recursiveReadOnly")]
    pub recursive_read_only: Option<String>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// The pod this Toleration is attached to tolerates any taint that matches
/// the triple <key,value,effect> using the matching operator <operator>.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorTolerations {
    /// Effect indicates the taint effect to match. Empty means match all taint effects.
    /// When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub effect: Option<String>,
    /// Key is the taint key that the toleration applies to. Empty means match all taint keys.
    /// If the key is empty, operator must be Exists; this combination means to match all values and all keys.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Operator represents a key's relationship to the value.
    /// Valid operators are Exists and Equal. Defaults to Equal.
    /// Exists is equivalent to wildcard for value, so that a pod can
    /// tolerate all taints of a particular category.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub operator: Option<String>,
    /// TolerationSeconds represents the period of time the toleration (which must be
    /// of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default,
    /// it is not set, which means tolerate the taint forever (do not evict). Zero and
    /// negative values will be treated as 0 (evict immediately) by the system.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tolerationSeconds")]
    pub toleration_seconds: Option<i64>,
    /// Value is the taint value the toleration matches to.
    /// If the operator is Exists, the value should be empty, otherwise just a regular string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// VolumeMount describes a mounting of a Volume within a container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationExecutorVolumeMounts {
    /// Path within the container at which the volume should be mounted.  Must
    /// not contain ':'.
    #[serde(rename = "mountPath")]
    pub mount_path: String,
    /// mountPropagation determines how mounts are propagated from the host
    /// to container and the other way around.
    /// When not set, MountPropagationNone is used.
    /// This field is beta in 1.10.
    /// When RecursiveReadOnly is set to IfPossible or to Enabled, MountPropagation must be None or unspecified
    /// (which defaults to None).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mountPropagation")]
    pub mount_propagation: Option<String>,
    /// This must match the Name of a Volume.
    pub name: String,
    /// Mounted read-only if true, read-write otherwise (false or unspecified).
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// RecursiveReadOnly specifies whether read-only mounts should be handled
    /// recursively.
    /// 
    /// If ReadOnly is false, this field has no meaning and must be unspecified.
    /// 
    /// If ReadOnly is true, and this field is set to Disabled, the mount is not made
    /// recursively read-only.  If this field is set to IfPossible, the mount is made
    /// recursively read-only, if it is supported by the container runtime.  If this
    /// field is set to Enabled, the mount is made recursively read-only if it is
    /// supported by the container runtime, otherwise the pod will not be started and
    /// an error will be generated to indicate the reason.
    /// 
    /// If this field is set to IfPossible or Enabled, MountPropagation must be set to
    /// None (or be unspecified, which defaults to None).
    /// 
    /// If this field is not specified, it is treated as an equivalent of Disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "recursiveReadOnly")]
    pub recursive_read_only: Option<String>,
    /// Path within the volume from which the container's volume should be mounted.
    /// Defaults to "" (volume's root).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPath")]
    pub sub_path: Option<String>,
    /// Expanded path within the volume from which the container's volume should be mounted.
    /// Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment.
    /// Defaults to "" (volume's root).
    /// SubPathExpr and SubPath are mutually exclusive.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subPathExpr")]
    pub sub_path_expr: Option<String>,
}

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationMode {
    #[serde(rename = "cluster")]
    Cluster,
    #[serde(rename = "client")]
    Client,
}

/// Monitoring configures how monitoring is handled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationMonitoring {
    /// ExposeDriverMetrics specifies whether to expose metrics on the driver.
    #[serde(rename = "exposeDriverMetrics")]
    pub expose_driver_metrics: bool,
    /// ExposeExecutorMetrics specifies whether to expose metrics on the executors.
    #[serde(rename = "exposeExecutorMetrics")]
    pub expose_executor_metrics: bool,
    /// MetricsProperties is the content of a custom metrics.properties for configuring the Spark metric system.
    /// If not specified, the content in spark-docker/conf/metrics.properties will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricsProperties")]
    pub metrics_properties: Option<String>,
    /// MetricsPropertiesFile is the container local path of file metrics.properties for configuring
    /// the Spark metric system. If not specified, value /etc/metrics/conf/metrics.properties will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricsPropertiesFile")]
    pub metrics_properties_file: Option<String>,
    /// Prometheus is for configuring the Prometheus JMX exporter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prometheus: Option<SparkApplicationMonitoringPrometheus>,
}

/// Prometheus is for configuring the Prometheus JMX exporter.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationMonitoringPrometheus {
    /// ConfigFile is the path to the custom Prometheus configuration file provided in the Spark image.
    /// ConfigFile takes precedence over Configuration, which is shown below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configFile")]
    pub config_file: Option<String>,
    /// Configuration is the content of the Prometheus configuration needed by the Prometheus JMX exporter.
    /// If not specified, the content in spark-docker/conf/prometheus.yaml will be used.
    /// Configuration has no effect if ConfigFile is set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub configuration: Option<String>,
    /// JmxExporterJar is the path to the Prometheus JMX exporter jar in the container.
    #[serde(rename = "jmxExporterJar")]
    pub jmx_exporter_jar: String,
    /// Port is the port of the HTTP server run by the Prometheus JMX exporter.
    /// If not specified, 8090 will be used as the default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// PortName is the port name of prometheus JMX exporter port.
    /// If not specified, jmx-exporter will be used as the default.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portName")]
    pub port_name: Option<String>,
}

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationPythonVersion {
    #[serde(rename = "2")]
    r#_2,
    #[serde(rename = "3")]
    r#_3,
}

/// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationRestartPolicy {
    /// OnFailureRetries the number of times to retry running an application before giving up.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onFailureRetries")]
    pub on_failure_retries: Option<i32>,
    /// OnFailureRetryInterval is the interval in seconds between retries on failed runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onFailureRetryInterval")]
    pub on_failure_retry_interval: Option<i64>,
    /// OnSubmissionFailureRetries is the number of times to retry submitting an application before giving up.
    /// This is best effort and actual retry attempts can be >= the value specified due to caching.
    /// These are required if RestartPolicy is OnFailure.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onSubmissionFailureRetries")]
    pub on_submission_failure_retries: Option<i32>,
    /// OnSubmissionFailureRetryInterval is the interval in seconds between retries on failed submissions.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onSubmissionFailureRetryInterval")]
    pub on_submission_failure_retry_interval: Option<i64>,
    /// Type specifies the RestartPolicyType.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<SparkApplicationRestartPolicyType>,
}

/// RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationRestartPolicyType {
    Never,
    Always,
    OnFailure,
}

/// SparkUIOptions allows configuring the Service and the Ingress to expose the sparkUI
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationSparkUiOptions {
    /// IngressAnnotations is a map of key,value pairs of annotations that might be added to the ingress object. i.e. specify nginx as ingress.class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressAnnotations")]
    pub ingress_annotations: Option<BTreeMap<String, String>>,
    /// TlsHosts is useful If we need to declare SSL certificates to the ingress object
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ingressTLS")]
    pub ingress_tls: Option<Vec<SparkApplicationSparkUiOptionsIngressTls>>,
    /// ServiceAnnotations is a map of key,value pairs of annotations that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAnnotations")]
    pub service_annotations: Option<BTreeMap<String, String>>,
    /// ServiceLabels is a map of key,value pairs of labels that might be added to the service object.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceLabels")]
    pub service_labels: Option<BTreeMap<String, String>>,
    /// ServicePort allows configuring the port at service level that might be different from the targetPort.
    /// TargetPort should be the same as the one defined in spark.ui.port
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "servicePort")]
    pub service_port: Option<i32>,
    /// ServicePortName allows configuring the name of the service port.
    /// This may be useful for sidecar proxies like Envoy injected by Istio which require specific ports names to treat traffic as proper HTTP.
    /// Defaults to spark-driver-ui-port.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "servicePortName")]
    pub service_port_name: Option<String>,
    /// ServiceType allows configuring the type of the service. Defaults to ClusterIP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceType")]
    pub service_type: Option<String>,
}

/// IngressTLS describes the transport layer security associated with an ingress.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationSparkUiOptionsIngressTls {
    /// hosts is a list of hosts included in the TLS certificate. The values in
    /// this list must match the name/s used in the tlsSecret. Defaults to the
    /// wildcard host setting for the loadbalancer controller fulfilling this
    /// Ingress, if left unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub hosts: Option<Vec<String>>,
    /// secretName is the name of the secret used to terminate TLS traffic on
    /// port 443. Field is left optional to allow TLS routing based on SNI
    /// hostname alone. If the SNI host in a listener conflicts with the "Host"
    /// header field used by an IngressRule, the SNI host is used for termination
    /// and value of the "Host" header is used for routing.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// SparkApplicationSpec defines the desired state of SparkApplication
/// It carries every pieces of information a spark-submit command takes and recognizes.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum SparkApplicationType {
    Java,
    Python,
    Scala,
    R,
}

/// Volume represents a named volume in a pod that may be accessed by any container in the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumes {
    /// awsElasticBlockStore represents an AWS Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// Deprecated: AWSElasticBlockStore is deprecated. All operations for the in-tree
    /// awsElasticBlockStore type are redirected to the ebs.csi.aws.com CSI driver.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "awsElasticBlockStore")]
    pub aws_elastic_block_store: Option<SparkApplicationVolumesAwsElasticBlockStore>,
    /// azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.
    /// Deprecated: AzureDisk is deprecated. All operations for the in-tree azureDisk type
    /// are redirected to the disk.csi.azure.com CSI driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "azureDisk")]
    pub azure_disk: Option<SparkApplicationVolumesAzureDisk>,
    /// azureFile represents an Azure File Service mount on the host and bind mount to the pod.
    /// Deprecated: AzureFile is deprecated. All operations for the in-tree azureFile type
    /// are redirected to the file.csi.azure.com CSI driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "azureFile")]
    pub azure_file: Option<SparkApplicationVolumesAzureFile>,
    /// cephFS represents a Ceph FS mount on the host that shares a pod's lifetime.
    /// Deprecated: CephFS is deprecated and the in-tree cephfs type is no longer supported.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cephfs: Option<SparkApplicationVolumesCephfs>,
    /// cinder represents a cinder volume attached and mounted on kubelets host machine.
    /// Deprecated: Cinder is deprecated. All operations for the in-tree cinder type
    /// are redirected to the cinder.csi.openstack.org CSI driver.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cinder: Option<SparkApplicationVolumesCinder>,
    /// configMap represents a configMap that should populate this volume
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<SparkApplicationVolumesConfigMap>,
    /// csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub csi: Option<SparkApplicationVolumesCsi>,
    /// downwardAPI represents downward API about the pod that should populate this volume
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "downwardAPI")]
    pub downward_api: Option<SparkApplicationVolumesDownwardApi>,
    /// emptyDir represents a temporary directory that shares a pod's lifetime.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "emptyDir")]
    pub empty_dir: Option<SparkApplicationVolumesEmptyDir>,
    /// ephemeral represents a volume that is handled by a cluster storage driver.
    /// The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts,
    /// and deleted when the pod is removed.
    /// 
    /// Use this if:
    /// a) the volume is only needed while the pod runs,
    /// b) features of normal volumes like restoring from snapshot or capacity
    ///    tracking are needed,
    /// c) the storage driver is specified through a storage class, and
    /// d) the storage driver supports dynamic volume provisioning through
    ///    a PersistentVolumeClaim (see EphemeralVolumeSource for more
    ///    information on the connection between this volume type
    ///    and PersistentVolumeClaim).
    /// 
    /// Use PersistentVolumeClaim or one of the vendor-specific
    /// APIs for volumes that persist for longer than the lifecycle
    /// of an individual pod.
    /// 
    /// Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to
    /// be used that way - see the documentation of the driver for
    /// more information.
    /// 
    /// A pod can use both types of ephemeral volumes and
    /// persistent volumes at the same time.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ephemeral: Option<SparkApplicationVolumesEphemeral>,
    /// fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub fc: Option<SparkApplicationVolumesFc>,
    /// flexVolume represents a generic volume resource that is
    /// provisioned/attached using an exec based plugin.
    /// Deprecated: FlexVolume is deprecated. Consider using a CSIDriver instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "flexVolume")]
    pub flex_volume: Option<SparkApplicationVolumesFlexVolume>,
    /// flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running.
    /// Deprecated: Flocker is deprecated and the in-tree flocker type is no longer supported.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub flocker: Option<SparkApplicationVolumesFlocker>,
    /// gcePersistentDisk represents a GCE Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// Deprecated: GCEPersistentDisk is deprecated. All operations for the in-tree
    /// gcePersistentDisk type are redirected to the pd.csi.storage.gke.io CSI driver.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gcePersistentDisk")]
    pub gce_persistent_disk: Option<SparkApplicationVolumesGcePersistentDisk>,
    /// gitRepo represents a git repository at a particular revision.
    /// Deprecated: GitRepo is deprecated. To provision a container with a git repo, mount an
    /// EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir
    /// into the Pod's container.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gitRepo")]
    pub git_repo: Option<SparkApplicationVolumesGitRepo>,
    /// glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime.
    /// Deprecated: Glusterfs is deprecated and the in-tree glusterfs type is no longer supported.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub glusterfs: Option<SparkApplicationVolumesGlusterfs>,
    /// hostPath represents a pre-existing file or directory on the host
    /// machine that is directly exposed to the container. This is generally
    /// used for system agents or other privileged things that are allowed
    /// to see the host machine. Most containers will NOT need this.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hostPath")]
    pub host_path: Option<SparkApplicationVolumesHostPath>,
    /// image represents an OCI object (a container image or artifact) pulled and mounted on the kubelet's host machine.
    /// The volume is resolved at pod startup depending on which PullPolicy value is provided:
    /// 
    /// - Always: the kubelet always attempts to pull the reference. Container creation will fail If the pull fails.
    /// - Never: the kubelet never pulls the reference and only uses a local image or artifact. Container creation will fail if the reference isn't present.
    /// - IfNotPresent: the kubelet pulls if the reference isn't already present on disk. Container creation will fail if the reference isn't present and the pull fails.
    /// 
    /// The volume gets re-resolved if the pod gets deleted and recreated, which means that new remote content will become available on pod recreation.
    /// A failure to resolve or pull the image during pod startup will block containers from starting and may add significant latency. Failures will be retried using normal volume backoff and will be reported on the pod reason and message.
    /// The types of objects that may be mounted by this volume are defined by the container runtime implementation on a host machine and at minimum must include all valid types supported by the container image field.
    /// The OCI object gets mounted in a single directory (spec.containers[*].volumeMounts.mountPath) by merging the manifest layers in the same way as for container images.
    /// The volume will be mounted read-only (ro) and non-executable files (noexec).
    /// Sub path mounts for containers are not supported (spec.containers[*].volumeMounts.subpath).
    /// The field spec.securityContext.fsGroupChangePolicy has no effect on this volume type.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<SparkApplicationVolumesImage>,
    /// iscsi represents an ISCSI Disk resource that is attached to a
    /// kubelet's host machine and then exposed to the pod.
    /// More info: https://examples.k8s.io/volumes/iscsi/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub iscsi: Option<SparkApplicationVolumesIscsi>,
    /// name of the volume.
    /// Must be a DNS_LABEL and unique within the pod.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    pub name: String,
    /// nfs represents an NFS mount on the host that shares a pod's lifetime
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub nfs: Option<SparkApplicationVolumesNfs>,
    /// persistentVolumeClaimVolumeSource represents a reference to a
    /// PersistentVolumeClaim in the same namespace.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "persistentVolumeClaim")]
    pub persistent_volume_claim: Option<SparkApplicationVolumesPersistentVolumeClaim>,
    /// photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine.
    /// Deprecated: PhotonPersistentDisk is deprecated and the in-tree photonPersistentDisk type is no longer supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "photonPersistentDisk")]
    pub photon_persistent_disk: Option<SparkApplicationVolumesPhotonPersistentDisk>,
    /// portworxVolume represents a portworx volume attached and mounted on kubelets host machine.
    /// Deprecated: PortworxVolume is deprecated. All operations for the in-tree portworxVolume type
    /// are redirected to the pxd.portworx.com CSI driver when the CSIMigrationPortworx feature-gate
    /// is on.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portworxVolume")]
    pub portworx_volume: Option<SparkApplicationVolumesPortworxVolume>,
    /// projected items for all in one resources secrets, configmaps, and downward API
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub projected: Option<SparkApplicationVolumesProjected>,
    /// quobyte represents a Quobyte mount on the host that shares a pod's lifetime.
    /// Deprecated: Quobyte is deprecated and the in-tree quobyte type is no longer supported.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub quobyte: Option<SparkApplicationVolumesQuobyte>,
    /// rbd represents a Rados Block Device mount on the host that shares a pod's lifetime.
    /// Deprecated: RBD is deprecated and the in-tree rbd type is no longer supported.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub rbd: Option<SparkApplicationVolumesRbd>,
    /// scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.
    /// Deprecated: ScaleIO is deprecated and the in-tree scaleIO type is no longer supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scaleIO")]
    pub scale_io: Option<SparkApplicationVolumesScaleIo>,
    /// secret represents a secret that should populate this volume.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secret: Option<SparkApplicationVolumesSecret>,
    /// storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.
    /// Deprecated: StorageOS is deprecated and the in-tree storageos type is no longer supported.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub storageos: Option<SparkApplicationVolumesStorageos>,
    /// vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine.
    /// Deprecated: VsphereVolume is deprecated. All operations for the in-tree vsphereVolume type
    /// are redirected to the csi.vsphere.vmware.com CSI driver.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vsphereVolume")]
    pub vsphere_volume: Option<SparkApplicationVolumesVsphereVolume>,
}

/// awsElasticBlockStore represents an AWS Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// Deprecated: AWSElasticBlockStore is deprecated. All operations for the in-tree
/// awsElasticBlockStore type are redirected to the ebs.csi.aws.com CSI driver.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesAwsElasticBlockStore {
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// partition is the partition in the volume that you want to mount.
    /// If omitted, the default is to mount by volume name.
    /// Examples: For volume /dev/sda1, you specify the partition as "1".
    /// Similarly, the volume partition for /dev/sda is "0" (or you can leave the property empty).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub partition: Option<i32>,
    /// readOnly value true will force the readOnly setting in VolumeMounts.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeID is unique ID of the persistent disk resource in AWS (Amazon EBS volume).
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.
/// Deprecated: AzureDisk is deprecated. All operations for the in-tree azureDisk type
/// are redirected to the disk.csi.azure.com CSI driver.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesAzureDisk {
    /// cachingMode is the Host Caching mode: None, Read Only, Read Write.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cachingMode")]
    pub caching_mode: Option<String>,
    /// diskName is the Name of the data disk in the blob storage
    #[serde(rename = "diskName")]
    pub disk_name: String,
    /// diskURI is the URI of data disk in the blob storage
    #[serde(rename = "diskURI")]
    pub disk_uri: String,
    /// fsType is Filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// kind expected values are Shared: multiple blob disks per storage account  Dedicated: single blob disk per storage account  Managed: azure managed data disk (only in managed availability set). defaults to shared
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kind: Option<String>,
    /// readOnly Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// azureFile represents an Azure File Service mount on the host and bind mount to the pod.
/// Deprecated: AzureFile is deprecated. All operations for the in-tree azureFile type
/// are redirected to the file.csi.azure.com CSI driver.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesAzureFile {
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretName is the  name of secret that contains Azure Storage Account Name and Key
    #[serde(rename = "secretName")]
    pub secret_name: String,
    /// shareName is the azure share Name
    #[serde(rename = "shareName")]
    pub share_name: String,
}

/// cephFS represents a Ceph FS mount on the host that shares a pod's lifetime.
/// Deprecated: CephFS is deprecated and the in-tree cephfs type is no longer supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCephfs {
    /// monitors is Required: Monitors is a collection of Ceph monitors
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    pub monitors: Vec<String>,
    /// path is Optional: Used as the mounted root, rather than the full Ceph tree, default is /
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub path: Option<String>,
    /// readOnly is Optional: Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretFile is Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretFile")]
    pub secret_file: Option<String>,
    /// secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty.
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesCephfsSecretRef>,
    /// user is optional: User is the rados user name, default is admin
    /// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty.
/// More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCephfsSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// cinder represents a cinder volume attached and mounted on kubelets host machine.
/// Deprecated: Cinder is deprecated. All operations for the in-tree cinder type
/// are redirected to the cinder.csi.openstack.org CSI driver.
/// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCinder {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is optional: points to a secret object containing parameters used to connect
    /// to OpenStack.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesCinderSecretRef>,
    /// volumeID used to identify the volume in cinder.
    /// More info: https://examples.k8s.io/mysql-cinder-pd/README.md
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// secretRef is optional: points to a secret object containing parameters used to connect
/// to OpenStack.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCinderSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// configMap represents a configMap that should populate this volume
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesConfigMap {
    /// defaultMode is optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// ConfigMap will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the ConfigMap,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesConfigMapItems>>,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional specify whether the ConfigMap or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesConfigMapItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCsi {
    /// driver is the name of the CSI driver that handles this volume.
    /// Consult with your admin for the correct name as registered in the cluster.
    pub driver: String,
    /// fsType to mount. Ex. "ext4", "xfs", "ntfs".
    /// If not provided, the empty value is passed to the associated CSI driver
    /// which will determine the default filesystem to apply.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// nodePublishSecretRef is a reference to the secret object containing
    /// sensitive information to pass to the CSI driver to complete the CSI
    /// NodePublishVolume and NodeUnpublishVolume calls.
    /// This field is optional, and  may be empty if no secret is required. If the
    /// secret object contains more than one secret, all secret references are passed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePublishSecretRef")]
    pub node_publish_secret_ref: Option<SparkApplicationVolumesCsiNodePublishSecretRef>,
    /// readOnly specifies a read-only configuration for the volume.
    /// Defaults to false (read/write).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeAttributes stores driver-specific properties that are passed to the CSI
    /// driver. Consult your driver's documentation for supported values.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeAttributes")]
    pub volume_attributes: Option<BTreeMap<String, String>>,
}

/// nodePublishSecretRef is a reference to the secret object containing
/// sensitive information to pass to the CSI driver to complete the CSI
/// NodePublishVolume and NodeUnpublishVolume calls.
/// This field is optional, and  may be empty if no secret is required. If the
/// secret object contains more than one secret, all secret references are passed.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesCsiNodePublishSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// downwardAPI represents downward API about the pod that should populate this volume
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApi {
    /// Optional: mode bits to use on created files by default. Must be a
    /// Optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// Items is a list of downward API volume file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesDownwardApiItems>>,
}

/// DownwardAPIVolumeFile represents information to create the file containing the pod field
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApiItems {
    /// Required: Selects a field of the pod: only annotations, labels, name, namespace and uid are supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationVolumesDownwardApiItemsFieldRef>,
    /// Optional: mode bits used to set permissions on this file, must be an octal value
    /// between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'
    pub path: String,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationVolumesDownwardApiItemsResourceFieldRef>,
}

/// Required: Selects a field of the pod: only annotations, labels, name, namespace and uid are supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApiItemsFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesDownwardApiItemsResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// emptyDir represents a temporary directory that shares a pod's lifetime.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEmptyDir {
    /// medium represents what type of storage medium should back this directory.
    /// The default is "" which means to use the node's default medium.
    /// Must be an empty string (default) or Memory.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub medium: Option<String>,
    /// sizeLimit is the total amount of local storage required for this EmptyDir volume.
    /// The size limit is also applicable for memory medium.
    /// The maximum usage on memory medium EmptyDir would be the minimum value between
    /// the SizeLimit specified here and the sum of memory limits of all containers in a pod.
    /// The default is nil which means that the limit is undefined.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sizeLimit")]
    pub size_limit: Option<IntOrString>,
}

/// ephemeral represents a volume that is handled by a cluster storage driver.
/// The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts,
/// and deleted when the pod is removed.
/// 
/// Use this if:
/// a) the volume is only needed while the pod runs,
/// b) features of normal volumes like restoring from snapshot or capacity
///    tracking are needed,
/// c) the storage driver is specified through a storage class, and
/// d) the storage driver supports dynamic volume provisioning through
///    a PersistentVolumeClaim (see EphemeralVolumeSource for more
///    information on the connection between this volume type
///    and PersistentVolumeClaim).
/// 
/// Use PersistentVolumeClaim or one of the vendor-specific
/// APIs for volumes that persist for longer than the lifecycle
/// of an individual pod.
/// 
/// Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to
/// be used that way - see the documentation of the driver for
/// more information.
/// 
/// A pod can use both types of ephemeral volumes and
/// persistent volumes at the same time.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeral {
    /// Will be used to create a stand-alone PVC to provision the volume.
    /// The pod in which this EphemeralVolumeSource is embedded will be the
    /// owner of the PVC, i.e. the PVC will be deleted together with the
    /// pod.  The name of the PVC will be `<pod name>-<volume name>` where
    /// `<volume name>` is the name from the `PodSpec.Volumes` array
    /// entry. Pod validation will reject the pod if the concatenated name
    /// is not valid for a PVC (for example, too long).
    /// 
    /// An existing PVC with that name that is not owned by the pod
    /// will *not* be used for the pod to avoid using an unrelated
    /// volume by mistake. Starting the pod is then blocked until
    /// the unrelated PVC is removed. If such a pre-created PVC is
    /// meant to be used by the pod, the PVC has to updated with an
    /// owner reference to the pod once the pod exists. Normally
    /// this should not be necessary, but it may be useful when
    /// manually reconstructing a broken cluster.
    /// 
    /// This field is read-only and no changes will be made by Kubernetes
    /// to the PVC after it has been created.
    /// 
    /// Required, must not be nil.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeClaimTemplate")]
    pub volume_claim_template: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplate>,
}

/// Will be used to create a stand-alone PVC to provision the volume.
/// The pod in which this EphemeralVolumeSource is embedded will be the
/// owner of the PVC, i.e. the PVC will be deleted together with the
/// pod.  The name of the PVC will be `<pod name>-<volume name>` where
/// `<volume name>` is the name from the `PodSpec.Volumes` array
/// entry. Pod validation will reject the pod if the concatenated name
/// is not valid for a PVC (for example, too long).
/// 
/// An existing PVC with that name that is not owned by the pod
/// will *not* be used for the pod to avoid using an unrelated
/// volume by mistake. Starting the pod is then blocked until
/// the unrelated PVC is removed. If such a pre-created PVC is
/// meant to be used by the pod, the PVC has to updated with an
/// owner reference to the pod once the pod exists. Normally
/// this should not be necessary, but it may be useful when
/// manually reconstructing a broken cluster.
/// 
/// This field is read-only and no changes will be made by Kubernetes
/// to the PVC after it has been created.
/// 
/// Required, must not be nil.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplate {
    /// May contain labels and annotations that will be copied into the PVC
    /// when creating it. No other fields are allowed and will be rejected during
    /// validation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateMetadata>,
    /// The specification for the PersistentVolumeClaim. The entire content is
    /// copied unchanged into the PVC that gets created from this
    /// template. The same fields as in a PersistentVolumeClaim
    /// are also valid here.
    pub spec: SparkApplicationVolumesEphemeralVolumeClaimTemplateSpec,
}

/// May contain labels and annotations that will be copied into the PVC
/// when creating it. No other fields are allowed and will be rejected during
/// validation.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateMetadata {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub finalizers: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<BTreeMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// The specification for the PersistentVolumeClaim. The entire content is
/// copied unchanged into the PVC that gets created from this
/// template. The same fields as in a PersistentVolumeClaim
/// are also valid here.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpec {
    /// accessModes contains the desired access modes the volume should have.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "accessModes")]
    pub access_modes: Option<Vec<String>>,
    /// dataSource field can be used to specify either:
    /// * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)
    /// * An existing PVC (PersistentVolumeClaim)
    /// If the provisioner or an external controller can support the specified data source,
    /// it will create a new volume based on the contents of the specified data source.
    /// When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,
    /// and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.
    /// If the namespace is specified, then dataSourceRef will not be copied to dataSource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSource")]
    pub data_source: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSource>,
    /// dataSourceRef specifies the object from which to populate the volume with data, if a non-empty
    /// volume is desired. This may be any object from a non-empty API group (non
    /// core object) or a PersistentVolumeClaim object.
    /// When this field is specified, volume binding will only succeed if the type of
    /// the specified object matches some installed volume populator or dynamic
    /// provisioner.
    /// This field will replace the functionality of the dataSource field and as such
    /// if both fields are non-empty, they must have the same value. For backwards
    /// compatibility, when namespace isn't specified in dataSourceRef,
    /// both fields (dataSource and dataSourceRef) will be set to the same
    /// value automatically if one of them is empty and the other is non-empty.
    /// When namespace is specified in dataSourceRef,
    /// dataSource isn't set to the same value and must be empty.
    /// There are three important differences between dataSource and dataSourceRef:
    /// * While dataSource only allows two specific types of objects, dataSourceRef
    ///   allows any non-core object, as well as PersistentVolumeClaim objects.
    /// * While dataSource ignores disallowed values (dropping them), dataSourceRef
    ///   preserves all values, and generates an error if a disallowed value is
    ///   specified.
    /// * While dataSource only allows local objects, dataSourceRef allows objects
    ///   in any namespaces.
    /// (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.
    /// (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataSourceRef")]
    pub data_source_ref: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSourceRef>,
    /// resources represents the minimum resources the volume should have.
    /// If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements
    /// that are lower than previous value but must still be higher than capacity recorded in the
    /// status field of the claim.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecResources>,
    /// selector is a label query over volumes to consider for binding.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelector>,
    /// storageClassName is the name of the StorageClass required by the claim.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storageClassName")]
    pub storage_class_name: Option<String>,
    /// volumeAttributesClassName may be used to set the VolumeAttributesClass used by this claim.
    /// If specified, the CSI driver will create or update the volume with the attributes defined
    /// in the corresponding VolumeAttributesClass. This has a different purpose than storageClassName,
    /// it can be changed after the claim is created. An empty string value means that no VolumeAttributesClass
    /// will be applied to the claim but it's not allowed to reset this field to empty string once it is set.
    /// If unspecified and the PersistentVolumeClaim is unbound, the default VolumeAttributesClass
    /// will be set by the persistentvolume controller if it exists.
    /// If the resource referred to by volumeAttributesClass does not exist, this PersistentVolumeClaim will be
    /// set to a Pending state, as reflected by the modifyVolumeStatus field, until such as a resource
    /// exists.
    /// More info: https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/
    /// (Beta) Using this field requires the VolumeAttributesClass feature gate to be enabled (off by default).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeAttributesClassName")]
    pub volume_attributes_class_name: Option<String>,
    /// volumeMode defines what type of volume is required by the claim.
    /// Value of Filesystem is implied when not included in claim spec.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeMode")]
    pub volume_mode: Option<String>,
    /// volumeName is the binding reference to the PersistentVolume backing this claim.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
}

/// dataSource field can be used to specify either:
/// * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)
/// * An existing PVC (PersistentVolumeClaim)
/// If the provisioner or an external controller can support the specified data source,
/// it will create a new volume based on the contents of the specified data source.
/// When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,
/// and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.
/// If the namespace is specified, then dataSourceRef will not be copied to dataSource.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSource {
    /// APIGroup is the group for the resource being referenced.
    /// If APIGroup is not specified, the specified Kind must be in the core API group.
    /// For any other third-party types, APIGroup is required.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiGroup")]
    pub api_group: Option<String>,
    /// Kind is the type of resource being referenced
    pub kind: String,
    /// Name is the name of resource being referenced
    pub name: String,
}

/// dataSourceRef specifies the object from which to populate the volume with data, if a non-empty
/// volume is desired. This may be any object from a non-empty API group (non
/// core object) or a PersistentVolumeClaim object.
/// When this field is specified, volume binding will only succeed if the type of
/// the specified object matches some installed volume populator or dynamic
/// provisioner.
/// This field will replace the functionality of the dataSource field and as such
/// if both fields are non-empty, they must have the same value. For backwards
/// compatibility, when namespace isn't specified in dataSourceRef,
/// both fields (dataSource and dataSourceRef) will be set to the same
/// value automatically if one of them is empty and the other is non-empty.
/// When namespace is specified in dataSourceRef,
/// dataSource isn't set to the same value and must be empty.
/// There are three important differences between dataSource and dataSourceRef:
/// * While dataSource only allows two specific types of objects, dataSourceRef
///   allows any non-core object, as well as PersistentVolumeClaim objects.
/// * While dataSource ignores disallowed values (dropping them), dataSourceRef
///   preserves all values, and generates an error if a disallowed value is
///   specified.
/// * While dataSource only allows local objects, dataSourceRef allows objects
///   in any namespaces.
/// (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.
/// (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecDataSourceRef {
    /// APIGroup is the group for the resource being referenced.
    /// If APIGroup is not specified, the specified Kind must be in the core API group.
    /// For any other third-party types, APIGroup is required.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiGroup")]
    pub api_group: Option<String>,
    /// Kind is the type of resource being referenced
    pub kind: String,
    /// Name is the name of resource being referenced
    pub name: String,
    /// Namespace is the namespace of resource being referenced
    /// Note that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details.
    /// (Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// resources represents the minimum resources the volume should have.
/// If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements
/// that are lower than previous value but must still be higher than capacity recorded in the
/// status field of the claim.
/// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecResources {
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// selector is a label query over volumes to consider for binding.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesEphemeralVolumeClaimTemplateSpecSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFc {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// lun is Optional: FC target lun number
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub lun: Option<i32>,
    /// readOnly is Optional: Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// targetWWNs is Optional: FC target worldwide names (WWNs)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetWWNs")]
    pub target_ww_ns: Option<Vec<String>>,
    /// wwids Optional: FC volume world wide identifiers (wwids)
    /// Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub wwids: Option<Vec<String>>,
}

/// flexVolume represents a generic volume resource that is
/// provisioned/attached using an exec based plugin.
/// Deprecated: FlexVolume is deprecated. Consider using a CSIDriver instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFlexVolume {
    /// driver is the name of the driver to use for this volume.
    pub driver: String,
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". The default filesystem depends on FlexVolume script.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// options is Optional: this field holds extra command options if any.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub options: Option<BTreeMap<String, String>>,
    /// readOnly is Optional: defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is Optional: secretRef is reference to the secret object containing
    /// sensitive information to pass to the plugin scripts. This may be
    /// empty if no secret object is specified. If the secret object
    /// contains more than one secret, all secrets are passed to the plugin
    /// scripts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesFlexVolumeSecretRef>,
}

/// secretRef is Optional: secretRef is reference to the secret object containing
/// sensitive information to pass to the plugin scripts. This may be
/// empty if no secret object is specified. If the secret object
/// contains more than one secret, all secrets are passed to the plugin
/// scripts.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFlexVolumeSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running.
/// Deprecated: Flocker is deprecated and the in-tree flocker type is no longer supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesFlocker {
    /// datasetName is Name of the dataset stored as metadata -> name on the dataset for Flocker
    /// should be considered as deprecated
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datasetName")]
    pub dataset_name: Option<String>,
    /// datasetUUID is the UUID of the dataset. This is unique identifier of a Flocker dataset
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datasetUUID")]
    pub dataset_uuid: Option<String>,
}

/// gcePersistentDisk represents a GCE Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// Deprecated: GCEPersistentDisk is deprecated. All operations for the in-tree
/// gcePersistentDisk type are redirected to the pd.csi.storage.gke.io CSI driver.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesGcePersistentDisk {
    /// fsType is filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// partition is the partition in the volume that you want to mount.
    /// If omitted, the default is to mount by volume name.
    /// Examples: For volume /dev/sda1, you specify the partition as "1".
    /// Similarly, the volume partition for /dev/sda is "0" (or you can leave the property empty).
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub partition: Option<i32>,
    /// pdName is unique name of the PD resource in GCE. Used to identify the disk in GCE.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(rename = "pdName")]
    pub pd_name: String,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// gitRepo represents a git repository at a particular revision.
/// Deprecated: GitRepo is deprecated. To provision a container with a git repo, mount an
/// EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir
/// into the Pod's container.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesGitRepo {
    /// directory is the target directory name.
    /// Must not contain or start with '..'.  If '.' is supplied, the volume directory will be the
    /// git repository.  Otherwise, if specified, the volume will contain the git repository in
    /// the subdirectory with the given name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub directory: Option<String>,
    /// repository is the URL
    pub repository: String,
    /// revision is the commit hash for the specified revision.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub revision: Option<String>,
}

/// glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime.
/// Deprecated: Glusterfs is deprecated and the in-tree glusterfs type is no longer supported.
/// More info: https://examples.k8s.io/volumes/glusterfs/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesGlusterfs {
    /// endpoints is the endpoint name that details Glusterfs topology.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    pub endpoints: String,
    /// path is the Glusterfs volume path.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    pub path: String,
    /// readOnly here will force the Glusterfs volume to be mounted with read-only permissions.
    /// Defaults to false.
    /// More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// hostPath represents a pre-existing file or directory on the host
/// machine that is directly exposed to the container. This is generally
/// used for system agents or other privileged things that are allowed
/// to see the host machine. Most containers will NOT need this.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesHostPath {
    /// path of the directory on the host.
    /// If the path is a symlink, it will follow the link to the real path.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    pub path: String,
    /// type for HostPath Volume
    /// Defaults to ""
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// image represents an OCI object (a container image or artifact) pulled and mounted on the kubelet's host machine.
/// The volume is resolved at pod startup depending on which PullPolicy value is provided:
/// 
/// - Always: the kubelet always attempts to pull the reference. Container creation will fail If the pull fails.
/// - Never: the kubelet never pulls the reference and only uses a local image or artifact. Container creation will fail if the reference isn't present.
/// - IfNotPresent: the kubelet pulls if the reference isn't already present on disk. Container creation will fail if the reference isn't present and the pull fails.
/// 
/// The volume gets re-resolved if the pod gets deleted and recreated, which means that new remote content will become available on pod recreation.
/// A failure to resolve or pull the image during pod startup will block containers from starting and may add significant latency. Failures will be retried using normal volume backoff and will be reported on the pod reason and message.
/// The types of objects that may be mounted by this volume are defined by the container runtime implementation on a host machine and at minimum must include all valid types supported by the container image field.
/// The OCI object gets mounted in a single directory (spec.containers[*].volumeMounts.mountPath) by merging the manifest layers in the same way as for container images.
/// The volume will be mounted read-only (ro) and non-executable files (noexec).
/// Sub path mounts for containers are not supported (spec.containers[*].volumeMounts.subpath).
/// The field spec.securityContext.fsGroupChangePolicy has no effect on this volume type.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesImage {
    /// Policy for pulling OCI objects. Possible values are:
    /// Always: the kubelet always attempts to pull the reference. Container creation will fail If the pull fails.
    /// Never: the kubelet never pulls the reference and only uses a local image or artifact. Container creation will fail if the reference isn't present.
    /// IfNotPresent: the kubelet pulls if the reference isn't already present on disk. Container creation will fail if the reference isn't present and the pull fails.
    /// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pullPolicy")]
    pub pull_policy: Option<String>,
    /// Required: Image or artifact reference to be used.
    /// Behaves in the same way as pod.spec.containers[*].image.
    /// Pull secrets will be assembled in the same way as for the container image by looking up node credentials, SA image pull secrets, and pod spec image pull secrets.
    /// More info: https://kubernetes.io/docs/concepts/containers/images
    /// This field is optional to allow higher level config management to default or override
    /// container images in workload controllers like Deployments and StatefulSets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reference: Option<String>,
}

/// iscsi represents an ISCSI Disk resource that is attached to a
/// kubelet's host machine and then exposed to the pod.
/// More info: https://examples.k8s.io/volumes/iscsi/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesIscsi {
    /// chapAuthDiscovery defines whether support iSCSI Discovery CHAP authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "chapAuthDiscovery")]
    pub chap_auth_discovery: Option<bool>,
    /// chapAuthSession defines whether support iSCSI Session CHAP authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "chapAuthSession")]
    pub chap_auth_session: Option<bool>,
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// initiatorName is the custom iSCSI Initiator Name.
    /// If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface
    /// <target portal>:<volume name> will be created for the connection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initiatorName")]
    pub initiator_name: Option<String>,
    /// iqn is the target iSCSI Qualified Name.
    pub iqn: String,
    /// iscsiInterface is the interface Name that uses an iSCSI transport.
    /// Defaults to 'default' (tcp).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "iscsiInterface")]
    pub iscsi_interface: Option<String>,
    /// lun represents iSCSI Target Lun number.
    pub lun: i32,
    /// portals is the iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port
    /// is other than default (typically TCP ports 860 and 3260).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub portals: Option<Vec<String>>,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is the CHAP Secret for iSCSI target and initiator authentication
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesIscsiSecretRef>,
    /// targetPortal is iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port
    /// is other than default (typically TCP ports 860 and 3260).
    #[serde(rename = "targetPortal")]
    pub target_portal: String,
}

/// secretRef is the CHAP Secret for iSCSI target and initiator authentication
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesIscsiSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// nfs represents an NFS mount on the host that shares a pod's lifetime
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesNfs {
    /// path that is exported by the NFS server.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    pub path: String,
    /// readOnly here will force the NFS export to be mounted with read-only permissions.
    /// Defaults to false.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// server is the hostname or IP address of the NFS server.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs
    pub server: String,
}

/// persistentVolumeClaimVolumeSource represents a reference to a
/// PersistentVolumeClaim in the same namespace.
/// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesPersistentVolumeClaim {
    /// claimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume.
    /// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
    #[serde(rename = "claimName")]
    pub claim_name: String,
    /// readOnly Will force the ReadOnly setting in VolumeMounts.
    /// Default false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
}

/// photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine.
/// Deprecated: PhotonPersistentDisk is deprecated and the in-tree photonPersistentDisk type is no longer supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesPhotonPersistentDisk {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// pdID is the ID that identifies Photon Controller persistent disk
    #[serde(rename = "pdID")]
    pub pd_id: String,
}

/// portworxVolume represents a portworx volume attached and mounted on kubelets host machine.
/// Deprecated: PortworxVolume is deprecated. All operations for the in-tree portworxVolume type
/// are redirected to the pxd.portworx.com CSI driver when the CSIMigrationPortworx feature-gate
/// is on.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesPortworxVolume {
    /// fSType represents the filesystem type to mount
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// volumeID uniquely identifies a Portworx volume
    #[serde(rename = "volumeID")]
    pub volume_id: String,
}

/// projected items for all in one resources secrets, configmaps, and downward API
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjected {
    /// defaultMode are the mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// sources is the list of volume projections. Each entry in this list
    /// handles one source.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sources: Option<Vec<SparkApplicationVolumesProjectedSources>>,
}

/// Projection that may be projected along with other supported volume types.
/// Exactly one of these fields must be set.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSources {
    /// ClusterTrustBundle allows a pod to access the `.spec.trustBundle` field
    /// of ClusterTrustBundle objects in an auto-updating file.
    /// 
    /// Alpha, gated by the ClusterTrustBundleProjection feature gate.
    /// 
    /// ClusterTrustBundle objects can either be selected by name, or by the
    /// combination of signer name and a label selector.
    /// 
    /// Kubelet performs aggressive normalization of the PEM contents written
    /// into the pod filesystem.  Esoteric PEM features such as inter-block
    /// comments and block headers are stripped.  Certificates are deduplicated.
    /// The ordering of certificates within the file is arbitrary, and Kubelet
    /// may change the order over time.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterTrustBundle")]
    pub cluster_trust_bundle: Option<SparkApplicationVolumesProjectedSourcesClusterTrustBundle>,
    /// configMap information about the configMap data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configMap")]
    pub config_map: Option<SparkApplicationVolumesProjectedSourcesConfigMap>,
    /// downwardAPI information about the downwardAPI data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "downwardAPI")]
    pub downward_api: Option<SparkApplicationVolumesProjectedSourcesDownwardApi>,
    /// secret information about the secret data to project
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub secret: Option<SparkApplicationVolumesProjectedSourcesSecret>,
    /// serviceAccountToken is information about the serviceAccountToken data to project
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountToken")]
    pub service_account_token: Option<SparkApplicationVolumesProjectedSourcesServiceAccountToken>,
}

/// ClusterTrustBundle allows a pod to access the `.spec.trustBundle` field
/// of ClusterTrustBundle objects in an auto-updating file.
/// 
/// Alpha, gated by the ClusterTrustBundleProjection feature gate.
/// 
/// ClusterTrustBundle objects can either be selected by name, or by the
/// combination of signer name and a label selector.
/// 
/// Kubelet performs aggressive normalization of the PEM contents written
/// into the pod filesystem.  Esoteric PEM features such as inter-block
/// comments and block headers are stripped.  Certificates are deduplicated.
/// The ordering of certificates within the file is arbitrary, and Kubelet
/// may change the order over time.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesClusterTrustBundle {
    /// Select all ClusterTrustBundles that match this label selector.  Only has
    /// effect if signerName is set.  Mutually-exclusive with name.  If unset,
    /// interpreted as "match nothing".  If set but empty, interpreted as "match
    /// everything".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "labelSelector")]
    pub label_selector: Option<SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelector>,
    /// Select a single ClusterTrustBundle by object name.  Mutually-exclusive
    /// with signerName and labelSelector.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// If true, don't block pod startup if the referenced ClusterTrustBundle(s)
    /// aren't available.  If using name, then the named ClusterTrustBundle is
    /// allowed not to exist.  If using signerName, then the combination of
    /// signerName and labelSelector is allowed to match zero
    /// ClusterTrustBundles.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
    /// Relative path from the volume root to write the bundle.
    pub path: String,
    /// Select all ClusterTrustBundles that match this signer name.
    /// Mutually-exclusive with name.  The contents of all selected
    /// ClusterTrustBundles will be unified and deduplicated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "signerName")]
    pub signer_name: Option<String>,
}

/// Select all ClusterTrustBundles that match this label selector.  Only has
/// effect if signerName is set.  Mutually-exclusive with name.  If unset,
/// interpreted as "match nothing".  If set but empty, interpreted as "match
/// everything".
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelector {
    /// matchExpressions is a list of label selector requirements. The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelectorMatchExpressions>>,
    /// matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels
    /// map is equivalent to an element of matchExpressions, whose key field is "key", the
    /// operator is "In", and the values array contains only "value". The requirements are ANDed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

/// A label selector requirement is a selector that contains values, a key, and an operator that
/// relates the key and values.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesClusterTrustBundleLabelSelectorMatchExpressions {
    /// key is the label key that the selector applies to.
    pub key: String,
    /// operator represents a key's relationship to a set of values.
    /// Valid operators are In, NotIn, Exists and DoesNotExist.
    pub operator: String,
    /// values is an array of string values. If the operator is In or NotIn,
    /// the values array must be non-empty. If the operator is Exists or DoesNotExist,
    /// the values array must be empty. This array is replaced during a strategic
    /// merge patch.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// configMap information about the configMap data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesConfigMap {
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// ConfigMap will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the ConfigMap,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesProjectedSourcesConfigMapItems>>,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional specify whether the ConfigMap or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesConfigMapItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// downwardAPI information about the downwardAPI data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApi {
    /// Items is a list of DownwardAPIVolume file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesProjectedSourcesDownwardApiItems>>,
}

/// DownwardAPIVolumeFile represents information to create the file containing the pod field
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApiItems {
    /// Required: Selects a field of the pod: only annotations, labels, name, namespace and uid are supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fieldRef")]
    pub field_ref: Option<SparkApplicationVolumesProjectedSourcesDownwardApiItemsFieldRef>,
    /// Optional: mode bits used to set permissions on this file, must be an octal value
    /// between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'
    pub path: String,
    /// Selects a resource of the container: only resources limits and requests
    /// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceFieldRef")]
    pub resource_field_ref: Option<SparkApplicationVolumesProjectedSourcesDownwardApiItemsResourceFieldRef>,
}

/// Required: Selects a field of the pod: only annotations, labels, name, namespace and uid are supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApiItemsFieldRef {
    /// Version of the schema the FieldPath is written in terms of, defaults to "v1".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    /// Path of the field to select in the specified API version.
    #[serde(rename = "fieldPath")]
    pub field_path: String,
}

/// Selects a resource of the container: only resources limits and requests
/// (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesDownwardApiItemsResourceFieldRef {
    /// Container name: required for volumes, optional for env vars
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// Specifies the output format of the exposed resources, defaults to "1"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub divisor: Option<IntOrString>,
    /// Required: resource to select
    pub resource: String,
}

/// secret information about the secret data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesSecret {
    /// items if unspecified, each key-value pair in the Data field of the referenced
    /// Secret will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the Secret,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesProjectedSourcesSecretItems>>,
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// optional field specify whether the Secret or its key must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesSecretItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// serviceAccountToken is information about the serviceAccountToken data to project
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesProjectedSourcesServiceAccountToken {
    /// audience is the intended audience of the token. A recipient of a token
    /// must identify itself with an identifier specified in the audience of the
    /// token, and otherwise should reject the token. The audience defaults to the
    /// identifier of the apiserver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub audience: Option<String>,
    /// expirationSeconds is the requested duration of validity of the service
    /// account token. As the token approaches expiration, the kubelet volume
    /// plugin will proactively rotate the service account token. The kubelet will
    /// start trying to rotate the token if the token is older than 80 percent of
    /// its time to live or if the token is older than 24 hours.Defaults to 1 hour
    /// and must be at least 10 minutes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "expirationSeconds")]
    pub expiration_seconds: Option<i64>,
    /// path is the path relative to the mount point of the file to project the
    /// token into.
    pub path: String,
}

/// quobyte represents a Quobyte mount on the host that shares a pod's lifetime.
/// Deprecated: Quobyte is deprecated and the in-tree quobyte type is no longer supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesQuobyte {
    /// group to map volume access to
    /// Default is no group
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub group: Option<String>,
    /// readOnly here will force the Quobyte volume to be mounted with read-only permissions.
    /// Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// registry represents a single or multiple Quobyte Registry services
    /// specified as a string as host:port pair (multiple entries are separated with commas)
    /// which acts as the central registry for volumes
    pub registry: String,
    /// tenant owning the given Quobyte volume in the Backend
    /// Used with dynamically provisioned Quobyte volumes, value is set by the plugin
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tenant: Option<String>,
    /// user to map volume access to
    /// Defaults to serivceaccount user
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
    /// volume is a string that references an already created Quobyte volume by name.
    pub volume: String,
}

/// rbd represents a Rados Block Device mount on the host that shares a pod's lifetime.
/// Deprecated: RBD is deprecated and the in-tree rbd type is no longer supported.
/// More info: https://examples.k8s.io/volumes/rbd/README.md
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesRbd {
    /// fsType is the filesystem type of the volume that you want to mount.
    /// Tip: Ensure that the filesystem type is supported by the host operating system.
    /// Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// image is the rados image name.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    pub image: String,
    /// keyring is the path to key ring for RBDUser.
    /// Default is /etc/ceph/keyring.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub keyring: Option<String>,
    /// monitors is a collection of Ceph monitors.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    pub monitors: Vec<String>,
    /// pool is the rados pool name.
    /// Default is rbd.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pool: Option<String>,
    /// readOnly here will force the ReadOnly setting in VolumeMounts.
    /// Defaults to false.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef is name of the authentication secret for RBDUser. If provided
    /// overrides keyring.
    /// Default is nil.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesRbdSecretRef>,
    /// user is the rados user name.
    /// Default is admin.
    /// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

/// secretRef is name of the authentication secret for RBDUser. If provided
/// overrides keyring.
/// Default is nil.
/// More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesRbdSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.
/// Deprecated: ScaleIO is deprecated and the in-tree scaleIO type is no longer supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesScaleIo {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs".
    /// Default is "xfs".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// gateway is the host address of the ScaleIO API Gateway.
    pub gateway: String,
    /// protectionDomain is the name of the ScaleIO Protection Domain for the configured storage.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "protectionDomain")]
    pub protection_domain: Option<String>,
    /// readOnly Defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef references to the secret for ScaleIO user and other
    /// sensitive information. If this is not provided, Login operation will fail.
    #[serde(rename = "secretRef")]
    pub secret_ref: SparkApplicationVolumesScaleIoSecretRef,
    /// sslEnabled Flag enable/disable SSL communication with Gateway, default false
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sslEnabled")]
    pub ssl_enabled: Option<bool>,
    /// storageMode indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned.
    /// Default is ThinProvisioned.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storageMode")]
    pub storage_mode: Option<String>,
    /// storagePool is the ScaleIO Storage Pool associated with the protection domain.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePool")]
    pub storage_pool: Option<String>,
    /// system is the name of the storage system as configured in ScaleIO.
    pub system: String,
    /// volumeName is the name of a volume already created in the ScaleIO system
    /// that is associated with this volume source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
}

/// secretRef references to the secret for ScaleIO user and other
/// sensitive information. If this is not provided, Login operation will fail.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesScaleIoSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// secret represents a secret that should populate this volume.
/// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesSecret {
    /// defaultMode is Optional: mode bits used to set permissions on created files by default.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values
    /// for mode bits. Defaults to 0644.
    /// Directories within the path are not affected by this setting.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "defaultMode")]
    pub default_mode: Option<i32>,
    /// items If unspecified, each key-value pair in the Data field of the referenced
    /// Secret will be projected into the volume as a file whose name is the
    /// key and content is the value. If specified, the listed keys will be
    /// projected into the specified paths, and unlisted keys will not be
    /// present. If a key is specified which is not present in the Secret,
    /// the volume setup will error unless it is marked optional. Paths must be
    /// relative and may not contain the '..' path or start with '..'.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub items: Option<Vec<SparkApplicationVolumesSecretItems>>,
    /// optional field specify whether the Secret or its keys must be defined
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub optional: Option<bool>,
    /// secretName is the name of the secret in the pod's namespace to use.
    /// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretName")]
    pub secret_name: Option<String>,
}

/// Maps a string key to a path within a volume.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesSecretItems {
    /// key is the key to project.
    pub key: String,
    /// mode is Optional: mode bits used to set permissions on this file.
    /// Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511.
    /// YAML accepts both octal and decimal values, JSON requires decimal values for mode bits.
    /// If not specified, the volume defaultMode will be used.
    /// This might be in conflict with other options that affect the file
    /// mode, like fsGroup, and the result can be other mode bits set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub mode: Option<i32>,
    /// path is the relative path of the file to map the key to.
    /// May not be an absolute path.
    /// May not contain the path element '..'.
    /// May not start with the string '..'.
    pub path: String,
}

/// storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.
/// Deprecated: StorageOS is deprecated and the in-tree storageos type is no longer supported.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesStorageos {
    /// fsType is the filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// readOnly defaults to false (read/write). ReadOnly here will force
    /// the ReadOnly setting in VolumeMounts.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readOnly")]
    pub read_only: Option<bool>,
    /// secretRef specifies the secret to use for obtaining the StorageOS API
    /// credentials.  If not specified, default values will be attempted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretRef")]
    pub secret_ref: Option<SparkApplicationVolumesStorageosSecretRef>,
    /// volumeName is the human-readable name of the StorageOS volume.  Volume
    /// names are only unique within a namespace.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeName")]
    pub volume_name: Option<String>,
    /// volumeNamespace specifies the scope of the volume within StorageOS.  If no
    /// namespace is specified then the Pod's namespace will be used.  This allows the
    /// Kubernetes name scoping to be mirrored within StorageOS for tighter integration.
    /// Set VolumeName to any name to override the default behaviour.
    /// Set to "default" if you are not using namespaces within StorageOS.
    /// Namespaces that do not pre-exist within StorageOS will be created.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "volumeNamespace")]
    pub volume_namespace: Option<String>,
}

/// secretRef specifies the secret to use for obtaining the StorageOS API
/// credentials.  If not specified, default values will be attempted.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesStorageosSecretRef {
    /// Name of the referent.
    /// This field is effectively required, but due to backwards compatibility is
    /// allowed to be empty. Instances of this type with an empty value here are
    /// almost certainly wrong.
    /// More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine.
/// Deprecated: VsphereVolume is deprecated. All operations for the in-tree vsphereVolume type
/// are redirected to the csi.vsphere.vmware.com CSI driver.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationVolumesVsphereVolume {
    /// fsType is filesystem type to mount.
    /// Must be a filesystem type supported by the host operating system.
    /// Ex. "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fsType")]
    pub fs_type: Option<String>,
    /// storagePolicyID is the storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePolicyID")]
    pub storage_policy_id: Option<String>,
    /// storagePolicyName is the storage Policy Based Management (SPBM) profile name.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "storagePolicyName")]
    pub storage_policy_name: Option<String>,
    /// volumePath is the path that identifies vSphere volume vmdk
    #[serde(rename = "volumePath")]
    pub volume_path: String,
}

/// SparkApplicationStatus defines the observed state of SparkApplication
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationStatus {
    /// AppState tells the overall application state.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "applicationState")]
    pub application_state: Option<SparkApplicationStatusApplicationState>,
    /// DriverInfo has information about the driver.
    #[serde(rename = "driverInfo")]
    pub driver_info: SparkApplicationStatusDriverInfo,
    /// ExecutionAttempts is the total number of attempts to run a submitted application to completion.
    /// Incremented upon each attempted run of the application and reset upon invalidation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionAttempts")]
    pub execution_attempts: Option<i32>,
    /// ExecutorState records the state of executors by executor Pod names.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executorState")]
    pub executor_state: Option<BTreeMap<String, String>>,
    /// LastSubmissionAttemptTime is the time for the last application submission attempt.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lastSubmissionAttemptTime")]
    pub last_submission_attempt_time: Option<String>,
    /// SparkApplicationID is set by the spark-distribution(via spark.app.id config) on the driver and executor pods
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkApplicationId")]
    pub spark_application_id: Option<String>,
    /// SubmissionAttempts is the total number of attempts to submit an application to run.
    /// Incremented upon each attempted submission of the application and reset upon invalidation and rerun.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "submissionAttempts")]
    pub submission_attempts: Option<i32>,
    /// SubmissionID is a unique ID of the current submission of the application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "submissionID")]
    pub submission_id: Option<String>,
    /// CompletionTime is the time when the application runs to completion if it does.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terminationTime")]
    pub termination_time: Option<String>,
}

/// AppState tells the overall application state.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationStatusApplicationState {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorMessage")]
    pub error_message: Option<String>,
    /// ApplicationStateType represents the type of the current state of an application.
    pub state: String,
}

/// DriverInfo has information about the driver.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct SparkApplicationStatusDriverInfo {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "podName")]
    pub pod_name: Option<String>,
    /// UI Details for the UI created via ClusterIP service accessible from within the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIAddress")]
    pub web_ui_address: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIIngressAddress")]
    pub web_ui_ingress_address: Option<String>,
    /// Ingress Details if an ingress for the UI was created.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIIngressName")]
    pub web_ui_ingress_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIPort")]
    pub web_ui_port: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "webUIServiceName")]
    pub web_ui_service_name: Option<String>,
}

