// WARNING: generated by kopium - manual changes will be overwritten
// kopium command: kopium --docs --derive=Default --derive=PartialEq --smart-derive-elision --filename crd-catalog/netobserv/network-observability-operator/flows.netobserv.io/v1beta1/flowcollectors.yaml
// kopium version: 0.21.1

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::BTreeMap;
    pub use k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// Defines the desired state of the FlowCollector resource.
/// <br><br>
/// *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature
/// is not officially supported by Red Hat. It might have been, for example, contributed by the community
/// and accepted without a formal agreement for maintenance. The product maintainers might provide some support
/// for these features as a best effort only.
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
#[kube(group = "flows.netobserv.io", version = "v1beta1", kind = "FlowCollector", plural = "flowcollectors")]
#[kube(status = "FlowCollectorStatus")]
#[kube(schema = "disabled")]
#[kube(derive="Default")]
#[kube(derive="PartialEq")]
pub struct FlowCollectorSpec {
    /// Agent configuration for flows extraction.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub agent: Option<FlowCollectorAgent>,
    /// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consolePlugin")]
    pub console_plugin: Option<FlowCollectorConsolePlugin>,
    /// `deploymentModel` defines the desired type of deployment for flow processing. Possible values are:<br>
    /// - `DIRECT` (default) to make the flow processor listening directly from the agents.<br>
    /// - `KAFKA` to make flows sent to a Kafka pipeline before consumption by the processor.<br>
    /// Kafka can provide better scalability, resiliency, and high availability (for more details, see https://www.redhat.com/en/topics/integration/what-is-apache-kafka).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deploymentModel")]
    pub deployment_model: Option<FlowCollectorDeploymentModel>,
    /// `exporters` define additional optional exporters for custom consumption or storage.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub exporters: Option<Vec<FlowCollectorExporters>>,
    /// Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the `spec.deploymentModel` is `KAFKA`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kafka: Option<FlowCollectorKafka>,
    /// `loki`, the flow store, client settings.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub loki: Option<FlowCollectorLoki>,
    /// Namespace where NetObserv pods are deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// `processor` defines the settings of the component that receives the flows from the agent,
    /// enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub processor: Option<FlowCollectorProcessor>,
    /// `prometheus` defines Prometheus settings, such as querier configuration used to fetch metrics from the Console plugin.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prometheus: Option<FlowCollectorPrometheus>,
}

/// Agent configuration for flows extraction.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgent {
    /// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type`
    /// is set to `EBPF`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ebpf: Option<FlowCollectorAgentEbpf>,
    /// `ipfix` [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when `spec.agent.type`
    /// is set to `IPFIX`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ipfix: Option<FlowCollectorAgentIpfix>,
    /// `type` [deprecated (*)] selects the flows tracing agent. The only possible value is `EBPF` (default), to use NetObserv eBPF agent.<br>
    /// Previously, using an IPFIX collector was allowed, but was deprecated and it is now removed.<br>
    /// Setting `IPFIX` is ignored and still use the eBPF Agent.
    /// Since there is only a single option here, this field will be remove in a future API version.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentType>,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type`
/// is set to `EBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpf {
    /// `cacheActiveTimeout` is the max period during which the reporter aggregates flows before sending.
    /// Increasing `cacheMaxFlows` and `cacheActiveTimeout` can decrease the network traffic overhead and the CPU load,
    /// however you can expect higher memory consumption and an increased latency in the flow collection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheActiveTimeout")]
    pub cache_active_timeout: Option<String>,
    /// `cacheMaxFlows` is the max number of flows in an aggregate; when reached, the reporter sends the flows.
    /// Increasing `cacheMaxFlows` and `cacheActiveTimeout` can decrease the network traffic overhead and the CPU load,
    /// however you can expect higher memory consumption and an increased latency in the flow collection.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheMaxFlows")]
    pub cache_max_flows: Option<i32>,
    /// `debug` allows setting some aspects of the internal configuration of the eBPF agent.
    /// This section is aimed exclusively for debugging and fine-grained performance optimizations,
    /// such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub debug: Option<FlowCollectorAgentEbpfDebug>,
    /// `excludeInterfaces` contains the interface names that are excluded from flow tracing.
    /// An entry enclosed by slashes, such as `/br-/`, is matched as a regular expression.
    /// Otherwise it is matched as a case-sensitive string.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "excludeInterfaces")]
    pub exclude_interfaces: Option<Vec<String>>,
    /// List of additional features to enable. They are all disabled by default. Enabling additional features might have performance impacts. Possible values are:<br>
    /// - `PacketDrop`: enable the packets drop flows logging feature. This feature requires mounting
    /// the kernel debug filesystem, so the eBPF pod has to run as privileged.
    /// If the `spec.agent.ebpf.privileged` parameter is not set, an error is reported.<br>
    /// - `DNSTracking`: enable the DNS tracking feature.<br>
    /// - `FlowRTT`: enable flow latency (sRTT) extraction in the eBPF agent from TCP traffic.<br>
    /// - `NetworkEvents`: enable the Network events monitoring feature. This feature requires mounting
    /// the kernel debug filesystem, so the eBPF pod has to run as privileged.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub features: Option<Vec<String>>,
    /// `flowFilter` defines the eBPF agent configuration regarding flow filtering
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "flowFilter")]
    pub flow_filter: Option<FlowCollectorAgentEbpfFlowFilter>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorAgentEbpfImagePullPolicy>,
    /// `interfaces` contains the interface names from where flows are collected. If empty, the agent
    /// fetches all the interfaces in the system, excepting the ones listed in ExcludeInterfaces.
    /// An entry enclosed by slashes, such as `/br-/`, is matched as a regular expression.
    /// Otherwise it is matched as a case-sensitive string.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub interfaces: Option<Vec<String>>,
    /// `kafkaBatchSize` limits the maximum size of a request in bytes before being sent to a partition. Ignored when not using Kafka. Default: 1MB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaBatchSize")]
    pub kafka_batch_size: Option<i64>,
    /// `logLevel` defines the log level for the NetObserv eBPF Agent
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorAgentEbpfLogLevel>,
    /// `metrics` defines the eBPF agent configuration regarding metrics
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<FlowCollectorAgentEbpfMetrics>,
    /// Privileged mode for the eBPF Agent container. When ignored or set to `false`, the operator sets
    /// granular capabilities (BPF, PERFMON, NET_ADMIN, SYS_RESOURCE) to the container.
    /// If for some reason these capabilities cannot be set, such as if an old kernel version not knowing CAP_BPF
    /// is in use, then you can turn on this mode for more global privileges.
    /// Some agent features require the privileged mode, such as packet drops tracking (see `features`) and SR-IOV support.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub privileged: Option<bool>,
    /// `resources` are the compute resources required by this container.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorAgentEbpfResources>,
    /// Sampling rate of the flow reporter. 100 means one flow on 100 is sent. 0 or 1 means all flows are sampled.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sampling: Option<i32>,
}

/// `debug` allows setting some aspects of the internal configuration of the eBPF agent.
/// This section is aimed exclusively for debugging and fine-grained performance optimizations,
/// such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfDebug {
    /// `env` allows passing custom environment variables to underlying components. Useful for passing
    /// some very concrete performance-tuning options, such as `GOGC` and `GOMAXPROCS`, that should not be
    /// publicly exposed as part of the FlowCollector descriptor, as they are only useful
    /// in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<BTreeMap<String, String>>,
}

/// `flowFilter` defines the eBPF agent configuration regarding flow filtering
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfFlowFilter {
    /// Action defines the action to perform on the flows that match the filter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub action: Option<FlowCollectorAgentEbpfFlowFilterAction>,
    /// CIDR defines the IP CIDR to filter flows by.
    /// Example: 10.10.10.0/24 or 100:100:100:100::/64
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cidr: Option<String>,
    /// DestPorts defines the destination ports to filter flows by.
    /// To filter a single port, set a single port as an integer value. For example, destPorts: 80.
    /// To filter a range of ports, use a "start-end" range in string format. For example, destPorts: "80-100".
    /// To filter two ports, use a "port1,port2" in string format. For example, `ports: "80,100"`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destPorts")]
    pub dest_ports: Option<IntOrString>,
    /// Direction defines the direction to filter flows by.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub direction: Option<FlowCollectorAgentEbpfFlowFilterDirection>,
    /// Set `enable` to `true` to enable eBPF flow filtering feature.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// ICMPCode defines the ICMP code to filter flows by.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "icmpCode")]
    pub icmp_code: Option<i64>,
    /// ICMPType defines the ICMP type to filter flows by.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "icmpType")]
    pub icmp_type: Option<i64>,
    /// PeerIP defines the IP address to filter flows by.
    /// Example: 10.10.10.10
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "peerIP")]
    pub peer_ip: Option<String>,
    /// `pktDrops`, to filter flows with packet drops
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pktDrops")]
    pub pkt_drops: Option<bool>,
    /// Ports defines the ports to filter flows by. it can be user for either source or destination ports.
    /// To filter a single port, set a single port as an integer value. For example, ports: 80.
    /// To filter a range of ports, use a "start-end" range in string format. For example, ports: "80-100".
    /// To filter two ports, use a "port1,port2" in string format. For example, `ports: "80,100"`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ports: Option<IntOrString>,
    /// Protocol defines the protocol to filter flows by.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub protocol: Option<FlowCollectorAgentEbpfFlowFilterProtocol>,
    /// SourcePorts defines the source ports to filter flows by.
    /// To filter a single port, set a single port as an integer value. For example, sourcePorts: 80.
    /// To filter a range of ports, use a "start-end" range in string format. For example, sourcePorts: "80-100".
    /// To filter two ports, use a "port1,port2" in string format. For example, `ports: "80,100"`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourcePorts")]
    pub source_ports: Option<IntOrString>,
    /// `tcpFlags` defines the TCP flags to filter flows by.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tcpFlags")]
    pub tcp_flags: Option<FlowCollectorAgentEbpfFlowFilterTcpFlags>,
}

/// `flowFilter` defines the eBPF agent configuration regarding flow filtering
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfFlowFilterAction {
    Accept,
    Reject,
}

/// `flowFilter` defines the eBPF agent configuration regarding flow filtering
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfFlowFilterDirection {
    Ingress,
    Egress,
}

/// `flowFilter` defines the eBPF agent configuration regarding flow filtering
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfFlowFilterProtocol {
    #[serde(rename = "TCP")]
    Tcp,
    #[serde(rename = "UDP")]
    Udp,
    #[serde(rename = "ICMP")]
    Icmp,
    #[serde(rename = "ICMPv6")]
    IcmPv6,
    #[serde(rename = "SCTP")]
    Sctp,
}

/// `flowFilter` defines the eBPF agent configuration regarding flow filtering
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfFlowFilterTcpFlags {
    #[serde(rename = "SYN")]
    Syn,
    #[serde(rename = "SYN-ACK")]
    SynAck,
    #[serde(rename = "ACK")]
    Ack,
    #[serde(rename = "FIN")]
    Fin,
    #[serde(rename = "RST")]
    Rst,
    #[serde(rename = "URG")]
    Urg,
    #[serde(rename = "ECE")]
    Ece,
    #[serde(rename = "CWR")]
    Cwr,
    #[serde(rename = "FIN-ACK")]
    FinAck,
    #[serde(rename = "RST-ACK")]
    RstAck,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type`
/// is set to `EBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `ebpf` describes the settings related to the eBPF-based flow reporter when `spec.agent.type`
/// is set to `EBPF`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `metrics` defines the eBPF agent configuration regarding metrics
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfMetrics {
    /// `disableAlerts` is a list of alerts that should be disabled.
    /// Possible values are:<br>
    /// `NetObservDroppedFlows`, which is triggered when the eBPF agent is missing packets or flows, such as when the BPF hashmap is busy or full, or the capacity limiter being triggered.<br>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "disableAlerts")]
    pub disable_alerts: Option<Vec<String>>,
    /// Set `enable` to `false` to disable eBPF agent metrics collection, by default it's `true`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// Metrics server endpoint configuration for Prometheus scraper
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<FlowCollectorAgentEbpfMetricsServer>,
}

/// Metrics server endpoint configuration for Prometheus scraper
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServer {
    /// The prometheus HTTP port
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// TLS configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorAgentEbpfMetricsServerTls>,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServerTls {
    /// `insecureSkipVerify` allows skipping client-side verification of the provided certificate.
    /// If set to `true`, the `providedCaFile` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// TLS configuration when `type` is set to `PROVIDED`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub provided: Option<FlowCollectorAgentEbpfMetricsServerTlsProvided>,
    /// Reference to the CA file when `type` is set to `PROVIDED`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providedCaFile")]
    pub provided_ca_file: Option<FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFile>,
    /// Select the type of TLS configuration:<br>
    /// - `DISABLED` (default) to not configure TLS for the endpoint.
    /// - `PROVIDED` to manually provide cert file and a key file. [Unsupported (*)].
    /// - `AUTO` to use OpenShift auto generated certificate using annotations.
    #[serde(rename = "type")]
    pub r#type: FlowCollectorAgentEbpfMetricsServerTlsType,
}

/// TLS configuration when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServerTlsProvided {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentEbpfMetricsServerTlsProvidedType>,
}

/// TLS configuration when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfMetricsServerTlsProvidedType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the CA file when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFile {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFileType>,
}

/// Reference to the CA file when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfMetricsServerTlsProvidedCaFileType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentEbpfMetricsServerTlsType {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "PROVIDED")]
    Provided,
    #[serde(rename = "AUTO")]
    Auto,
}

/// `resources` are the compute resources required by this container.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorAgentEbpfResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentEbpfResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
    /// Request is the name chosen for a request in the referenced claim.
    /// If empty, everything from the claim is made available, otherwise
    /// only the result of this request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub request: Option<String>,
}

/// `ipfix` [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when `spec.agent.type`
/// is set to `IPFIX`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentIpfix {
    /// `cacheActiveTimeout` is the max period during which the reporter aggregates flows before sending.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheActiveTimeout")]
    pub cache_active_timeout: Option<String>,
    /// `cacheMaxFlows` is the max number of flows in an aggregate; when reached, the reporter sends the flows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cacheMaxFlows")]
    pub cache_max_flows: Option<i32>,
    /// `clusterNetworkOperator` defines the settings related to the OpenShift Cluster Network Operator, when available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterNetworkOperator")]
    pub cluster_network_operator: Option<FlowCollectorAgentIpfixClusterNetworkOperator>,
    /// `forceSampleAll` allows disabling sampling in the IPFIX-based flow reporter.
    /// It is not recommended to sample all the traffic with IPFIX, as it might generate cluster instability.
    /// If you REALLY want to do that, set this flag to `true`. Use at your own risk.
    /// When it is set to `true`, the value of `sampling` is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "forceSampleAll")]
    pub force_sample_all: Option<bool>,
    /// `ovnKubernetes` defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN's IPFIX exports, without OpenShift. When using OpenShift, refer to the `clusterNetworkOperator` property instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ovnKubernetes")]
    pub ovn_kubernetes: Option<FlowCollectorAgentIpfixOvnKubernetes>,
    /// `sampling` is the sampling rate on the reporter. 100 means one flow on 100 is sent.
    /// To ensure cluster stability, it is not possible to set a value below 2.
    /// If you really want to sample every packet, which might impact the cluster stability,
    /// refer to `forceSampleAll`. Alternatively, you can use the eBPF Agent instead of IPFIX.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sampling: Option<i32>,
}

/// `clusterNetworkOperator` defines the settings related to the OpenShift Cluster Network Operator, when available.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentIpfixClusterNetworkOperator {
    /// Namespace  where the config map is going to be deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// `ovnKubernetes` defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN's IPFIX exports, without OpenShift. When using OpenShift, refer to the `clusterNetworkOperator` property instead.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorAgentIpfixOvnKubernetes {
    /// `containerName` defines the name of the container to configure for IPFIX.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerName")]
    pub container_name: Option<String>,
    /// `daemonSetName` defines the name of the DaemonSet controlling the OVN-Kubernetes pods.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "daemonSetName")]
    pub daemon_set_name: Option<String>,
    /// Namespace where OVN-Kubernetes pods are deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

/// Agent configuration for flows extraction.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorAgentType {
    #[serde(rename = "EBPF")]
    Ebpf,
    #[serde(rename = "IPFIX")]
    Ipfix,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePlugin {
    /// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub autoscaler: Option<FlowCollectorConsolePluginAutoscaler>,
    /// Enables the console plugin deployment.
    /// `spec.loki.enable` must also be `true`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorConsolePluginImagePullPolicy>,
    /// `logLevel` for the console plugin backend
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorConsolePluginLogLevel>,
    /// `port` is the plugin service port. Do not use 9002, which is reserved for metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// `portNaming` defines the configuration of the port-to-service name translation
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portNaming")]
    pub port_naming: Option<FlowCollectorConsolePluginPortNaming>,
    /// `quickFilters` configures quick filter presets for the Console plugin
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "quickFilters")]
    pub quick_filters: Option<Vec<FlowCollectorConsolePluginQuickFilters>>,
    /// `register` allows, when set to `true`, to automatically register the provided console plugin with the OpenShift Console operator.
    /// When set to `false`, you can still register it manually by editing console.operator.openshift.io/cluster with the following command:
    /// `oc patch console.operator.openshift.io cluster --type='json' -p '[{"op": "add", "path": "/spec/plugins/-", "value": "netobserv-plugin"}]'`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub register: Option<bool>,
    /// `replicas` defines the number of replicas (pods) to start.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub replicas: Option<i32>,
    /// `resources`, in terms of compute resources, required by this container.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorConsolePluginResources>,
}

/// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscaler {
    /// `maxReplicas` is the upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than MinReplicas.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxReplicas")]
    pub max_replicas: Option<i32>,
    /// Metrics used by the pod autoscaler. For documentation, refer to https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<FlowCollectorConsolePluginAutoscalerMetrics>>,
    /// `minReplicas` is the lower limit for the number of replicas to which the autoscaler
    /// can scale down. It defaults to 1 pod. minReplicas is allowed to be 0 if the
    /// alpha feature gate HPAScaleToZero is enabled and at least one Object or External
    /// metric is configured. Scaling is active as long as at least one metric value is
    /// available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minReplicas")]
    pub min_replicas: Option<i32>,
    /// `status` describes the desired status regarding deploying an horizontal pod autoscaler.<br>
    /// - `DISABLED` does not deploy an horizontal pod autoscaler.<br>
    /// - `ENABLED` deploys an horizontal pod autoscaler.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub status: Option<FlowCollectorConsolePluginAutoscalerStatus>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetrics {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerResource")]
    pub container_resource: Option<FlowCollectorConsolePluginAutoscalerMetricsContainerResource>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub external: Option<FlowCollectorConsolePluginAutoscalerMetricsExternal>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub object: Option<FlowCollectorConsolePluginAutoscalerMetricsObject>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pods: Option<FlowCollectorConsolePluginAutoscalerMetricsPods>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resource: Option<FlowCollectorConsolePluginAutoscalerMetricsResource>,
    #[serde(rename = "type")]
    pub r#type: String,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsContainerResource {
    pub container: String,
    pub name: String,
    pub target: FlowCollectorConsolePluginAutoscalerMetricsContainerResourceTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsContainerResourceTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternal {
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsExternalMetric,
    pub target: FlowCollectorConsolePluginAutoscalerMetricsExternalTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetric {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelector>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelector {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelectorMatchExpressions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalMetricSelectorMatchExpressions {
    pub key: String,
    pub operator: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsExternalTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObject {
    #[serde(rename = "describedObject")]
    pub described_object: FlowCollectorConsolePluginAutoscalerMetricsObjectDescribedObject,
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsObjectMetric,
    pub target: FlowCollectorConsolePluginAutoscalerMetricsObjectTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectDescribedObject {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    pub kind: String,
    pub name: String,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetric {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelector>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelector {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelectorMatchExpressions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectMetricSelectorMatchExpressions {
    pub key: String,
    pub operator: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsObjectTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPods {
    pub metric: FlowCollectorConsolePluginAutoscalerMetricsPodsMetric,
    pub target: FlowCollectorConsolePluginAutoscalerMetricsPodsTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetric {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelector>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelector {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelectorMatchExpressions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsMetricSelectorMatchExpressions {
    pub key: String,
    pub operator: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsPodsTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsResource {
    pub name: String,
    pub target: FlowCollectorConsolePluginAutoscalerMetricsResourceTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginAutoscalerMetricsResourceTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// `autoscaler` spec of a horizontal pod autoscaler to set up for the plugin Deployment.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginAutoscalerStatus {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "ENABLED")]
    Enabled,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `consolePlugin` defines the settings related to the OpenShift Console plugin, when available.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorConsolePluginLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `portNaming` defines the configuration of the port-to-service name translation
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginPortNaming {
    /// Enable the console plugin port-to-service name translation
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `portNames` defines additional port names to use in the console,
    /// for example, `portNames: {"3100": "loki"}`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "portNames")]
    pub port_names: Option<BTreeMap<String, String>>,
}

/// `QuickFilter` defines preset configuration for Console's quick filters
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginQuickFilters {
    /// `default` defines whether this filter should be active by default or not
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub default: Option<bool>,
    /// `filter` is a set of keys and values to be set when this filter is selected. Each key can relate to a list of values using a coma-separated string,
    /// for example, `filter: {"src_namespace": "namespace1,namespace2"}`.
    pub filter: BTreeMap<String, String>,
    /// Name of the filter, that is displayed in the Console
    pub name: String,
}

/// `resources`, in terms of compute resources, required by this container.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorConsolePluginResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorConsolePluginResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
    /// Request is the name chosen for a request in the referenced claim.
    /// If empty, everything from the claim is made available, otherwise
    /// only the result of this request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub request: Option<String>,
}

/// Defines the desired state of the FlowCollector resource.
/// <br><br>
/// *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature
/// is not officially supported by Red Hat. It might have been, for example, contributed by the community
/// and accepted without a formal agreement for maintenance. The product maintainers might provide some support
/// for these features as a best effort only.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorDeploymentModel {
    #[serde(rename = "DIRECT")]
    Direct,
    #[serde(rename = "KAFKA")]
    Kafka,
}

/// `FlowCollectorExporter` defines an additional exporter to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorExporters {
    /// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ipfix: Option<FlowCollectorExportersIpfix>,
    /// Kafka configuration, such as the address and topic, to send enriched flows to.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kafka: Option<FlowCollectorExportersKafka>,
    /// `type` selects the type of exporters. The available options are `KAFKA` and `IPFIX`.
    #[serde(rename = "type")]
    pub r#type: FlowCollectorExportersType,
}

/// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersIpfix {
    /// Address of the IPFIX external receiver
    #[serde(rename = "targetHost")]
    pub target_host: String,
    /// Port for the IPFIX external receiver
    #[serde(rename = "targetPort")]
    pub target_port: i64,
    /// Transport protocol (`TCP` or `UDP`) to be used for the IPFIX connection, defaults to `TCP`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub transport: Option<FlowCollectorExportersIpfixTransport>,
}

/// IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersIpfixTransport {
    #[serde(rename = "TCP")]
    Tcp,
    #[serde(rename = "UDP")]
    Udp,
}

/// Kafka configuration, such as the address and topic, to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersKafka {
    /// Address of the Kafka server
    pub address: String,
    /// SASL authentication configuration. [Unsupported (*)].
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl: Option<FlowCollectorExportersKafkaSasl>,
    /// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorExportersKafkaTls>,
    /// Kafka topic to use. It must exist. NetObserv does not create it.
    pub topic: String,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersKafkaSasl {
    /// Reference to the secret or config map containing the client ID
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientIDReference")]
    pub client_id_reference: Option<FlowCollectorExportersKafkaSaslClientIdReference>,
    /// Reference to the secret or config map containing the client secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientSecretReference")]
    pub client_secret_reference: Option<FlowCollectorExportersKafkaSaslClientSecretReference>,
    /// Type of SASL authentication to use, or `DISABLED` if SASL is not used
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersKafkaSaslClientIdReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslClientIdReferenceType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslClientIdReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersKafkaSaslClientSecretReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaSaslClientSecretReferenceType>,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslClientSecretReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaSaslType {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "PLAIN")]
    Plain,
    #[serde(rename = "SCRAM-SHA512")]
    ScramSha512,
}

/// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersKafkaTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorExportersKafkaTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate.
    /// If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorExportersKafkaTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersKafkaTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorExportersKafkaTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorExportersKafkaTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersKafkaTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `FlowCollectorExporter` defines an additional exporter to send enriched flows to.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorExportersType {
    #[serde(rename = "KAFKA")]
    Kafka,
    #[serde(rename = "IPFIX")]
    Ipfix,
}

/// Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the `spec.deploymentModel` is `KAFKA`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorKafka {
    /// Address of the Kafka server
    pub address: String,
    /// SASL authentication configuration. [Unsupported (*)].
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl: Option<FlowCollectorKafkaSasl>,
    /// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorKafkaTls>,
    /// Kafka topic to use. It must exist. NetObserv does not create it.
    pub topic: String,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorKafkaSasl {
    /// Reference to the secret or config map containing the client ID
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientIDReference")]
    pub client_id_reference: Option<FlowCollectorKafkaSaslClientIdReference>,
    /// Reference to the secret or config map containing the client secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientSecretReference")]
    pub client_secret_reference: Option<FlowCollectorKafkaSaslClientSecretReference>,
    /// Type of SASL authentication to use, or `DISABLED` if SASL is not used
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorKafkaSaslClientIdReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslClientIdReferenceType>,
}

/// Reference to the secret or config map containing the client ID
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslClientIdReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorKafkaSaslClientSecretReference {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaSaslClientSecretReferenceType>,
}

/// Reference to the secret or config map containing the client secret
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslClientSecretReferenceType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// SASL authentication configuration. [Unsupported (*)].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaSaslType {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "PLAIN")]
    Plain,
    #[serde(rename = "SCRAM-SHA512")]
    ScramSha512,
}

/// TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorKafkaTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorKafkaTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate.
    /// If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorKafkaTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorKafkaTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorKafkaTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorKafkaTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorKafkaTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `loki`, the flow store, client settings.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorLoki {
    /// `authToken` describes the way to get a token to authenticate to Loki.<br>
    /// - `DISABLED` does not send any token with the request.<br>
    /// - `FORWARD` forwards the user token for authorization.<br>
    /// - `HOST` [deprecated (*)] - uses the local pod service account to authenticate to Loki.<br>
    /// When using the Loki Operator, this must be set to `FORWARD`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "authToken")]
    pub auth_token: Option<FlowCollectorLokiAuthToken>,
    /// `batchSize` is the maximum batch size (in bytes) of logs to accumulate before sending.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<i64>,
    /// `batchWait` is the maximum time to wait before sending a batch.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchWait")]
    pub batch_wait: Option<String>,
    /// Set `enable` to `true` to store flows in Loki.
    /// The Console plugin can use either Loki or Prometheus as a data source for metrics (see also `spec.prometheus.querier`), or both.
    /// Not all queries are transposable from Loki to Prometheus. Hence, if Loki is disabled, some features of the plugin are disabled as well,
    /// such as getting per-pod information or viewing raw flows.
    /// If both Prometheus and Loki are enabled, Prometheus takes precedence and Loki is used as a fallback for queries that Prometheus cannot handle.
    /// If they are both disabled, the Console plugin is not deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `maxBackoff` is the maximum backoff time for client connection between retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxBackoff")]
    pub max_backoff: Option<String>,
    /// `maxRetries` is the maximum number of retries for client connections.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxRetries")]
    pub max_retries: Option<i32>,
    /// `minBackoff` is the initial backoff time for client connection between retries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minBackoff")]
    pub min_backoff: Option<String>,
    /// `querierURL` specifies the address of the Loki querier service, in case it is different from the
    /// Loki ingester URL. If empty, the URL value is used (assuming that the Loki ingester
    /// and querier are in the same server). When using the Loki Operator, do not set it, since
    /// ingestion and queries use the Loki gateway.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "querierUrl")]
    pub querier_url: Option<String>,
    /// `readTimeout` is the maximum loki query total time limit.
    /// A timeout of zero means no timeout.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "readTimeout")]
    pub read_timeout: Option<String>,
    /// `staticLabels` is a map of common labels to set on each flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "staticLabels")]
    pub static_labels: Option<BTreeMap<String, String>>,
    /// TLS client configuration for Loki status URL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusTls")]
    pub status_tls: Option<FlowCollectorLokiStatusTls>,
    /// `statusURL` specifies the address of the Loki `/ready`, `/metrics` and `/config` endpoints, in case it is different from the
    /// Loki querier URL. If empty, the `querierURL` value is used.
    /// This is useful to show error messages and some context in the frontend.
    /// When using the Loki Operator, set it to the Loki HTTP query frontend service, for example
    /// https://loki-query-frontend-http.netobserv.svc:3100/.
    /// `statusTLS` configuration is used when `statusUrl` is set.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statusUrl")]
    pub status_url: Option<String>,
    /// `tenantID` is the Loki `X-Scope-OrgID` that identifies the tenant for each request.
    /// When using the Loki Operator, set it to `network`, which corresponds to a special tenant mode.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tenantID")]
    pub tenant_id: Option<String>,
    /// `timeout` is the maximum processor time connection / request limit.
    /// A timeout of zero means no timeout.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timeout: Option<String>,
    /// TLS client configuration for Loki URL.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorLokiTls>,
    /// `url` is the address of an existing Loki service to push the flows to. When using the Loki Operator,
    /// set it to the Loki gateway service with the `network` tenant set in path, for example
    /// https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub url: Option<String>,
}

/// `loki`, the flow store, client settings.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiAuthToken {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "HOST")]
    Host,
    #[serde(rename = "FORWARD")]
    Forward,
}

/// TLS client configuration for Loki status URL.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorLokiStatusTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiStatusTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate.
    /// If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiStatusTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorLokiStatusTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiStatusTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiStatusTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorLokiStatusTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiStatusTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiStatusTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS client configuration for Loki URL.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorLokiTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorLokiTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate.
    /// If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorLokiTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorLokiTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorLokiTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorLokiTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorLokiTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `processor` defines the settings of the component that receives the flows from the agent,
/// enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessor {
    /// `addZone` allows availability zone awareness by labelling flows with their source and destination zones.
    /// This feature requires the "topology.kubernetes.io/zone" label to be set on nodes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "addZone")]
    pub add_zone: Option<bool>,
    /// `clusterName` is the name of the cluster to appear in the flows data. This is useful in a multi-cluster context. When using OpenShift, leave empty to make it automatically determined.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    pub cluster_name: Option<String>,
    /// `conversationEndTimeout` is the time to wait after a network flow is received, to consider the conversation ended.
    /// This delay is ignored when a FIN packet is collected for TCP flows (see `conversationTerminatingTimeout` instead).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationEndTimeout")]
    pub conversation_end_timeout: Option<String>,
    /// `conversationHeartbeatInterval` is the time to wait between "tick" events of a conversation
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationHeartbeatInterval")]
    pub conversation_heartbeat_interval: Option<String>,
    /// `conversationTerminatingTimeout` is the time to wait from detected FIN flag to end a conversation. Only relevant for TCP flows.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "conversationTerminatingTimeout")]
    pub conversation_terminating_timeout: Option<String>,
    /// `debug` allows setting some aspects of the internal configuration of the flow processor.
    /// This section is aimed exclusively for debugging and fine-grained performance optimizations,
    /// such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub debug: Option<FlowCollectorProcessorDebug>,
    /// `dropUnusedFields` [deprecated (*)] this setting is not used anymore.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dropUnusedFields")]
    pub drop_unused_fields: Option<bool>,
    /// `enableKubeProbes` is a flag to enable or disable Kubernetes liveness and readiness probes
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKubeProbes")]
    pub enable_kube_probes: Option<bool>,
    /// `healthPort` is a collector HTTP port in the Pod that exposes the health check API
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "healthPort")]
    pub health_port: Option<i32>,
    /// `imagePullPolicy` is the Kubernetes pull policy for the image defined above
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imagePullPolicy")]
    pub image_pull_policy: Option<FlowCollectorProcessorImagePullPolicy>,
    /// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages.
    /// This setting is ignored when Kafka is disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerAutoscaler")]
    pub kafka_consumer_autoscaler: Option<FlowCollectorProcessorKafkaConsumerAutoscaler>,
    /// `kafkaConsumerBatchSize` indicates to the broker the maximum batch size, in bytes, that the consumer accepts. Ignored when not using Kafka. Default: 10MB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerBatchSize")]
    pub kafka_consumer_batch_size: Option<i64>,
    /// `kafkaConsumerQueueCapacity` defines the capacity of the internal message queue used in the Kafka consumer client. Ignored when not using Kafka.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerQueueCapacity")]
    pub kafka_consumer_queue_capacity: Option<i64>,
    /// `kafkaConsumerReplicas` defines the number of replicas (pods) to start for `flowlogs-pipeline-transformer`, which consumes Kafka messages.
    /// This setting is ignored when Kafka is disabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kafkaConsumerReplicas")]
    pub kafka_consumer_replicas: Option<i32>,
    /// `logLevel` of the processor runtime
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logLevel")]
    pub log_level: Option<FlowCollectorProcessorLogLevel>,
    /// `logTypes` defines the desired record types to generate. Possible values are:<br>
    /// - `FLOWS` (default) to export regular network flows<br>
    /// - `CONVERSATIONS` to generate events for started conversations, ended conversations as well as periodic "tick" updates<br>
    /// - `ENDED_CONVERSATIONS` to generate only ended conversations events<br>
    /// - `ALL` to generate both network flows and all conversations events<br>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logTypes")]
    pub log_types: Option<FlowCollectorProcessorLogTypes>,
    /// `Metrics` define the processor configuration regarding metrics
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<FlowCollectorProcessorMetrics>,
    /// Set `multiClusterDeployment` to `true` to enable multi clusters feature. This adds clusterName label to flows data
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "multiClusterDeployment")]
    pub multi_cluster_deployment: Option<bool>,
    /// Port of the flow collector (host port).
    /// By convention, some values are forbidden. It must be greater than 1024 and different from
    /// 4500, 4789 and 6081.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// `profilePort` allows setting up a Go pprof profiler listening to this port
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "profilePort")]
    pub profile_port: Option<i32>,
    /// `resources` are the compute resources required by this container.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<FlowCollectorProcessorResources>,
    /// `subnetLabels` allows to define custom labels on subnets and IPs or to enable automatic labelling of recognized subnets in OpenShift.
    /// When a subnet matches the source or destination IP of a flow, a corresponding field is added: `SrcSubnetLabel` or `DstSubnetLabel`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "subnetLabels")]
    pub subnet_labels: Option<FlowCollectorProcessorSubnetLabels>,
}

/// `debug` allows setting some aspects of the internal configuration of the flow processor.
/// This section is aimed exclusively for debugging and fine-grained performance optimizations,
/// such as `GOGC` and `GOMAXPROCS` env vars. Set these values at your own risk.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorDebug {
    /// `env` allows passing custom environment variables to underlying components. Useful for passing
    /// some very concrete performance-tuning options, such as `GOGC` and `GOMAXPROCS`, that should not be
    /// publicly exposed as part of the FlowCollector descriptor, as they are only useful
    /// in edge debug or support scenarios.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub env: Option<BTreeMap<String, String>>,
}

/// `processor` defines the settings of the component that receives the flows from the agent,
/// enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorImagePullPolicy {
    IfNotPresent,
    Always,
    Never,
}

/// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages.
/// This setting is ignored when Kafka is disabled.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscaler {
    /// `maxReplicas` is the upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than MinReplicas.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxReplicas")]
    pub max_replicas: Option<i32>,
    /// Metrics used by the pod autoscaler. For documentation, refer to https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetrics>>,
    /// `minReplicas` is the lower limit for the number of replicas to which the autoscaler
    /// can scale down. It defaults to 1 pod. minReplicas is allowed to be 0 if the
    /// alpha feature gate HPAScaleToZero is enabled and at least one Object or External
    /// metric is configured. Scaling is active as long as at least one metric value is
    /// available.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minReplicas")]
    pub min_replicas: Option<i32>,
    /// `status` describes the desired status regarding deploying an horizontal pod autoscaler.<br>
    /// - `DISABLED` does not deploy an horizontal pod autoscaler.<br>
    /// - `ENABLED` deploys an horizontal pod autoscaler.<br>
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub status: Option<FlowCollectorProcessorKafkaConsumerAutoscalerStatus>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetrics {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerResource")]
    pub container_resource: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResource>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub external: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternal>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub object: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObject>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pods: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPods>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resource: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResource>,
    #[serde(rename = "type")]
    pub r#type: String,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResource {
    pub container: String,
    pub name: String,
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResourceTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsContainerResourceTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternal {
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetric,
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetric {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelector>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelector {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelectorMatchExpressions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalMetricSelectorMatchExpressions {
    pub key: String,
    pub operator: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsExternalTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObject {
    #[serde(rename = "describedObject")]
    pub described_object: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectDescribedObject,
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetric,
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectDescribedObject {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    pub api_version: Option<String>,
    pub kind: String,
    pub name: String,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetric {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelector>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelector {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelectorMatchExpressions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectMetricSelectorMatchExpressions {
    pub key: String,
    pub operator: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsObjectTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPods {
    pub metric: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetric,
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetric {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub selector: Option<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelector>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelector {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchExpressions")]
    pub match_expressions: Option<Vec<FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelectorMatchExpressions>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<BTreeMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsMetricSelectorMatchExpressions {
    pub key: String,
    pub operator: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsPodsTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResource {
    pub name: String,
    pub target: FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResourceTarget,
}

#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorKafkaConsumerAutoscalerMetricsResourceTarget {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageUtilization")]
    pub average_utilization: Option<i32>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "averageValue")]
    pub average_value: Option<IntOrString>,
    #[serde(rename = "type")]
    pub r#type: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<IntOrString>,
}

/// `kafkaConsumerAutoscaler` is the spec of a horizontal pod autoscaler to set up for `flowlogs-pipeline-transformer`, which consumes Kafka messages.
/// This setting is ignored when Kafka is disabled.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorKafkaConsumerAutoscalerStatus {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "ENABLED")]
    Enabled,
}

/// `processor` defines the settings of the component that receives the flows from the agent,
/// enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorLogLevel {
    #[serde(rename = "trace")]
    Trace,
    #[serde(rename = "debug")]
    Debug,
    #[serde(rename = "info")]
    Info,
    #[serde(rename = "warn")]
    Warn,
    #[serde(rename = "error")]
    Error,
    #[serde(rename = "fatal")]
    Fatal,
    #[serde(rename = "panic")]
    Panic,
}

/// `processor` defines the settings of the component that receives the flows from the agent,
/// enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorLogTypes {
    #[serde(rename = "FLOWS")]
    Flows,
    #[serde(rename = "CONVERSATIONS")]
    Conversations,
    #[serde(rename = "ENDED_CONVERSATIONS")]
    EndedConversations,
    #[serde(rename = "ALL")]
    All,
}

/// `Metrics` define the processor configuration regarding metrics
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorMetrics {
    /// `disableAlerts` is a list of alerts that should be disabled.
    /// Possible values are:<br>
    /// `NetObservNoFlows`, which is triggered when no flows are being observed for a certain period.<br>
    /// `NetObservLokiError`, which is triggered when flows are being dropped due to Loki errors.<br>
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "disableAlerts")]
    pub disable_alerts: Option<Vec<String>>,
    /// `ignoreTags` [deprecated (*)] is a list of tags to specify which metrics to ignore. Each metric is associated with a list of tags. More details in https://github.com/netobserv/network-observability-operator/tree/main/controllers/flowlogspipeline/metrics_definitions .
    /// Available tags are: `egress`, `ingress`, `flows`, `bytes`, `packets`, `namespaces`, `nodes`, `workloads`, `nodes-flows`, `namespaces-flows`, `workloads-flows`.
    /// Namespace-based metrics are covered by both `workloads` and `namespaces` tags, hence it is recommended to always ignore one of them (`workloads` offering a finer granularity).<br>
    /// Deprecation notice: use `includeList` instead.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ignoreTags")]
    pub ignore_tags: Option<Vec<String>>,
    /// `includeList` is a list of metric names to specify which ones to generate.
    /// The names correspond to the names in Prometheus without the prefix. For example,
    /// `namespace_egress_packets_total` will show up as `netobserv_namespace_egress_packets_total` in Prometheus.
    /// Note that the more metrics you add, the bigger is the impact on Prometheus workload resources.
    /// Metrics enabled by default are:
    /// `namespace_flows_total`, `node_ingress_bytes_total`, `workload_ingress_bytes_total`, `namespace_drop_packets_total` (when `PacketDrop` feature is enabled),
    /// `namespace_rtt_seconds` (when `FlowRTT` feature is enabled), `namespace_dns_latency_seconds` (when `DNSTracking` feature is enabled).
    /// More information, with full list of available metrics: https://github.com/netobserv/network-observability-operator/blob/main/docs/Metrics.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeList")]
    pub include_list: Option<Vec<String>>,
    /// Metrics server endpoint configuration for Prometheus scraper
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub server: Option<FlowCollectorProcessorMetricsServer>,
}

/// Metrics server endpoint configuration for Prometheus scraper
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorMetricsServer {
    /// The prometheus HTTP port
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub port: Option<i32>,
    /// TLS configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorProcessorMetricsServerTls>,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTls {
    /// `insecureSkipVerify` allows skipping client-side verification of the provided certificate.
    /// If set to `true`, the `providedCaFile` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// TLS configuration when `type` is set to `PROVIDED`.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub provided: Option<FlowCollectorProcessorMetricsServerTlsProvided>,
    /// Reference to the CA file when `type` is set to `PROVIDED`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providedCaFile")]
    pub provided_ca_file: Option<FlowCollectorProcessorMetricsServerTlsProvidedCaFile>,
    /// Select the type of TLS configuration:<br>
    /// - `DISABLED` (default) to not configure TLS for the endpoint.
    /// - `PROVIDED` to manually provide cert file and a key file. [Unsupported (*)].
    /// - `AUTO` to use OpenShift auto generated certificate using annotations.
    #[serde(rename = "type")]
    pub r#type: FlowCollectorProcessorMetricsServerTlsType,
}

/// TLS configuration when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTlsProvided {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsProvidedType>,
}

/// TLS configuration when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsProvidedType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Reference to the CA file when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorMetricsServerTlsProvidedCaFile {
    /// File name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    /// Name of the config map or secret containing the file
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the file reference: "configmap" or "secret"
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorProcessorMetricsServerTlsProvidedCaFileType>,
}

/// Reference to the CA file when `type` is set to `PROVIDED`.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsProvidedCaFileType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// TLS configuration.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorProcessorMetricsServerTlsType {
    #[serde(rename = "DISABLED")]
    Disabled,
    #[serde(rename = "PROVIDED")]
    Provided,
    #[serde(rename = "AUTO")]
    Auto,
}

/// `resources` are the compute resources required by this container.
/// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorResources {
    /// Claims lists the names of resources, defined in spec.resourceClaims,
    /// that are used by this container.
    /// 
    /// This is an alpha field and requires enabling the
    /// DynamicResourceAllocation feature gate.
    /// 
    /// This field is immutable. It can only be set for containers.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claims: Option<Vec<FlowCollectorProcessorResourcesClaims>>,
    /// Limits describes the maximum amount of compute resources allowed.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<BTreeMap<String, IntOrString>>,
    /// Requests describes the minimum amount of compute resources required.
    /// If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
    /// otherwise to an implementation-defined value. Requests cannot exceed Limits.
    /// More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub requests: Option<BTreeMap<String, IntOrString>>,
}

/// ResourceClaim references one entry in PodSpec.ResourceClaims.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorResourcesClaims {
    /// Name must match the name of one entry in pod.spec.resourceClaims of
    /// the Pod where this field is used. It makes that resource available
    /// inside a container.
    pub name: String,
    /// Request is the name chosen for a request in the referenced claim.
    /// If empty, everything from the claim is made available, otherwise
    /// only the result of this request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub request: Option<String>,
}

/// `subnetLabels` allows to define custom labels on subnets and IPs or to enable automatic labelling of recognized subnets in OpenShift.
/// When a subnet matches the source or destination IP of a flow, a corresponding field is added: `SrcSubnetLabel` or `DstSubnetLabel`.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorSubnetLabels {
    /// `customLabels` allows to customize subnets and IPs labelling, such as to identify cluster-external workloads or web services.
    /// If you enable `openShiftAutoDetect`, `customLabels` can override the detected subnets in case they overlap.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customLabels")]
    pub custom_labels: Option<Vec<FlowCollectorProcessorSubnetLabelsCustomLabels>>,
    /// `openShiftAutoDetect` allows, when set to `true`, to detect automatically the machines, pods and services subnets based on the
    /// OpenShift install configuration and the Cluster Network Operator configuration. Indirectly, this is a way to accurately detect
    /// external traffic: flows that are not labeled for those subnets are external to the cluster. Enabled by default on OpenShift.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "openShiftAutoDetect")]
    pub open_shift_auto_detect: Option<bool>,
}

/// SubnetLabel allows to label subnets and IPs, such as to identify cluster-external workloads or web services.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorProcessorSubnetLabelsCustomLabels {
    /// List of CIDRs, such as `["1.2.3.4/32"]`.
    pub cidrs: Vec<String>,
    /// Label name, used to flag matching flows.
    pub name: String,
}

/// `prometheus` defines Prometheus settings, such as querier configuration used to fetch metrics from the Console plugin.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorPrometheus {
    /// Prometheus querying configuration, such as client settings, used in the Console plugin.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub querier: Option<FlowCollectorPrometheusQuerier>,
}

/// Prometheus querying configuration, such as client settings, used in the Console plugin.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub struct FlowCollectorPrometheusQuerier {
    /// Set `enable` to `true` to make the Console plugin querying flow metrics from Prometheus instead of Loki whenever possible.
    /// The Console plugin can use either Loki or Prometheus as a data source for metrics (see also `spec.loki`), or both.
    /// Not all queries are transposable from Loki to Prometheus. Hence, if Loki is disabled, some features of the plugin are disabled as well,
    /// such as getting per-pod information or viewing raw flows.
    /// If both Prometheus and Loki are enabled, Prometheus takes precedence and Loki is used as a fallback for queries that Prometheus cannot handle.
    /// If they are both disabled, the Console plugin is not deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// Prometheus configuration for `Manual` mode.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub manual: Option<FlowCollectorPrometheusQuerierManual>,
    /// `mode` must be set according to the type of Prometheus installation that stores NetObserv metrics:<br>
    /// - Use `Auto` to try configuring automatically. In OpenShift, it uses the Thanos querier from OpenShift Cluster Monitoring<br>
    /// - Use `Manual` for a manual setup<br>
    pub mode: FlowCollectorPrometheusQuerierMode,
    /// `timeout` is the read timeout for console plugin queries to Prometheus.
    /// A timeout of zero means no timeout.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timeout: Option<String>,
}

/// Prometheus configuration for `Manual` mode.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorPrometheusQuerierManual {
    /// Set `true` to forward logged in user token in queries to Prometheus
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "forwardUserToken")]
    pub forward_user_token: Option<bool>,
    /// TLS client configuration for Prometheus URL.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls: Option<FlowCollectorPrometheusQuerierManualTls>,
    /// `url` is the address of an existing Prometheus service to use for querying metrics.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub url: Option<String>,
}

/// TLS client configuration for Prometheus URL.
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorPrometheusQuerierManualTls {
    /// `caCert` defines the reference of the certificate for the Certificate Authority
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "caCert")]
    pub ca_cert: Option<FlowCollectorPrometheusQuerierManualTlsCaCert>,
    /// Enable TLS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enable: Option<bool>,
    /// `insecureSkipVerify` allows skipping client-side verification of the server certificate.
    /// If set to `true`, the `caCert` field is ignored.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "insecureSkipVerify")]
    pub insecure_skip_verify: Option<bool>,
    /// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "userCert")]
    pub user_cert: Option<FlowCollectorPrometheusQuerierManualTlsUserCert>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorPrometheusQuerierManualTlsCaCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorPrometheusQuerierManualTlsCaCertType>,
}

/// `caCert` defines the reference of the certificate for the Certificate Authority
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorPrometheusQuerierManualTlsCaCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorPrometheusQuerierManualTlsUserCert {
    /// `certFile` defines the path to the certificate file name within the config map or secret
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certFile")]
    pub cert_file: Option<String>,
    /// `certKey` defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "certKey")]
    pub cert_key: Option<String>,
    /// Name of the config map or secret containing certificates
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where NetObserv is deployed.
    /// If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
    /// Type for the certificate reference: `configmap` or `secret`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<FlowCollectorPrometheusQuerierManualTlsUserCertType>,
}

/// `userCert` defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorPrometheusQuerierManualTlsUserCertType {
    #[serde(rename = "configmap")]
    Configmap,
    #[serde(rename = "secret")]
    Secret,
}

/// Prometheus querying configuration, such as client settings, used in the Console plugin.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
pub enum FlowCollectorPrometheusQuerierMode {
    Manual,
    Auto,
}

/// `FlowCollectorStatus` defines the observed state of FlowCollector
#[derive(Serialize, Deserialize, Clone, Debug, Default, PartialEq)]
pub struct FlowCollectorStatus {
    /// `conditions` represent the latest available observations of an object's state
    pub conditions: Vec<Condition>,
    /// Namespace where console plugin and flowlogs-pipeline have been deployed.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub namespace: Option<String>,
}

